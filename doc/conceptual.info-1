This is conceptual.info, produced by makeinfo version 4.12 from
conceptual.texi.

INFO-DIR-SECTION Programming
START-INFO-DIR-ENTRY
* coNCePTuaL: (conceptual).   A domain-specific language for network benchmarks
END-INFO-DIR-ENTRY

   This file describes the coNCePTuaL language and tool suite.

   Copyright (C) 2009, Los Alamos National Security, LLC


File: conceptual.info,  Node: Top,  Next: Introduction,  Prev: (dir),  Up: (dir)

coNCePTuaL
**********

   This document presents a simple, special-purpose language called
CONCEPTUAL.  CONCEPTUAL is intended for rapidly generating programs that
measure the performance and/or test the correctness of networks and
network protocol layers.  A few lines of CONCEPTUAL code can produce
programs that would take significantly more effort to write in a
conventional programming language.

   This document describes CONCEPTUAL version 1.2.

* Menu:

* Introduction::                Introduction to coNCePTuaL and this manual
* Installation::                Installing coNCePTuaL on your computer
* Usage::                       Running the compiler and related tools
* Grammar::                     Specification of the coNCePTuaL grammar
* Examples::                    Examples of complete coNCePTuaL programs
* Implementation::              How coNCePTuaL is implemented
* Tips and Tricks::             Helpful advice for using coNCePTuaL
* Troubleshooting::             Diagnosing coNCePTuaL errors
* Reserved Words::              Lists of token names not available as varaibles
* Backend Developer's Reference::  Lists of importance to backend developers
* Environment Variables::       List of environment variables honored
* Referenced Applications::     URLs for applications mentioned in this manual
* License::                     The coNCePTuaL copyright and license agreement
* Index::                       Index to terms used in this manual


File: conceptual.info,  Node: Introduction,  Next: Installation,  Prev: Top,  Up: Top

1 Introduction
**************

This document presents a simple, special-purpose language called
CONCEPTUAL.  CONCEPTUAL is intended for rapidly generating programs that
measure the performance and/or test the correctness of networks and
network protocol layers.  A few lines of CONCEPTUAL code can produce
programs that would take significantly more effort to write in a
conventional programming language.

   CONCEPTUAL is not merely a language specification.  The CONCEPTUAL
toolset includes a compiler, run-time library, and associated utility
programs that enable users to analyze network behavior quickly,
conveniently, and accurately.

* Menu:

* Motivation::                  Why there's a need for coNCePTuaL
* Limitations::                 Things coNCePTuaL can't do
* Typesetting conventions::     How to read this manual


File: conceptual.info,  Node: Motivation,  Next: Limitations,  Prev: Introduction,  Up: Introduction

1.1 Motivation
==============

A frequently reinvented wheel among network researchers is a suite of
programs that test a network's performance.  A problem with having
umpteen versions of performance tests is that it leads to a variety in
the way results are reported; colloquially, apples are often compared
to oranges.  Consider a bandwidth test.  Does a bandwidth test run for
a fixed number of iterations or a fixed length of time?  Is bandwidth
measured as ping-pong bandwidth (i.e., 2 * message length / round-trip
time) or unidirectional throughput (N messages in one direction
followed by a single acknowledgement message)?  Is the acknowledgement
message of minimal length or as long as the entire message?  Does its
length contribute to the total bandwidth?  Is data sent
unidirectionally or in both directions at once?  How many warmup
messages (if any) are sent before the timing loop?  Is there a delay
after the warmup messages (to give the network a chance to reclaim any
scarce resources)?  Are receives nonblocking (possibly allowing overlap
in the NIC) or blocking?

   The motivation behind creating CONCEPTUAL, a simple specification
language designed for describing network benchmarks, is that it enables
a benchmark to be described sufficiently tersely as to fit easily in a
report or research paper, facilitating peer review of the experimental
setup and timing measurements.  Because CONCEPTUAL code is simple to
write, network tests can be developed and deployed with low turnaround
times--useful when the results of one test suggest a following test
that should be written.  Because CONCEPTUAL is special-purpose its
run-time system can perform the following functions, which benchmark
writers often neglect to implement:

   * logging information about the environment under which the benchmark
     ran: operating system, CPU architecture and clock speed, timer type
     and resolution, etc.

   * aborting a program if it takes longer than a predetermined length
     of time to complete

   * writing measurement data and descriptive statistics to a variety of
     output formats, including the input formats of various
     graph-plotting programs

CONCEPTUAL is not limited to network peformance tests, however.  It can
also be used for network verification.  That is, CONCEPTUAL programs can
be used to locate failed links or to determine the frequency of bit
errors--even those that may sneak past the network's CRC hardware.

   In addition, because CONCEPTUAL is a very high-level language, the
CONCEPTUAL compiler's backend has a great deal of potential.  It would
be possible for the backend to produce a variety of target formats such
as Fortran + MPI, Perl + sockets, C + a network vendor's low-level
messaging layer, and so forth.  It could directly manipulate a network
simulator.  It could feed into a graphics program to produce a
space-time diagram of a CONCEPTUAL program.  The possibilities are
endless.


File: conceptual.info,  Node: Limitations,  Next: Typesetting conventions,  Prev: Motivation,  Up: Introduction

1.2 Limitations
===============

Although CONCEPTUAL can express a wide variety of race-free
communication patterns it cannot currently express data-dependent
communication.  For example, CONCEPTUAL canot express a master-worker
pattern in which a master task sends a message to a worker task as a
reaction to that particular worker's sending of a message to the
master.  Such a communication pattern is not independent of the order
in which messages happen to arrive from the workers.  Similarly,
CONCEPTUAL cannot use run-time performance data to guide its operations.
It is therefore not currently possible to express a communication
benchmark which repeats until the standard error of some performance
metric drops below a given threshold.  These limitations may be lifted
in a future release of the system.


File: conceptual.info,  Node: Typesetting conventions,  Prev: Limitations,  Up: Introduction

1.3 Typesetting conventions
===========================

The following table showcases the typesetting conventions used in this
manual to attribute various meanings to text.  Note that not all of the
conventions are typographically distinct.

`-a'
`--abcdef'
     command-line options (e.g., `-C' or `--help')

`ABCDEF'
     environment variables (e.g., `PATH')

<ABCDEF>
     nonterminals in the CONCEPTUAL grammar (e.g., <IDENT>)

`abcdef'
     commands to enter on the keyboard (e.g., `make install')

`abcdef'
     file and directory names (e.g., `conceptual.pdf')

`ABCDEF'
     CONCEPTUAL keywords (e.g., `RECEIVE')

`abcdef'
     variables, constants, functions, and types in any language (e.g.,
     `bit_errors' or `gettimeofday()')

ABCDEF
     metasyntactic variables and formal function parameters (e.g.,
     FAN-OUT)

`abcdef'
     snippets of code, command lines, files, etc. (e.g., `10 MOD 3')


File: conceptual.info,  Node: Installation,  Next: Usage,  Prev: Introduction,  Up: Top

2 Installation
**************

CONCEPTUAL uses the GNU Autotools (Autoconf, Automake, and Libtool) to
increase portability, to automate compilation, and to facilitate
installation.  As of this writing, CONCEPTUAL has passed `make check'
(*note make::) on the following platforms:

Architecture    OS           Compiler
IA-32           Linux        `gcc'     (GNU)
                             `icc'     (Intel)
                             `opencc'  (Open64)
                FreeBSD      `gcc'     (GNU)
                OpenBSD      `gcc'     (GNU)
                NetBSD       `gcc'     (GNU)
                Solaris      `gcc'     (GNU)
                             `cc'      (Sun)
                Syllable     `gcc'     (GNU)
                Windows      `gcc'  (GNU)
                (via         
                Cygwin)      

x86-64          Linux        `gcc'     (GNU)
                             `pgcc'    (PGI)
                             `pathcc'  (PathScale)
                Catamount    `gcc'     (GNU)
                             `pgcc'    (PGI)

IA-64           Linux        `gcc'     (GNU)
                             `ecc'     (Intel)

PowerPC         Linux        `gcc'     (GNU)
                             `xlc'     (IBM)
                AIX          `gcc'     (GNU)
                             `xlc'     (IBM)
                MacOS X      `gcc'     (GNU)
                BLRTS        `xlc'     (IBM)

Cell (Power)    Linux        `gcc'     (GNU)
Cray X1         UNICOS/mp    `cc'      (Cray)

UltraSPARC      Solaris      `gcc'     (GNU)
                             `cc'      (Sun)

MIPS            IRIX         `gcc'     (GNU)
                             `cc'      (MIPSpro)

Alpha           Linux        `gcc'     (GNU)
                             `ccc'     (HP)
                Tru64        `gcc'     (GNU)
                             `cc'      (HP)
ARM             Linux        `gcc'     (GNU)


   In its simplest form, CONCEPTUAL installation works by executing the
following commands at the operating-system prompt:

     ./configure
     make
     make install

(`configure' is normally run as `./configure ' to force it to run from
the current directory on the assumption that `.' is not in the
executable search path.)  We now describe those three installation
steps in detail, listing a variety of customization options for each
step.

* Menu:

* configure::                   Create a customized Makefile for your system
* make::                        Compile the coNCePTuaL run-time library
* make install::                Install coNCePTuaL in your system


File: conceptual.info,  Node: configure,  Next: make,  Prev: Installation,  Up: Installation

2.1 `configure'
===============

`configure' is a Bourne-shell script that analyzes your system's
capabilities (compiler features, library and header-file availability,
function and datatype availability, linker flags for various options,
etc.) and custom-generates a `Makefile' and miscellaneous other files.  `configure'
accepts a variety of command-line options.  `./configure --help' lists
all of the options.  The following are some of the more useful ones:

`--disable-shared'
     CONCEPTUAL normally installs both static and dynamic libraries.
     While dynamic libraries have a number of advantages they do need
     to be installed on all nodes that run the compiled CONCEPTUAL
     programs.  If global installation is not convenient/feasible, `--disable-shared'
     can be used to force static linking of executables.  Note, however,
     that `libncptlmodule.so', the Python interface to the CONCEPTUAL
     run-time library, needs to be built as a shared object so that it
     can be loaded dynamically into a running Python interpreter.  `--disable-shared'
     inhibits the compilation and installation of `libncptlmodule.so'.

`--prefix=DIRECTORY'
     `make install' normally installs CONCEPTUAL into the `/usr/local'
     directory.  The `--prefix' option instructs `configure' to write a `Makefile'
     with a different installation directory.  For example, `--prefix=/local/encap/conceptual-1.2'
     will cause CONCEPTUAL's files to be installed in
     `/local/encap/conceptual-1.2/bin',
     `/local/encap/conceptual-1.2/include', etc.

`--with-ignored-libs=LIB1,LIB2,...'
     In some circumstances it may be necessary to prevent CONCEPTUAL
     from using certain libraries even when `./configure ' detects them
     and believes them to be usable.  The `--with-ignored-libs'
     configuration option forces `./configure ' to ignore one or more
     specified libraries.  Only the base name of each library should be
     used; omit directory names, the `lib' prefix (on Unix-like
     systems), and the file suffix.  For example, to disable the use of
     `/usr/local/lib/libpapi.a' you should specify `--with-ignored-libs=papi'.

`--without-fork'
     `./configure ' detects automatically if your system provides a
     working `fork()' function.  However, it cannot detect if `fork()'
     correctly spawns a child process but corrupts the parent's memory
     map while doing so, as is the case when using some InfiniBand
     software stacks.  The `--without-fork' option inhibits the use of `fork()'
     and well as functions that implicitly invoke `fork()' such as `system()'
     and `popen()'.

`--with-gettimeofday'
     The CONCEPTUAL run-time library is able to use any of a variety of
     platform-specific microsecond timers to take timing measurements.
     (*Note Time-related functions::, for a complete list.)  The `--with-gettimeofday'
     option forces the run-time library to utilize instead the generic
     C `gettimeofday()' function.  This can be useful in the rare, but
     not impossible, case that a quirk in some particular platform
     misleads one of CONCEPTUAL's other timers.  The `validatetimer'
     utility (*note Validating the coNCePTuaL timer::) can help
     determine whether `--with-gettimeofday' is necessary.

`--with-mpi-wtime'
     On some systems the most accurate timer available is provided by
     the `MPI_Wtime()' function in the MPI library.  The `--with-mpi-wtime'
     option forces the run-time library to measure elapsed time using `MPI_Wtime()'
     instead of any of the other available timers.  (*Note Time-related
     functions::, for a complete list).  The ramifications of `--with-mpi-wtime'
     are threefold:

       1. The option requires that you link all CONCEPTUAL programs
          against an MPI library and run them like any other MPI
          program.  (You may need to set `CPPFLAGS', `LIBS', `LDFLAGS',
          or some of the other command-line variables described below.)

       2. `MPI_Wtime()' may be _less_ accurate than some of the other
          timers available to CONCEPTUAL.  In many MPI implementations, `MPI_Wtime()'
          simply invokes `gettimeofday()', for instance.

       3. Although this is a rare problem, it may not be safe to invoke `MPI_Wtime()'
          without first invoking `MPI_Init()'.  Fortunately, proper
          juxtaposition of the two functions is not a concern for the
          CONCEPTUAL C+MPI backend (*note The c_mpi backend::), which
          ensures that `MPI_Init()' is invoked before `MPI_Wtime()'.

     In short, you should specify `--with-mpi-wtime' only if you have
     good reason to believe that `MPI_Wtime()' is likely to produce the
     most accurate timing measurements on your system.

`CC=C COMPILER'
     `configure' automatically searches for a C compiler to use.  To
     override its selection, assign a value to `CC' on the command
     line.  For example, `./configure CC=ecc' will cause CONCEPTUAL to
     be built with `ecc'.

`CFLAGS=C COMPILER FLAGS'
`LDFLAGS=LINKER FLAGS'
`CPPFLAGS=C PREPROCESSOR FLAGS'
`LIBS=EXTRA LIBRARIES'
     Like `CC', these variables override the values determined
     automatically by `configure'.  As an illustration, `./configure
     CPPFLAGS="-DSPECIAL -I/home/pakin/include/special -I."
     CFLAGS="-O3 -g -Wall -W" LDFLAGS=--static LIBS="-lz
     /usr/lib/libstuff.a"' assigns values to all four variables.

`MPICC=C COMPILER'
`MPICPPFLAGS=C PREPROCESSOR FLAGS'
`MPILDFLAGS=EXTRA LINKER FLAGS'
`MPILIBS=EXTRA LIBRARIES'
     These variables are analagous to `CC', `CPPFLAGS', `LDFLAGS', and `LIBS',
     respectively.  The difference is that they are not used to build
     the CONCEPTUAL run-time library but rather to build user programs
     targeted to the C+MPI compiler backend.  For example, if your MPI
     installation lacks an `mpicc' script, you may need to specify
     extra header files and libraries explicitly: `./configure
     MPICPPFLAGS="-I/usr/lib/mpi/include" MPILIBS="-lmpich"'.

   As a rather complex illustration of how some of the preceding options
(as well as a few mentioned by `./configure --help') might be combined,
the following is how CONCEPTUAL was once configured to cross-compile
from a Linux/PowerPC build machine to a prototype of the BlueGene/L
supercomputer (containing, at the time, 2048 embedded PowerPC
processors, each executing a minimal run-time system, BLRTS).  IBM's `xlc'
compiler was accessed via a wrapper script called `mpcc'.

     `./configure CFLAGS="-g -O -qmaxmem=64000" CC=/bgl/local/bin/mpcc
     CPP="gcc -E" --host=powerpc-ibm-linux-gnu
     --build=powerpc-unknown-linux-gnu --with-alignment=8
     --with-gettimeofday --prefix=/bgl/bgguest/LANL/ncptl
     MPICC=/bgl/local/bin/mpcc
     CPPFLAGS=-I/BlueLight/floor/bglsys/include'

   It's always best to specify environment variables as arguments to `./configure
' because the `configure' script writes its entire command line as a
comment to `config.log' and as a shell command to `config.status' to
enable re-running `./configure ' with exactly the same parameters.


   When `./configure ' finishes running it outputs a list of the warning
messages that were issued during the run.  If no warnings were issued, `./configure
' will output `Configuration completed without any errors or
warnings.'.  Warnings are also written to `config.log' and can
therefore be redisplayed at any time by executing a shell command such
as `grep WARNING config.log'.


File: conceptual.info,  Node: make,  Next: make install,  Prev: configure,  Up: Installation

2.2 `make'
==========

Running `make' by itself will compile the CONCEPTUAL run-time library.
However, the `Makefile' generated by `configure' can perform a variety
of other actions, as well:

`make check'
     Perform a series of regression tests on the CONCEPTUAL run-time
     library.  This is a good thing to do after a `make' to ensure that
     the run-time library built properly on your system.  When `make check'
     finishes it summarizes the test results.  The following output
     signifies a successful completion of `make check':

          ===================
          All 21 tests passed
          ===================

     The total number of tests performed depends upon the way that
     CONCEPTUAL was configured.  CONCEPTUAL components that could not
     be built are not tested.

     If any tests behave unexpectedly it may be possible to gain more
     information about the source of the problem by re-running check
     `make check' with the `DEBUG' environment variable set to a
     non-empty value:

          env DEBUG=1 make check

     Tests can also be re-run individually:

          cd tests
          env DEBUG=1 runtime_random

`make clean'
`make distclean'
`make maintainer-clean'
     `make clean' deletes all files generated by a preceding `make'
     command.  `make distclean' deletes all files generated by a
     preceding `./configure ' command.  maintainer-clean
     `make maintainer-clean' delete all generated files.  Run maintainer-clean
     `make maintainer-clean' only if you have fairly recent versions of
     the GNU Autotools (Autoconf 2.53, Automake 1.6, and Libtool 1.4)
     because those are needed to regenerate some of the generated
     files.  The sequence of operations to regenerate all of the
     configuration files needed by CONCEPTUAL is shown below.

          libtoolize --force --copy
          aclocal
          autoheader
          automake --add-missing --copy
          autoconf

`make install'
     Install CONCEPTUAL, including the compiler, run-time library,
     header files, and tools.  `make install' is described in detail in
     *note make install::.

`make uninstall'
     Remove all of the files that `make install' installed.  Most of
     the top-level directories are retained, however, as `make' cannot
     guarantee that these are not needed by other applications.

`make info'
`make pdf'
`make docbook'
     Produce the CONCEPTUAL user's guide (this document) in,
     respectively, Emacs info format, PDF format, or DocBook format.
     The resulting documentation (`conceptual.info*', `conceptual.pdf',
     or `conceptual.xml') is created in the `doc' subdirectory.

`make ncptl-logextract.html'
     CONCEPTUAL comes with a postprocessor called `ncptl-logextract'
     which facilitates extracting information from CONCEPTUAL-produced
     log files.  The complete `ncptl-logextract' documentation is
     presented in *note ncptl-logextract::.  As is readily apparent
     from that documentation, `ncptl-logextract' supports an
     overwhelming number of command-line options.  To make the `ncptl-logextract'
     documentation more approachable, the `make ncptl-logextract.html'
     command creates a dynamic HTML version of it (and stores in the
     `doc' subdirectory).  The result, `ncptl-logextract.html',
     initially presents only the top level of the `ncptl-logextract'
     option hierarchy.  Users can then click on the name of a
     command-line option to expand or contract the list of subobtions.
     This interactive behavior makes it easy for a user to get more
     information on some options without being distracted by the
     documentation for the others.

`make empty.log'
     Create an empty log file called `empty.log' which contains a
     complete prologue and epilogue but no data.  This is convenient for
     validating that the CONCEPTUAL run-time library was built using
     your preferred build options.

`make stylesheets'
     CONCEPTUAL can automatically produce stylesheets for a variety of
     programs.  These stylesheets make keywords, comments, strings, and
     other terms in the language visually distinct from each other for a
     more aesthetically appealing appearance.  Currently, `make stylesheets'
     produces a LaTeX2e package (`ncptl.sty'), an a2ps style sheet (`ncptl.ssh'),
     an Emacs major mode (`ncptl-mode.el'/`ncptl-mode.elc'), a Vim
     syntax file (`ncptl.vim'), and a Source-highlight language
     definition (`ncptl.lang').  Note that the `Makefile' currently
     lacks provisions for installing these files so whichever
     stylesheets are desired will need to be installed manually.
     Stylesheet installation is detailed in *note Installing
     stylesheets::.

`make modulefile'
     The Environment Modules package facilitates configuring the
     operating-system shell for a given application.  The modulefile
     `make modulefile' command creates a `conceptual_1.2' modulefile
     that checks for conflicts with previously loaded CONCEPTUAL
     modulefiles then sets the `PATH', `MANPATH', and `LD_LIBRARY_PATH'
     environment variables to values appropriate values as determined
     by `configure' (*note configure::).

     Normally, `conceptual_1.2' should be installed in the system's
     module path (as described by the `MODULEPATH' environment
     variable).  However, users without administrator access can still
     use the CONCEPTUAL modulefile as a convenient mechanism for
     properly setting all of the environment variables needed by
     CONCEPTUAL:

          make modulefile
          module load ./conceptual_1.2

     See the `module' man page for more information about modules.

`make dist'
     Package together all of the files needed to rebuild CONCEPTUAL.
     The resulting file is called `conceptual-1.2.tar.gz' (for this
     version of CONCEPTUAL).

`make all'
     Although `all' is the default target it can also be specified
     explicitly.  Doing so is convenient when performing multiple
     actions at once, e.g., `make clean all'.

`make tags'
     Produce/update a `TAGS' file that the Emacs text editor can use to
     find function declarations, macro definitions, variable
     definitions, `typedef's, etc. in the CONCEPTUAL run-time library
     source code.  This is useful primarily for developers wishing to
     interface with the CONCEPTUAL run-time library.  Read the Emacs
     documentation for `M-x find-tag' for more information.

`make gui'
     Compile the CONCEPTUAL GUI, producing `ncptlGUI-1.2.jar'.  Note
     that compilation requires both a Java compiler (e.g., `javac') and
     the Jython Python-to-Java compiler (`jythonc').  Unfortunately, at
     the time of this writing (January 2009), `jythonc''s future is
     uncertain (cf. `http://www.jython.org/Project/jythonc.html').
     Hence, `make gui' has been tested only with `jythonc'
     version 2.2.X, not any later versions.


* Menu:

* Validating the coNCePTuaL timer::  Ensuring timing results are meaningful


File: conceptual.info,  Node: Validating the coNCePTuaL timer,  Prev: make,  Up: make

Validating the coNCePTuaL timer
-------------------------------

`make' automatically builds a program called `validatetimer'.  `validatetimer'
helps validate that the real-time clock used by the CONCEPTUAL run-time
library accurately measures wall-clock time.  The idea is to compare
CONCEPTUAL's timer to an external clock (i.e., one not associated with
the computer).  Simply follow the program's prompts:

     % validatetimer
     Press <Enter> to start the clock ...
     Press <Enter> again in exactly 60 seconds ...

     coNCePTuaL measured 60.005103 seconds.
     coNCePTuaL timer error = 0.008505%

   If the difference between CONCEPTUAL's timer and an external clock is
significant, then performance results from CONCEPTUAL--and possibly
from other programs, as well--should not be trusted.  Note that only
extreme differences in timings are significant; there will always be
_some_ error caused by human response time and by system I/O speed.  In
the case that there _is_ an extreme performance difference,(1)  the `--with-gettimeofday'
option to `configure' (*note configure::) may be a viable workaround.

   `validatetimer' takes an optional command-line argument, which is
the number of seconds of wall-clock time to expect.  The default is
`60'.  Larger numbers help amortize error; smaller numbers enable the
program to finish sooner.

   ---------- Footnotes ----------

   (1) To date, extreme performance differences have been observed
primarily on PowerPC-based systems.  The PowerPC cycle counter is
clocked at a different rate from the CPU speed, which may confuse
CONCEPTUAL.  The run-time library compensates for this behavior on all
tested platforms (*note Installation::), but the user should
nevertheless make sure to run `validatetimer' to verify that
CONCEPTUAL's timer is sufficiently accurate.


File: conceptual.info,  Node: make install,  Prev: make,  Up: Installation

2.3 `make install'
==================

The CONCEPTUAL compiler and run-time library are installed with `make install'.
Although `configure' can specify the default installation directory
(*note configure::), this can be overridden at `make install' time in
one of two ways.  `make DESTDIR=PREFIX install' prepends PREFIX to
every directory when installing.  However, the files are installed
believing that `DESTDIR' was not specified.  For example, `make DESTDIR=/mnt install'
would cause executables to be installed into `/mnt/usr/local/bin', but
if any of these are symbolic links, the link will omit the `/mnt'
prefix.

   The second technique for overriding installation directories is to
specify a new value for `prefix' on the command line.  That is, `make prefix=/opt/ncptl install'
will install into `/opt/ncptl/bin', `/opt/ncptl/include',
`/opt/ncptl/man', etc., regardless of the `--prefix' value given to `configure'.
CONCEPTUAL's `Makefile' provides even finer-grained control than that.
Instead of--or in addition to--specifying a `prefix' option on the
command line, individual installation directories can be named
explicitly.  These include `bindir', `datadir', `libdir', `includedir',
`infodir', `mandir', `pkgdatadir', `pythondir', and many others.
Scrutinize the `Makefile' to find a particular directory that should be
overridden.

   The remainder of this section presents a number of optional
installation steps that add CONCEPTUAL support to a variety of
third-party software packages.

* Menu:

* Installing stylesheets::      Where to put the various stylesheet files
* SLOCCount::                   Automatically counting lines of coNCePTuaL code
* pkg-config::                  Linking with the coNCePTuaL run-time library


File: conceptual.info,  Node: Installing stylesheets,  Next: SLOCCount,  Prev: make install,  Up: make install

Installing stylesheets
----------------------

The `make stylesheets' command (*note make::) produces a variety of
stylesheets for presenting CONCEPTUAL code in a more pleasant format
than ordinary, monochromatic text.  Stylesheets must currently be
installed manually as per the following instructions:

`ncptl.sty'
     `ncptl.sty' is typically installed in `TEXMF/tex/latex/misc',
     where TEXMF is likely to be `/usr/local/share/texmf'.  On a Web2c
     version of TeX the command `kpsewhich -expand-var='$TEXMFLOCAL''
     should output the correct value of TEXMF.  In most TeX
     distributions the filename database needs to be refreshed after a
     new package is installed.  See
     `http://www.tex.ac.uk/cgi-bin/texfaq2html?label=instpackages' for
     more information.  `ncptl.sty' is merely a customization of the `listings'
     package that defines a new language called `ncptl'.  See the `listings'
     documentation for instructions on typesetting source code.

`ncptl.ssh'
     Running `a2ps --list=defaults' outputs (among other things) the a2ps
     library path.  `ncptl.ssh' should be installed in one of the `sheets'
     directories listed there, typically `/usr/share/a2ps/sheets'.

`ncptl-mode.el'
`ncptl-mode.elc'
     `ncptl-mode.el' and `ncptl-mode.elc' belong in a local Elisp
     directory that is part of the Emacs `load-path', e.g.,
     `/usr/share/emacs/site-lisp'.  The following Elisp code, which
     belongs in `~/.emacs' for GNU Emacs or `~/.xemacs/init.el' for XEmacs,
     makes Emacs set `ncptl-mode' whenever opening a file with
     extension `.ncptl':

          (autoload 'ncptl-mode "ncptl-mode"
            "Major mode for editing coNCePTuaL programs." t)
          (add-to-list 'auto-mode-alist '("\\.ncptl$" . ncptl-mode))

     Syntax highlighting should be enabled by default.  If it isn't, the Emacs
     command `M-x font-lock-mode' should enable it for the current
     buffer.

`ncptl.vim'
     Vim's syntax-file directory may be named after the Vim version,
     e.g., `/usr/share/vim/vim61/syntax' for Vim 6.1.  Put `ncptl.vim'
     there.  To associate `.ncptl' files with CONCEPTUAL code, the
     following lines need to be added to Vim's `filetype.vim' file
     somewhere between the `augroup filetypedetect' line and the
     `augroup END' line:

          " coNCePTuaL
          au BufNewFile,BufRead *.ncptl           setf ncptl

`ncptl.lang'
     Source-highlight stores all of its helper files in a single
     directory, typically `/usr/share/source-highlight'.  Put `ncptl.lang'
     there.  To associate `.ncptl' files with CONCEPTUAL code you will
     also need to add the following line to the `lang.map' file in the
     same directory:

          ncptl = ncptl.lang



File: conceptual.info,  Node: SLOCCount,  Next: pkg-config,  Prev: Installing stylesheets,  Up: make install

SLOCCount
---------

SLOCCount is a utility that counts the number of lines of code in a
file, excluding blank lines and comments.  SLOCCount supports a variety
of programming languages and it is straightforward to get it to support
CONCEPTUAL, as well.  The procedure follows the "Adding support for new
languages" section of the SLOCCount manual:

  1. Create an `ncptl_count' script with the following contents:

          #! /bin/sh

          generic_count "#" $@

  2. Mark the script executable and install it somewhere in your
     executable search path.

  3. Edit SLOCCount's `break_filelist' Perl script to include the
     following association in the `%file_extensions' hash:

            "ncptl" => "ncptl",    # coNCePTuaL



File: conceptual.info,  Node: pkg-config,  Prev: SLOCCount,  Up: make install

pkg-config
----------

The pkg-config utility helps ensure that programs are given appropriate
compiler and linker flags to use a particular package's C header files
and libraries.  CONCEPTUAL's `configure' script (*note configure::)
automatically produces a pkg-config configuration file for the
CONCEPTUAL header file (`ncptl.h') and run-time library (`libncptl').
This configuration file, `ncptl.pc', should be installed in one of the
directories searched by pkg-config (`/usr/lib/pkgconfig' on some
systems).  Once `ncptl.pc' is installed, pkg-config can be used to
compile C programs that require the CONCEPTUAL header file and link
programs that require the CONCEPTUAL run-time library, as is shown in
the following example:

     cc `pkg-config --cflags ncptl` -c myprog.c
     cc -o myprog myprog.o `pkg-config --libs ncptl`


File: conceptual.info,  Node: Usage,  Next: Grammar,  Prev: Installation,  Up: Top

3 Usage
*******

CONCEPTUAL is more than just a language; it is a complete toolset which
consists of the following components:

   * the CONCEPTUAL language (*note Grammar::)

   * a compiler and run-time library for CONCEPTUAL programs (*note
     Compiling coNCePTuaL programs::)

   * a set of compiler backends that can generate code for a variety of
     languages and communication layers (*note Supplied backends::)

   * utilities to help analyze the results (*note ncptl-logextract::)

   This chapter explains how to compile and run CONCEPTUAL programs and
how to interpret the log files they output.

* Menu:

* The coNCePTuaL GUI::          Drawing communication patterns
* Compiling coNCePTuaL programs::  Options accepted by the compiler
* Supplied backends::           Descriptions of the standard backends
* Running coNCePTuaL programs::  Options accepted by coNCePTuaL programs
* Interpreting coNCePTuaL log files::  Understanding program output


File: conceptual.info,  Node: The coNCePTuaL GUI,  Next: Compiling coNCePTuaL programs,  Prev: Usage,  Up: Usage

3.1 The coNCePTuaL GUI
======================

The CONCEPTUAL graphical user interface (GUI) is the easiest way to get
started with CONCEPTUAL.  Instead of writing code in the CONCEPTUAL
language (documented in detail in *note Grammar::), a user merely
_draws_ a communication pattern using the mouse, and the CONCEPTUAL GUI
automatically produces a CONCEPTUAL program from that illustration.
The generated program can then be compiled just like a hand-coded
CONCEPTUAL program as per the instructions in *note Compiling
coNCePTuaL programs::.

   The CONCEPTUAL GUI is written in Java and therefore requires a Java
virtual machine (JVM) to run.  However, the CONCEPTUAL GUI is quite
portable and should run identically on every platform for which a JVM
exists.  The following is a typical command for launching the CONCEPTUAL
GUI from the command line:

     java -jar ncptlGUI-1.2.jar

   The CONCEPTUAL GUI is split into two main panels. The _program
panel_ displays the components that make up the program and how they
interact with each other.  The _dialog panel_ displays fields for
setting the options of selected components.  Furthermore, a _menu bar_
provides access to various GUI-wide operations, and a _command bar_
includes buttons for creating new components in the program panel.

 [image src="gui.png" alt="Screenshot of the coNCePTuaL GUI" text="
+----------------------------------------------------------------+
| File  Edit  Options  Advanced                                  | Menu bar
+----------------------------------------------------------------+
| [Add Row] [Delete] [Loop] [Measure] [Compute] [Communicate]    | Command
| [Wait] [Extend] [Synchronize] [Reduce] [Multicast] [Normalize] | bar
+----------------------------------------------------------------+
|                                                                | Program
| (0) (1) (2) (3) (4) (5) (6) (7)                                | panel
|                                                                |
| ===============================                                |
|                                                                |
+----------------------------------------------------------------+
| To enable:                                                     | Dialog
|  -Delete: select one or more components.                       | panel
|  -Loop: select one or more task rows or blocks.                |
|  -Measure: select one or more task rows or blocks.             |
|  -Compute: select one or more tasks.                           |
|  -Communicate: select tasks in two task rows.                  |
+----------------------------------------------------------------+
" ]
* Menu:

* Components::                  Graphical representations of coNCePTuaL objects
* Menu bar::                    Operations performed by each menu item
* Command bar::                 Operations performed by each button


File: conceptual.info,  Node: Components,  Next: Menu bar,  Prev: The coNCePTuaL GUI,  Up: The coNCePTuaL GUI

3.1.1 Components
----------------

Components are graphical representations of CONCEPTUAL objects and
operations used by the CONCEPTUAL GUI to specify programs.  Components
are added to the program panel by clicking on their corresponding
buttons in the command bar.  To select a component in the program
panel, simply left-click on it.  If the component has parameters that
can be edited, a dialog will appear in the dialog panel.  Multiple
components can be selected by dragging the mouse over target components
or holding down Ctrl as you left-click on several components.

   The CONCEPTUAL GUI lets a user manipulate the following components:

task/task row
     Tasks represent operational units in CONCEPTUAL programs and are
     analogous to a process or thread in a parallel program.  For
     example, a CONCEPTUAL operation like point-to-point communication
     has a source task that specifies how the message is sent and a
     target task that specifies how the message is received.  Tasks are
     displayed graphically as numbered circles in the CONCEPTUAL GUI.
     A task row represents the total number of tasks available for
     operations at each step of a CONCEPTUAL program.  *Note Task
     descriptions::, for information on how to specify subsets of a
     program's tasks.

communication message
     Messages between tasks are displayed as directed edges (arrows)
     from source task to target task in the CONCEPTUAL GUI.  Messages
     are added to the program panel via the command bar or by dragging
     the mouse from a source task to a target task.  Communication
     messages in the CONCEPTUAL GUI correspond to the `SEND' and `RECEIVE'
     statements in the CONCEPTUAL language (*note Sending::, and *note
     Receiving::).

awaiting completion
     Messages that are sent/received asynchronously must eventually be
     waited on.  Awaits message completion is displayed as a solid line
     under the associated tasks in the CONCEPTUAL GUI.  Awaiting
     completion in the CONCEPTUAL GUI correspond to the `AWAIT
     COMPLETION' statement in the CONCEPTUAL language (*note Awaiting
     completion::).

loop
     One can add a loop around selected components to repeat the
     corresponding CONCEPTUAL operations.  Loops in the CONCEPTUAL GUI
     correspond to the `FOR' statement in the CONCEPTUAL language
     (*note Iterating::).

measurement block
     One can log timing or other measurements of CONCEPTUAL operations
     by placing them in a measurement block.  Measurement blocks in the CONCEPTUAL
     GUI correspond to the `LOGS' statement in the CONCEPTUAL language
     (*note Writing to a log file::).

computation/sleeping
     Artificial computation (really a spin loop) and sleeping, both of
     which delay the program for a given length of time, can be
     performed on a set of tasks.  Computation is shown with `cmp'
     under a task in the CONCEPTUAL GUI, and sleeping is shown with
     `slp'.  Computation/sleeping in the CONCEPTUAL GUI corresponds to
     the `COMPUTE' and `SLEEP' statements in the CONCEPTUAL language
     (*note Delaying execution::).

multicasting
     A multicast operation sends a message from a source task to
     multiple target tasks.  Multicasting in the CONCEPTUAL GUI
     corresponds to the `MULTICAST' statement in the CONCEPTUAL language
     (*note Multicasting::).

reduction
     A reduction operation combines messages from multiple source tasks
     to a single target task.  Reduction in the CONCEPTUAL GUI
     corresponds to the `REDUCE' statement in the CONCEPTUAL language
     (*note Reducing::).

synchronization
     Barrier synchronization forces a set of tasks to wait until each
     task in the set reaches the synchronization point before any task
     in the set proceeds past the synchronization point.
     Synchronization is displayed as a dotted line under the associated
     tasks in the CONCEPTUAL GUI.  Synchronization in the CONCEPTUAL
     GUI corresponds to the `SYNCHRONIZE' statement in the CONCEPTUAL
     language (*note Synchronizing::).


File: conceptual.info,  Node: Menu bar,  Next: Command bar,  Prev: Components,  Up: The coNCePTuaL GUI

3.1.2 Menu bar
--------------

The _File_ menu, which appears only when the CONCEPTUAL GUI is granted
access to the filesystem, contains _New_, _Open_, _Save_, _Save As_, _Print_,
and _Quit_ commands that exhibit the expected behavior.  Programs are
saved as CONCEPTUAL source code that can then be compiled with the
CONCEPTUAL compiler as per *note Compiling coNCePTuaL programs::.  _Print_
prints a graphical view of the program as it appears on screen.  (*Note
The latex_vis backend::, for a more sophisticated way to produce
graphical views of CONCEPTUAL programs.)

   The _Edit_ menu provides the usual _Cut_, _Copy_, _Paste_, and _Undo_
commands.

   _Options_->_Settings_ opens a dialog in the dialog panel for setting
the number of tasks in a task row. The default number of tasks in a
task row is 16.

   _Advanced_->_Add conditional_ adds a conditional statement to a
program at the current cursor position in the program panel.  A dialog
in the dialog panel will open for entering the conditional expression.
A placeholder expression `1 = 1' is set by default.

   _Advanced_->_Command line options_ opens a dialog in the dialog
panel for adding command-line options to a program.  A placeholder
`reps' (`number of repetitions') option is set by default when this
command is selected.


File: conceptual.info,  Node: Command bar,  Prev: Menu bar,  Up: The coNCePTuaL GUI

3.1.3 Command bar
-----------------

The following buttons appear on the command bar:

_Add Row_
     Insert a new, empty task row at the cursor position.

_Delete_
     Delete the selected components.

_Loop_
     Add a loop around the selected components.

_Measure_
     Add a measurement block around the selected components.

_Compute_
     Make the selected tasks "compute" or sleep for a length of time.

_Communicate_
     Add point-to-point communication between selected tasks (different
     task rows).  This can also be achieved by dragging an arrow from a
     source task to a target task.

_Wait_
     Make the selected tasks (same task row) or all tasks in the row
     above the cursor wait for all outstanding messages sent or received
     asynchronously to complete.

_Extend_
     Extend a communication or computation pattern across an entire task
     row.

_Synchronize_
     Synchronize the selected tasks (same task row) or all tasks in the
     task row above the cursor.

_Reduce_
     Reduce data from the selected tasks in one task row to the selected
     tasks in the next task row.  With no selection, reduce data from
     all tasks above the cursor to task 0 below the cursor.

_Multicast_
     Multicast data from the selected tasks in one task row to the
     selected tasks in the next task row.  With no selection, multicast
     data from task 0 above the cursor to all tasks below the cursor.

_Normalize_
     Put the program into standard form as it will appear when
     translated into CONCEPTUAL code.  After normalization, components
     are drawn as early in time as possible without changing the
     program's semantics.


File: conceptual.info,  Node: Compiling coNCePTuaL programs,  Next: Supplied backends,  Prev: The coNCePTuaL GUI,  Up: Usage

3.2 Compiling coNCePTuaL programs
=================================

The CONCEPTUAL compiler is called `ncptl' and is, by default, installed
into `/usr/local/bin'.  Executing `ncptl --help' produces a brief usage
string:

     Usage: ncptl [--backend=<string>] [--quiet] [--no-link | --no-compile]
              [--keep-ints] [--lenient] [--filter=<sed expr>] [--output=<file>]
              <file.ncptl> | --program=<program>
              [<backend-specific options>]

            ncptl --help

            ncptl [--backend=<string>] --help-backend

The usage string is followed by a list of installed backends.

   The following list describes each compiler option in turn:

`--backend' (abbreviation: `-b')
     Specify the module that CONCEPTUAL should use as the compiler
     backend.  `ncptl' must be told which backend to use with either `--backend=BACKEND'
     or by setting the environment variable `NCPTL_BACKEND' to the
     desired backend.  Running `ncptl --help' lists the available
     backends.  Most CONCEPTUAL backends are code generators.  For
     example, `c_mpi' causes `ncptl' to compile CONCEPTUAL programs
     into C using MPI as the communication library.  However, a backend
     need not generate code directly--or at all.  The `c_trace' backend
     (*note The c_trace backend::), for instance, supplements the code
     generated by another backend by adding tracing output to it.

     `ncptl' searches for backends first using `NCPTL_PATH', an
     environment variable containing a colon-separated list of
     directories (default: empty); then, in the directory in which
     CONCEPTUAL installed all of its Python files; and finally, in the
     default Python search path.  Non-directories (e.g., the `.zip'
     archives used in newer versions of Python) are not searched.

     If no backend is specified, `ncptl' runs the given CONCEPTUAL
     program through the lexer, parser, and semantic analyzer but does
     not generate an output file.

`--quiet' (abbreviation: `-q')
     The `--quiet' option tells `ncptl' and the chosen backend to
     output minimal status information.

`--no-link' (abbreviation: `-c')
     By default, `ncptl' instructs the backend to compile and link the
     user's CONCEPTUAL program into an executable file.  `--no-link'
     tells the backend to skip the linking step and produce only an
     object file.

`--no-compile' (abbreviation: `-E')
     By default, `ncptl' instructs the backend to compile and link the
     user's CONCEPTUAL program into an executable file.  `--no-compile'
     tells the backend to skip both the compilation and the linking
     step and to produce only a source file in the target language.

`--keep-ints' (abbreviation: `-K')
     CONCEPTUAL backends normally delete any files created as part of
     the compiling or linking process.  `--keep-ints' tells `ncptl' and
     the chosen backend to preserve their intermediate files.

`--lenient' (abbreviation: `-L')
     The `--lenient' option tells the compiler to permit certain
     constructs that would otherwise result in a compilation error.
     First, using the same command-line option (either the long or
     short variant) for two different variables normally generates an
     `Option OPT is multiply defined' error.  (*Note Command-line
     arguments::, for a description of how to declare command-line
     options in CONCEPTUAL.)  `--lenient' tells the CONCEPTUAL compiler
     to automatically rename duplicate options to avoid conflicts.
     Only the option strings can be renamed; the programmer must still
     ensure that the option variables are unique.  Second, using a
     variable without declaring it normally produces an error message
     at compile time.  Passing `--lenient' to `ncptl' tells the
     compiler to automatically generate a command-line option for each
     missing variable.  This is convenient when entering brief programs
     on the command line with `--program' (described below) as it can
     save a significant amount of typing.

`--filter' (abbreviation: `-f')
     The `--filter' option applies a `sed'-style substitution
     expression to the backend-translated code (e.g., a `.c' file
     output by the `c_udgram' backend or a `.tex' file output by the `latex_vis'
     backend) before the backend compiles it.  The `--filter' option
     can be used multiple times on the command line; filters are
     applied in the order specified.  Substitution expressions must be
     of the form `s/PATTERN/REPLACEMENT/FLAGS', although the `/'
     characters can be replaced by any other character.  PATTERN is a
     regular expression; REPLACEMENT is an optional replacement string;
     and, FLAGS is a sequence of zero or more modifiers from the set
     {`i', `l', `m', `s', `u', `x'}, as described in the Python Library
     Reference.  For example, `i' means to perform a case-insensitive
     substitution.  In addition, the `g' flag performs a global
     search-and-replace instead of replacing only the first occurrence
     of PATTERN.  An important difference between `--filter' and `sed'
     is that omitting the `g' flag instructs `--filter' to make at most
     one substitution _total_ while it instructs `sed' to make at most
     one substitution _per line_.

`--output' (abbreviation: `-o')
     `ncptl' normally writes its output to a file with the same base
     name as the input file (or `a.out' if the program was specified on
     the command line using `--program').  `--output' lets the user
     specify a file to which to write the generated code.

`--program' (abbreviation: `-p')
     Because CONCEPTUAL programs can be quite short `--program' enables
     a program to be specified in its entirety on the command line.  The
     alternative to using `--program' is to specify the name of a file
     containing a CONCEPTUAL program.  By convention, CONCEPTUAL
     programs have a `.ncptl' file extension.

`--help-backend' (abbreviation: `-H')
     Describe additional options that are meaningful only to the
     specified backend.  The `--backend' option must be used in
     conjunction with `--help-backend'.

   The following--to be entered without line breaks--is a sample
command line:

     ncptl --backend=c_mpi --output=sample.c --program='Task 0 sends a 0
       byte message to task 1 then task 1 sends a 0 byte message to task 0
       then task 0 logs elapsed_usecs/2 as "Startup latency (usecs)".'

   `ncptl' stops processing the command line at the first unrecognized
option it encounters.  That option and all subsequent
options--including those which `ncptl' would otherwise process--are
passed to the backend without interpretation by `ncptl'.  Furthermore,
the `--' (i.e., empty) option tells `ncptl' explicitly to stop
processing the command line at that point.  For example, in the command
`ncptl --backend=some_backend --lenient myprogram.ncptl -- --program', `ncptl'
will process the `--backend' and `--lenient' options but will pass `--program'
to the `some_backend' backend even though `ncptl' has its own `--program'
option.(1)

   ---------- Footnotes ----------

   (1) As an aside, `--help-backend' is essentially equivalent to `--
--help'; the `--help-backend' synonym is provided merely for
convenience.


File: conceptual.info,  Node: Supplied backends,  Next: Running coNCePTuaL programs,  Prev: Compiling coNCePTuaL programs,  Up: Usage

3.3 Supplied backends
=====================

The CONCEPTUAL 1.2 distribution includes the following compiler
backends:

`c_seq'
     Generate ANSI C code with no communication support.

`c_mpi'
     Generate ANSI C code with calls to the MPI library for
     communication.

`c_udgram'
     Generate ANSI C code that communicates using Unix-domain (i.e.,
     local to a single machine) datagram sockets.

`c_trace'
     Instrument a C-based backend either to include a call to `fprintf()'
     before every program event or to utilize the `curses' library to
     display graphically the execution of a selected task.

`c_profile'
     Instrument a C-based backend either to write event timings and
     tallies to each log file or to the standard error device if the
     CONCEPTUAL program doesn't use a log file.

`interpret'
     Interpret a CONCEPTUAL program, simulating any number of
     processors and checking for common problems such as deadlocks and
     mismatched sends and receives.

`stats'
     Output statistics of a program's execution--message tallies, byte
     counts, communication peers, network bisection crossings, event
     tallies, etc.

`picl'
     Output in the PICL trace format a logical-time trace of a
     CONCEPTUAL program's communication pattern.

`latex_vis'
     Use LaTeX to produce an Encapsulated PostScript visualization of a
     program's communication pattern.

`dot_ast'
     Output a program's parse tree in the Graphviz dot format.

   CONCEPTUAL employs a highly modular software structure for its
backends.  Many of the backends listed above are built atop other
backends.  The following figure illustrates the current set of
dependencies:


      +--------------------------+
      |  c_trace and c_profile   |
      +-------+----------+-------+-------+------+-----------+
      | c_mpi | c_udgram | c_seq | stats | picl | latex_vis |
      +-------+----------+-------+-------+------+-----------+---------+
      |        c_generic         |    interpret             | dot_ast |
      +--------------------------+--------------------------+---------+

Some dependencies are defined as a static characteristic of a backend.
For example, the `c_mpi' backend is hardwired to derive some of its
functionality from `c_generic'.  Other dependencies are determined
dynamically.  For example, the `c_trace' backend must be instructed to
derive its functionality from one of `c_profile', `c_mpi', `c_udgram',
or `c_seq'.  (*Note The c_trace backend::, for more information on the `c_trace'
backend.)

   Except for `c_generic', all of the compiler backends are described
in turn in the following sections.  The `c_generic' backend is unique
because it is used exclusively to construct C-based backends; it does
nothing by itself.  *Note Backend creation::, for a detailed
description of `c_generic'.

   Each backend accepts a `--help' option that explains the backend's
command-line options.  The easiest way to request help from a specific
backend is with `ncptl --backend=BACKEND -- --help'.  The empty `--'
option, mentioned in *note Compiling coNCePTuaL programs::, prevents
the compiler from intercepting `--help' and providing the standard,
backend-independent information.

* Menu:

* The c_seq backend::           ANSI C, sequential code only
* The c_mpi backend::           ANSI C + MPI
* The c_udgram backend::        ANSI C + Unix-domain datagram sockets
* The c_trace backend::         Instrument any of the above with tracing output
* The c_profile backend::       Profile events in any of the above
* The interpret backend::       Interpret coNCePTuaL programs
* The stats backend::           Report statistics on a program's execution
* The picl backend::            Trace a coNCePTuaL program's logical execution
* The latex_vis backend::       Visualize a communication pattern
* The dot_ast backend::         Graphviz DOT format (show abstract-syntax tree)


File: conceptual.info,  Node: The c_seq backend,  Next: The c_mpi backend,  Prev: Supplied backends,  Up: Supplied backends

3.3.1 The `c_seq' backend
-------------------------

The `c_seq' backend is intended primarily to provide backend developers
with a minimal C-based backend that can be used as a starting point for
creating new backends.  *Note Backend creation::, explains how to write
backends.


File: conceptual.info,  Node: The c_mpi backend,  Next: The c_udgram backend,  Prev: The c_seq backend,  Up: Supplied backends

3.3.2 The `c_mpi' backend
-------------------------

The `c_mpi' backend is CONCEPTUAL's workhorse.  It generates parallel
programs written in ANSI C that communicate using the industry-standard MPI
messaging library.

   By default, `c_mpi' produces an executable program that can be run
with `mpirun', `prun', `pdsh', or whatever other job-launching program
is normally used to run MPI programs.  When `ncptl' is run with the `--no-link'
option, `c_mpi' produces an object file that needs to be linked with
the appropriate MPI library.  When `ncptl' is run with the `--no-compile'
option, `c_mpi' outputs ANSI C code that must be both compiled and
linked.

   `c_mpi' honors the following environment variables when compiling
and linking C+MPI programs: `MPICC', `MPICPPFLAGS', `MPICFLAGS', `MPILDFLAGS', `MPILIBS'.
If any of these variables is not found in the environment, `c_mpi' will
use the value specified/discovered at configuration time (*note
configure::).  `MPICC' defaults to the value of `CC'; `MPICFLAGS'
defaults to the value of `CFLAGS'; the remaining variables are appended
respectively to `CPPFLAGS', `LDFLAGS', and `LIBS'.

   The following is a complete list of MPI functions employed by the `c_mpi'
backend: `MPI_Allreduce()', `MPI_Barrier()', `MPI_Bcast()', `MPI_Comm_rank()', `MPI_Comm_size()', `MPI_Comm_split()', `MPI_Errhandler_create()', `MPI_Errhandler_set()', `MPI_Finalize()', `MPI_Init()', `MPI_Irecv()', `MPI_Isend()', `MPI_Recv()', `MPI_Reduce()', `MPI_Send()', `MPI_Ssend()', `MPI_Waitall()'.
In addition, if `./configure ' is passed the `--with-mpi-wtime' option
as described in *note configure::, then _all_ backends that utilize the
CONCEPTUAL run-time library, including `c_mpi', will use `MPI_Wtime()'
for taking performance measurements.

* Menu:

* Command-line options for c_mpi::  Control over the generated C+MPI code
* Implementation of reductions::  How c_mpi implements the REDUCE statement


File: conceptual.info,  Node: Command-line options for c_mpi,  Next: Implementation of reductions,  Prev: The c_mpi backend,  Up: The c_mpi backend

Command-line options for `c_mpi'
................................

When `ncptl' is passed `--backend=c_mpi' as a command-line option, `c_mpi'
processes the following backend-specific command-line options:

`--ssend'
     In the generated code, `MPI_Isend()' and `MPI_Irecv()' are used
     for asynchronous communication and `MPI_Send()' and `MPI_Recv()'
     are normally used for synchronous communication.  However, the `c_mpi'-specific
     compiler option `--ssend' instructs `c_mpi' to replace all calls
     to `MPI_Send()' in the generated code with calls to `MPI_Ssend()', MPI's
     synchronizing send function.  A program's log files indicate
     whether the program was built to use `MPI_Send()' or `MPI_Ssend()'.

`--reduce=MPI_OP'
     The default reduction operation is `MPI_SUM' but a different
     operation can be specified using the `c_mpi'-specific `--reduce'
     compiler option.  A program's log files indicate the reduction
     operation that was used.


File: conceptual.info,  Node: Implementation of reductions,  Prev: Command-line options for c_mpi,  Up: The c_mpi backend

Implementation of reductions
............................

The `c_mpi' backend implements the CONCEPTUAL `REDUCE' statement (*note
Reducing::) as follows.  Many-to-one reductions are implemented with a
single call to `MPI_Reduce()'.  Many-to-many reductions in which the
sources exactly match the targets are implemented with a single call to `MPI_Allreduce()'.
All other reductions are implemented by calling `MPI_Reduce()' to reduce
the data to the first target task then `MPI_Bcast()' to distribute the
reduced data to the remaining targets.

   Because MPI requires that the source and target buffers in an `MPI_Reduce()'
or `MPI_Allreduce()' be different, the `c_mpi' backend utilizes _two_
buffers when `UNIQUE' (*note Unique messages::) is specified.  It
utilizes two _adjacent_ buffers when `FROM BUFFER' or `INTO BUFFER'
(*note Buffer control::) is specified.


File: conceptual.info,  Node: The c_udgram backend,  Next: The c_trace backend,  Prev: The c_mpi backend,  Up: Supplied backends

3.3.3 The `c_udgram' backend
----------------------------

CONCEPTUAL program development on a workstation is facilitated by the `c_udgram'
backend.  `c_udgram' runs on only a single machine but, unlike `c_seq',
supports all of CONCEPTUAL's communication statements.  Communication
is performed over Unix-domain datagram sockets.  Unix-domain datagrams
are reliable and guarantee order (unlike UDP/IP datagrams) but have a
maximum packet size.  `c_udgram' backend write this maximum to every
log file and automatically packetizes larger messages.

   By default, `c_udgram' produces an executable program that can be
run directly from the command line.  When `ncptl' is run with the `--no-link'
option, `c_udgram' produces an object file that needs to be linked with
the appropriate sockets library (on systems that require a separate
library for socket calls).  When `ncptl' is run with the `--no-compile'
option, `c_udgram' outputs ANSI C code that must be both compiled and
linked.  Like all C-based backends, `c_udgram' honors the `CC', `CPPFLAGS', `LDFLAGS',
and `LIBS' environment variables when compiling and linking.  Values
not found in the environment are taken from those specified/discovered
at configuration time (*note configure::).

   In addition to supporting the default set of command-line options,
programs generated using the `c_udgram' backend further support a `--tasks'
option that designates the number of tasks to use:

       -T, --tasks=<number>        Number of tasks to use [default: 1]

`c_udgram' programs spawn one OS-level process for each task in the
program.  They also create a number of sockets in the current directory
named `c_udgram_<TAG>'.  These are automatically deleted if the program
exits cleanly but will need to be removed manually in the case that the
program is killed by an non-trappable signal.


File: conceptual.info,  Node: The c_trace backend,  Next: The c_profile backend,  Prev: The c_udgram backend,  Up: Supplied backends

3.3.4 The `c_trace' backend
---------------------------

While most CONCEPTUAL backends are code generators, the `c_trace'
backend adds tracing code to the code produced by a code-generating
backend.  `c_trace' is useful as a debugging aid and as a means to help
understand the control flow of a CONCEPTUAL program.

* Menu:

* Command-line options for c_trace::  Compiling with tracing enabled
* Default c_trace tracing::     Tracing to the standard error device
* c_trace tracing with curses::  Interactively tracing a live program
* Offline tracing with curses::  Playing back trace data interactively


File: conceptual.info,  Node: Command-line options for c_trace,  Next: Default c_trace tracing,  Prev: The c_trace backend,  Up: The c_trace backend

Command-line options for `c_trace'
..................................

When `ncptl' is passed `--backend=c_trace' as a command-line option, `c_trace'
processes the following backend-specific command-line options:

`--trace=BACKEND'
     Specify a backend that will produce C code for `c_trace' to trace.
     The `--trace' option is required to use `c_trace'; `c_trace' will
     issue an error message if `--trace' is not specified.  The
     restrictions on BACKEND are that it must produce C code and must
     be derived from the `c_generic' backend (*note Backend
     creation::).  Improper backends cause `c_trace' to abort
     abnormally.

`--curses'
     Instead of injecting `fprintf()' statements into the generated
     C code, inject calls to the `curses' (or `ncurses') library to
     show graphically the line of code currently executing on a given
     processor.


File: conceptual.info,  Node: Default c_trace tracing,  Next: c_trace tracing with curses,  Prev: Command-line options for c_trace,  Up: The c_trace backend

Default `c_trace' tracing
.........................

Without `--curses', `c_trace' alters the generated C code to write data
like the following to the standard error device:

     [TRACE] phys: 1 | virt: 1 | action: RECV | event: 1 / 44001 | lines: 18 - 18
     [TRACE] phys: 0 | virt: 0 | action: RESET | event: 1 / 88023 | lines: 17 - 17
     [TRACE] phys: 0 | virt: 0 | action: SEND | event: 2 / 88023 | lines: 18 - 18
     [TRACE] phys: 0 | virt: 0 | action: RECV | event: 3 / 88023 | lines: 19 - 19
     [TRACE] phys: 1 | virt: 1 | action: SEND | event: 2 / 44001 | lines: 19 - 19
     [TRACE] phys: 1 | virt: 1 | action: RECV | event: 3 / 44001 | lines: 18 - 18
     [TRACE] phys: 0 | virt: 0 | action: CODE | event: 4 / 88023 | lines: 20 - 21
     [TRACE] phys: 0 | virt: 0 | action: RESET | event: 5 / 88023 | lines: 17 - 17
     [TRACE] phys: 0 | virt: 0 | action: SEND | event: 6 / 88023 | lines: 18 - 18
     [TRACE] phys: 0 | virt: 0 | action: RECV | event: 7 / 88023 | lines: 19 - 19
                                            .
                                            .
                                            .

The format is designed to be easy to read and easy for a program to
parse.  Each line of trace data begins with the string `[TRACE]' and
lists the (physical) processor number, the (virtual) task ID, the
action (a.k.a., event type) that is about to be performed, the current
event number and total number of events that will execute on the given
processor, and the range of lines of source code to which the current
event corresponds.  An "event" corresponds more-or-less to a statement
in the CONCEPTUAL language.(1)  Loops are unrolled at initialization
time and therefore produce no events.  *note Event types::, lists and
briefly describes the various event types.

   ---------- Footnotes ----------

   (1) A more precise correspondence is to a <SIMPLE_STMT> in the
formal grammar presented in *note Grammar::.


File: conceptual.info,  Node: c_trace tracing with curses,  Next: Offline tracing with curses,  Prev: Default c_trace tracing,  Up: The c_trace backend

`c_trace' tracing with `curses'
...............................

The `--curses' option enables a more interactive tracing environment.
Generated programs must be linked with the `curses' (or compatible,
such as `ncurses') library.  The resulting executable supports the
following additional command-line options:

`-D', `--delay=NUMBER'
     delay in milliseconds after each screen update (`0'=no delay)

`-M', `--monitor=NUMBER'
     processor number to monitor

`-B', `--breakpoint=NUMBER'
     source line at which to enter single-stepping mode (`-1'=none;
     `0'=first event)

   When the program is run it brings up a screen like the following:

      1.  # Determine computational "noise"
      2.
      3.  Require language version "1.2".
      4.
      5.  accesses is "Number of data accesses to perform" and comes from
      6.    "--accesses" or "-a" with default 500000.
      7.
      8.  trials is "Number of timings to take" and comes from "--timings" or
      9.    "-t" with default 1000.
     10.
     11.  For trials repetitions {
     12.    all tasks reset their counters then
     13.    all tasks touch a 1 word memory region accesses times with stride 0 w
     14.    all tasks log a histogram of elapsed_usecs as "Actual time (usecs)"
     15.  }


     Phys: 0  Virt: 0  Action: RESET    Event:    1/3001

The program displays its source code (truncated vertically if too tall
and truncated horizontally if too wide) at the top of the screen and a
status bar at the bottom of the screen.  As the program executes, a
cursor indicates the line of source code that is currently executing.
Likewise, the status bar updates dynamically to indicate the
processor's current task ID, action, and event number.  In `curses'
mode, the program's standard output (*note Writing to standard
output::) is suppressed so as not to disrupt the trace display.

   Programs traced with `c_trace' and the `--curses' option are made
interactive and support the following (case-insensitive) keyboard
commands:

`S'
     Enable single-stepping mode.  While single-stepping mode is enabled
     the traced processor will execute only one event per keystroke from
     the user.

`SPACE'
     Disable single-stepping mode.  The program executes without further
     user intervention.

`D'
     Delete the breakpoint.

`Q'
     Quit the program.  The log file will indicate that the program did
     not run to completion.

All other keystrokes cause the program to advance to the next event
immediately.

   A single breakpoint can be set using the program's `-B' or `--breakpoint'
command-line option.  Whenever the monitored processor reaches the
source-code line at which a breakpoint has been set, it enters
single-stepping mode exactly as if `S' were pressed.  Setting a
breakpoint at line 0 tells the program to begin single-stepping as soon
as the program begins.  Note that only lines corresponding to
CONCEPTUAL events can support breakpoints.


File: conceptual.info,  Node: Offline tracing with curses,  Prev: c_trace tracing with curses,  Up: The c_trace backend

Offline tracing with `curses'
.............................

The `c_trace' backend can be told to trace by writing messages to the
standard error device (*note Default c_trace tracing::) or by employing
an interactive display (*note c_trace tracing with curses::).  These
two alternatives can be combined to support offline tracing of a
CONCEPTUAL program.  The idea is to compile the program without the `--curses'
option.  When running the program, the standard-error output should be
redirected to a file.  The `ncptl-replaytrace' utility, which comes
with CONCEPTUAL, can then be used to play back the program's execution
by reading and displaying the file of redirected trace data.

   `ncptl-replaytrace' accepts the following command-line options,
which correspond closely to those accepted by a program compiled with
the `--curses' option to `c_trace' (*note c_trace tracing with
curses::):

`--trace=FILE'
     Specify a file containing redirected trace data.  FILE defaults to
     the standard input device.

`--delay=NUMBER'
     Specify the delay in milliseconds after each screen update (`0'=no
     delay).

`--monitor=PROCESSOR'
     Specify the processor number to monitor.  PROCESSOR defaults to
     `0'.

`--breakpoint=LINE'
     Specify a line of source code at which to enter single-stepping
     mode (`-1'=none; `0'=first event).

In addition, `ncptl-replaytrace' requires that the CONCEPTUAL
source-code file be specified on the command line, as the source code
is not included in the trace data.

   The interactive display presented by the offline `ncptl-replaytrace'
tool is nearly identical to that presented by a program compiled with
the `--curses' option to `c_trace'.  *Note c_trace tracing with
curses::, for a usage description.


File: conceptual.info,  Node: The c_profile backend,  Next: The interpret backend,  Prev: The c_trace backend,  Up: Supplied backends

3.3.5 The `c_profile' backend
-----------------------------

Profiling communication events can help explain surprising performance
measurements.  A profiling tool which automatically instruments
CONCEPTUAL programs is simpler than manually adding `LOGS' or `OUTPUTS'
statements to an existing program.

   Like the `c_trace' backend (*note The c_trace backend::), the `c_profile'
backend adds code to that produced by other backends.  The `c_profile'
backend accepts a single--mandatory--command-line argument, `--profile=BACKEND',
which designates a target backend to use.  The restrictions on BACKEND
are that it must produce C code and must be derived from the `c_generic'
backend (*note Backend creation::).  Improper backends cause `c_profile'
to abort abnormally.

   If the profiled CONCEPTUAL program produces log files, the log files
will include profiling information in the epilogue, one line for each
event type that was used at least once by the corresponding process.
(*note Event types::, lists and briefly describes the various event
types.)  Each line names an event and presents the total number of
microseconds spent processing that event, a tally of the number of
times that event type was executed, and the quotient of those two
numbers.  In addition, the amount of memory used to store the event
list is also logged.

   For example, the following extract from a log file indicates that the
process which wrote it spent a total of 6.7s processing 22,000 `RECV'
events (which includes waiting time) for an average of 303.5us per
`RECV' event:

     # Profile of SEND (microseconds, count, average): 5267469 22001 239.4
     # Profile of RECV (microseconds, count, average): 6676521 22000 303.5
     # Profile of REPEAT (microseconds, count, average): 11985167 1 11985167.0
     # Profile of NEWSTMT (microseconds, count, average): 43 1 43.0
     # Profile of CODE (microseconds, count, average): 5516 1 5516.0
     # Profile of event memory: 528 bytes (6 events * 88 bytes/event)

   Each `REPEAT' event includes the time for all of the events that it
repeats.

   Although the preceding log-file excerpt indicates that a total of
22001+22000+1+1+1 = 44004 events were executed, the `event memory' line
clarifies that the event list contained only 6 unique events and
therefore required only 528 bytes of memory.

   Profiled programs that do not produce log files write profiling
information to the standard error device.  Because all processes may
share a single standard error device, each line of output is preceded
by a processor ID as in the following example:

     1 SEND 5527125 22000 251.2
     1 RECV 6322699 22001 287.4
     1 REPEAT 11894523 1 11894523.0
     1 event-memory 352 4 88
     0 SEND 5267469 22001 239.4
     0 RECV 6676521 22000 303.5
     0 REPEAT 11985167 1 11985167.0
     0 event-memory 352 4 88

The columns are PROCESSOR ID, EVENT, TOTAL MICROSECONDS, TALLY, and
AVERAGE MICROSECONDS except when EVENT is `event-memory' in which case
the columns are PROCESSOR ID, `event-memory', TOTAL BYTES, NUMBER OF
EVENTS, and BYTES PER EVENT.  The intention is for the output to be
easily parseable using tools such as `awk'.


File: conceptual.info,  Node: The interpret backend,  Next: The stats backend,  Prev: The c_profile backend,  Up: Supplied backends

3.3.6 The `interpret' backend
-----------------------------

Like the `c_udgram' backend (*note The c_udgram backend::), the `interpret'
backend is designed to help programmers ensure the correctness of
CONCEPTUAL code.  The `interpret' backend does not output code.  As its
name implies, `interpret' is an _interpreter_ of CONCEPTUAL programs
rather than a compiler.  `interpret' exhibits the following salient
features:

  1. Some programs run faster than with a compiler because the
     interpreter does not actually send messages.  `interpret' merely
     simulates communication.  It also skips over statements such as `COMPUTES'/`SLEEPS'
     (*note Delaying execution::) and `TOUCHES' (*note Touching
     memory::).

  2. `interpret' can simulate massively parallel computer systems from
     a single process.

  3. As `interpret' runs it checks for common communication errors such
     as deadlocks, asynchronous sends and receives that are never
     completed, and blocking operations left over at the end of the
     program (which would likely cause hung tasks under a real
     messaging layer).

The drawbacks are that `interpret' is slow when interpreting
control-intensive programs and that timing measurements are not
indicative of any real network.  (The `interpret' backend utilizes
logical time rather than physical time.)  `interpret' is intended
primarily as a development tool for helping ensure the correctness of
CONCEPTUAL programs.

   The `interpret' backend accepts all of the command-line options
described in *note Running coNCePTuaL programs::, plus the following
three options:

       -H, --hierarchy=<string>     Latency hierarchy as a comma-separated list
                                    of task_factor:latency_delta pairs [default:
                                    "tasks:1"]
       -M, --mcastsync=<number>     Perform an implicit synchronization after a
                                    multicast (0=no; 1=yes) [default: 0]
       -T, --tasks=<number>         Number of tasks to use [default: 1]

   Normally, the `interpret' backend assigns unit latency to every
communication operation.  The `--hierarchy' option can make
communication with distant tasks observe more latency than
communication with nearby tasks.  An explanation of the argument to `--hierarchy'
is presented in *note Task latency hierarchies::.

   A multicast operation (*note Multicasting::) is normally treated as
multiple point-to-point operations with the same send time.  The `--mcastsync'
option instructs the `interpret' backend to perform an implicit barrier
synchronization at the end of the multicast.

   The `--tasks' option specifies the number of tasks to simulate.
Because this number can be quite large the `NCPTL_LOG_ONLY' environment
variable (*note Environment Variables::) may be used to limit the set
of processors that are allowed to create log files.  That way, if
task 0 is the only task out of thousands that logs any data, `NCPTL_LOG_ONLY'
can specify that only one log file will be produced, not thousands.  By
default, all processors create a log file.

   All other command-line arguments are passed to the program being
interpreted.

   The `--output' option described in *note Compiling coNCePTuaL
programs::, has special meaning to the `interpret' backend.  When `--output'
is used, `interpret' dumps a list of events to the specified file after
a successful run.  For example, the CONCEPTUAL program `ALL TASKS t
ASYNCHRONOUSLY SEND A 384 BYTE MESSAGE TO TASK t XOR 2 THEN ALL TASKS
AWAIT COMPLETION' results in the following event dump:

     Task 0 posted a NEWSTMT at time 0 and completed it at time 0
     Task 0 posted a RECEIVE at time 0 and completed it at time 0
     Task 0 posted a SEND at time 1 and completed it at time 1
     Task 0 posted a WAIT_ALL at time 2 and completed it at time 2
     Task 1 posted a NEWSTMT at time 0 and completed it at time 0
     Task 1 posted a RECEIVE at time 0 and completed it at time 0
     Task 1 posted a SEND at time 1 and completed it at time 1
     Task 1 posted a WAIT_ALL at time 2 and completed it at time 2
     Task 2 posted a NEWSTMT at time 0 and completed it at time 0
     Task 2 posted a RECEIVE at time 0 and completed it at time 0
     Task 2 posted a SEND at time 1 and completed it at time 1
     Task 2 posted a WAIT_ALL at time 2 and completed it at time 2
     Task 3 posted a NEWSTMT at time 0 and completed it at time 0
     Task 3 posted a RECEIVE at time 0 and completed it at time 0
     Task 3 posted a SEND at time 1 and completed it at time 1
     Task 3 posted a WAIT_ALL at time 2 and completed it at time 2

   As an example of the `interpret' backend's usage, here's how to
simulate 100,000 processors communicating in a simple ring pattern:

     % ncptl --backend=interpret --lenient --program='All tasks t send
         nummsgs 1024 gigabyte messages to task t+1 then task num_tasks-1
         sends nummsgs 1024 gigabyte messages to task 0.' --tasks=100000
         --nummsgs=5

The preceding command ran to completion in under 5 minutes on a 1.5GHz
Xeon uniprocessor workstation--not too bad considering that
488 petabytes of data are transmitted on the program's critical path.

   The `interpret' backend is especially useful for finding
communication-related program errors:

     % ncptl --backend=interpret --quiet --program='All tasks t send
         a 10 doubleword message to task (t+1) mod num_tasks.' --tasks=3
     <command line>: The following tasks have deadlocked: 0 --> 2 --> 1
         --> 0

Deadlocked tasks are shown with `-->' signifying "is blocked waiting
for".  In the preceding example, all receives are posted before all
sends.  Hence, task 0 is blocked waiting for task 2 to send it a
message.  Task 2, in turn, is blocked waiting for task 1 to sent it a
message.  Finally, task 1 is blocked waiting for task 0 to send it a
message, which creates a cycle of dependencies.

   The `interpret' backend can find other errors, as well:

     % ncptl --backend=interpret --quiet --program='All tasks t
         asynchronously send a 10 doubleword message to task (t+1) mod
         num_tasks.' --tasks=4
     <command line>: The program ended with the following leftover-event
         errors:
        * Task 0 posted an asynchronous RECEIVE that was never waited for
        * Task 0 posted an asynchronous SEND that was never waited for
        * Task 0 sent a message to task 1 that was never received
        * Task 1 posted an asynchronous RECEIVE that was never waited for
        * Task 1 posted an asynchronous SEND that was never waited for
        * Task 1 sent a message to task 2 that was never received
        * Task 2 posted an asynchronous RECEIVE that was never waited for
        * Task 2 posted an asynchronous SEND that was never waited for
        * Task 2 sent a message to task 3 that was never received
        * Task 3 posted an asynchronous RECEIVE that was never waited for
        * Task 3 posted an asynchronous SEND that was never waited for
        * Task 3 sent a message to task 0 that was never received

(A message received `ASYNCHRONOUSLY' is not considered received until
after the corresponding `AWAITS COMPLETION'; hence, all of the `was
never received' messages listed above.)

     % ncptl --backend=interpret --quiet --program='Task 0 sends a 40
         kilobyte message to unsuspecting task 1 then task 0 receives a 40
         kilobyte message from task 1.' --tasks=2
     <command line>: The program ended with the following leftover-event
         errors:
        * Task 0 sent a message to task 1 that was never received
        * Task 1 terminated before satisfying task 0's RECEIVE operation

   In short, it is well worth testing the correctness of new CONCEPTUAL
programs with `interpret' before performing timing runs with one of the
message-passing backends.

* Menu:

* Task latency hierarchies::    Specifying latency as a function of distance


File: conceptual.info,  Node: Task latency hierarchies,  Prev: The interpret backend,  Up: The interpret backend

Task latency hierarchies
........................

The `interpret' backend normally simulates a flat network in which
communication between any two tasks takes unit latency.  However, the `--hierarchy'
option lets one specify a hierarchy of latencies: latency l1 within a
set of t1 tasks, latency l1+l2 within a set of t1*t2 tasks, latency
l1+l2+l3 within a set of t1*t2*t3 tasks, and so forth.

   The argument to `--hierarchy' is a comma-separated list of <TASK
FACTOR:LATENCY DELTA> pairs.  The LATENCY DELTA component is optional
and defaults to `1'.  If the list ends with `...', the final <TASK
FACTOR:LATENCY DELTA> pair is repeatedly indefinitely.  All TASK FACTOR
values must be positive integers, and all LATENCY DELTA values must be
nonnegative integers.

   As an example, `--hierarchy=4' (or `--hierarchy=4:1') partitions the
program's tasks into sets of four with one unit of additional latency
to communicate with another set.  Hence, tasks 0, 1, 2, and 3 can
communicate with each other in unit time, as can tasks 4, 5, 6, and 7.
However, communication between any task in the first set and any task
in the second set (e.g., between tasks 3 and 4) takes an additional
unit of time for a total latency of two units.

   As a more complex example, consider a cluster of symmetric
multiprocessors (SMPs) in which each SMP comprises two processor
sockets with each socket containing a quad-core processor (four CPUs).
Further assume that the SMPs are networked together via a fat-tree
network consisting of 12-port switches.  In this example, let's say
that it takes unit latency to communicate within a socket, two units of
latency to communicate with another socket in the same SMP, and an
additional three units of latency to traverse each switch.  This
configuration can be specified to the `interpret' backend as `--hierarchy="4:1, 2:1, 12:3, 12:6, ..."'
(or simply `--hierarchy=4,2,12:3,12:6, ...').  With this setting,
task 0, for instance, communicates with tasks 1-3 in 1 time unit, with
tasks 4-7 in 2 time units, with tasks 8-95 in 5 time units (one switch
crossing), with tasks 96-1151 in 11 time units (three switch
crossings--a level 0 switch, a level 1 switch, and another level 0
switch), with tasks 1152-13,823 in 17 time units (five switch
crossings--of switch levels 0, 1, 2, 1, and 0), and so forth.


File: conceptual.info,  Node: The stats backend,  Next: The picl backend,  Prev: The interpret backend,  Up: Supplied backends

3.3.7 The `stats' backend
-------------------------

The `stats' backend outputs various statistics about a CONCEPTUAL
program.  It can be used to help verify a program's correctness or
merely to search for interesting patterns within a complex
communication pattern.  The following is some sample output from a
program compiled with `stats':

     Execution parameters
     --------------------
       Number of processors:          16
       Random-number seed:   -1727114895
       Command line:         ncptl --backend=stats hycom.ncptl --xdim=4 --ydim=4 --tasks=16 --iter=10
       Timestamp:            Wed Jan  4 17:32:04 2006

     Message traffic
     ---------------
       Total messages sent:           710
       Total bytes sent:          1454080
       Unique message sizes sent: 2048

     Per-processor message traffic
     -----------------------------
       Processors sending a total of 61440 bytes:    0
       Processors sending a total of 81920 bytes:    1-3, 12-15
       Processors sending a total of 102400 bytes:   4-11
       Processors receiving a total of 61440 bytes:  1-3, 13-15
       Processors receiving a total of 81920 bytes:  5-7, 9-11
       Processors receiving a total of 122880 bytes: 12
       Processors receiving a total of 143360 bytes: 4, 8
       Processors receiving a total of 184320 bytes: 0
       Processors sending a total of 30 messages:    0
       Processors sending a total of 40 messages:    1-3, 12-15
       Processors sending a total of 50 messages:    4-11
       Processors receiving a total of 30 messages:  1-3, 13-15
       Processors receiving a total of 40 messages:  5-7, 9-11
       Processors receiving a total of 60 messages:  12
       Processors receiving a total of 70 messages:  4, 8
       Processors receiving a total of 90 messages:  0

     Processor SEND-event peers
     --------------------------
       Processors posting SEND events to offsets {-12, -4, +1, +3}:    12
       Processors posting SEND events to offsets {-8, -4, +1, +3, +4}: 8
       Processors posting SEND events to offsets {-4, -3, -1}:         15
       Processors posting SEND events to offsets {-4, -3, -1, +4}:     7, 11
       Processors posting SEND events to offsets {-4, -2, -1, +1}:     14
       Processors posting SEND events to offsets {-4, -2, -1, +1, +4}: 6, 10
       Processors posting SEND events to offsets {-4, -1, +1}:         13
       Processors posting SEND events to offsets {-4, -1, +1, +4}:     5, 9
       Processors posting SEND events to offsets {-4, +1, +3, +4}:     4
       Processors posting SEND events to offsets {-3, -1, +4}:         3
       Processors posting SEND events to offsets {-2, -1, +1, +4}:     2
       Processors posting SEND events to offsets {-1, +1, +4}:         1
       Processors posting SEND events to offsets {+1, +3, +4}:         0

     Network bisection crossings
     ---------------------------
       Bisection messages:    100
       Bisection bytes:    204800

     Event tallies
     -------------
       Total number of LOG events:       10
       Total number of NEWSTMT events:   48
       Total number of RECEIVE events:  710
       Total number of RESET events:     10
       Total number of SEND events:     710
       Total number of WAIT_ALL events: 480

     Per-processor event sets
     ------------------------
       Processors executing only {LOG, NEWSTMT, RECEIVE, RESET, SEND, WAIT_ALL}: 0
       Processors executing only {NEWSTMT, RECEIVE, SEND, WAIT_ALL}:             1-15

     Per-processor event tallies
     ---------------------------
       Processors executing 10 LOG events:      0
       Processors executing 3 NEWSTMT events:   0-15
       Processors executing 30 RECEIVE events:  1-3, 13-15
       Processors executing 40 RECEIVE events:  5-7, 9-11
       Processors executing 60 RECEIVE events:  12
       Processors executing 70 RECEIVE events:  4, 8
       Processors executing 90 RECEIVE events:  0
       Processors executing 10 RESET events:    0
       Processors executing 30 SEND events:     0
       Processors executing 40 SEND events:     1-3, 12-15
       Processors executing 50 SEND events:     4-11
       Processors executing 30 WAIT_ALL events: 0-15

   `stats' is derived from `interpret' (*note The interpret backend::)
and therefore supports the `interpret' backend's `--tasks' and `--mcastsync'
options as well as the standard options described in *note Running
coNCePTuaL programs::.  However, because `stats' does not produce log
files, the `--logfile' option is absent.  `stats' additionally supports
the following three command-line options:

       -E, --expand-lists=<number>     0=collapse lists of numbers into ranges;
                                       1=show all numbers [default: 0]
       -F, --format=<string>           Output format, either "text", "excelcsv",
                                       or "sep:<string>" [default: "text"]
       -X, --exclude=<string>          Name of a category or individual field to
                                       exclude from output [default: ""]

   The `--expand-lists' option tells the `stats' backend whether it
should output sets of numbers as comma-separated ranges
(e.g., `1-3, 12-15') or as individual numbers (e.g., `1, 2, 3, 12, 13,
14, 15').  The `--format' option specifies the output format.  By
default, it outputs human-readable text, as shown in the above example.
However, `--format=excelcsv' outputs the data in comma-separated value
format suitable for loading directly into Microsoft Excel.  A more
general form of computer-parseable output is specified with the `sep'
sub-option, which takes a separator string as a sub-option.  For
example, `--format=sep:"#"' produces output like the following:

     "Event tallies"#"Total number of LOG events"#10
     #"Total number of NEWSTMT events"#48
     #"Total number of RECEIVE events"#710
     #"Total number of RESET events"#10
     #"Total number of SEND events"#710
     #"Total number of WAIT_ALL events"#480

The difference between `--format=excelcsv' and `--format=sep:","' is
that the former employs a number of bizarre special cases when quoting
strings so as to coerce Excel into properly processing the file.  The
latter simply inserts a comma between columns with no special string
processing other than preceding the characters `"' and `\' with `\'.

   The `stats' backend issues a `Column separator SEP appears within a
field' warning if the separator string appears in any of the output
fields.  (Note that it will always appear in the `Command line' line
because the `--format' line is specified on the command line.)  The
idea is to warn the user that automatic parsing of the output
(e.g., using `awk') may need to be careful when processing lines
containing such fields.

   As can be seen from the sample output, `stats' outputs a lot of
information.  The `--exclude' option--which can be specified repeatedly
on the command line--provides the user with fine-grained control over
which output to suppress.  For example, `--exclude="Total number of NEWSTMT events"'
tells `stats' not to output the line with that key.  Similarly, `--exclude="Processor SEND-event peers"'
eliminates an entire category of information.


File: conceptual.info,  Node: The picl backend,  Next: The latex_vis backend,  Prev: The stats backend,  Up: Supplied backends

3.3.8 The `picl' backend
------------------------

PICL, the Portable Instrumented Communication Library, defines a trace
file format that records an execution trace of a message-passing
application.  In particular, MPICL is an MPI-specific implementation of PICL
that facilitates instrumenting MPI programs.  Normally, one passes a PICL
file to a performance-visualization tool such as ParaGraph, which
renders the trace data using any of a number of graphical views.

   Although MPICL is compatible with the `c_mpi' backend (*note The
c_mpi backend::), CONCEPTUAL provides a `picl' backend which produces PICL
files directly.  The intention is to use the PICL format to represent
an idealized view of a communication pattern rather than to store a
time-sensitive execution trace.  For example, timing events recorded by `picl'
occur at logical times instead of physical times and statements such as `COMPUTES'/`SLEEPS'
(*note Delaying execution::) and `TOUCHES' (*note Touching memory::)
take either zero time or unit time, depending upon a command-line
option.  By providing an idealized view of a communication pattern, `picl'
abstracts away timing artifacts so events that one would expect to
occur simultaneously are written to the trace file as being exactly
simultaneous.

   As an example, the following is the PICL output produced by passing `picl'
the CONCEPTUAL program `TASK 0 SENDS A 32 KILOBYTE MESSAGE TO TASK 1
THEN TASK 1 SENDS A 256 BYTE MESSAGE TO TASK 0.':

     -3 -901 0.00000 0 0 0
     -3 -901 0.00000 1 1 0
     -3 -21 0.00001 0 0 4 2 32768 1 1 1
     -4 -21 0.00001 0 0 0
     -3 -52 0.00001 1 1 3 2 1 0 0
     -3 -52 0.00002 0 0 3 2 1 1 1
     -4 -52 0.00002 1 1 4 2 32768 1 0 0
     -3 -21 0.00002 1 1 4 2 256 1 0 0
     -4 -21 0.00002 1 1 0
     -4 -52 0.00003 0 0 4 2 256 1 1 1
     -4 -901 0.00003 1 1 0
     -4 -901 0.00004 0 0 0

See the PICL manual, `A new PICL trace file format' (ORNL/TM-12125),
for a detailed description of the various fields used in the preceding
trace file.

   ParaGraph is a visualization tool that reads PICL trace files and
graphically displays/animates the corresponding execution in a variety
of formats.  Running ParaGraph on output from `picl' makes it easy for
a CONCEPTUAL programmer to explain complex communication patterns to
other people.

   `picl' is derived from `interpret' (*note The interpret backend::)
and therefore supports the `interpret' backend's `--tasks' and `--mcastsync'
options as well as the standard options described in *note Running
coNCePTuaL programs::.  However, because `picl' does not produce log
files, the `--logfile' option is absent.  `picl' additionally supports
the following two command-line options:

       -A, --all-events=<number>     0=include only communication events;
                                     1=include all events  [default: 0]
       -F, --frequency=<number>      PICL event frequency (Hz) [default: 100000]

   By default, only communication events are written to the trace file.
If `--all-events' is set to `1' then all events are written to the
trace file.  Events not related to communication are defined to take
unit time.  Regardless of the setting of `--all-events', the `OUTPUTS'
statement (*note Writing to standard output::) writes a PICL `tracemsg'
event to the trace file.  A user can tell ParaGraph to pause
visualization at `tracemsg' events, making it possible to isolate key
components of a CONCEPTUAL program's execution.

   Because PICL events are marked with physical time (a floating-point
number of seconds) but `picl' uses exclusively logical time, `picl'
needs to associate a (fabricated) physical time with each logical time.
The `--frequency' option specifies that mapping.  By default, `picl'
pretends that each unit of logical time corresponds to 1/100000 of a
second (i.e, 10 microseconds) of physical time.  As an example, the
default time step in ParaGraph is 1 microsecond; this can be specified
to `picl' with `--frequency=1E6'.

   One of the goals of CONCEPTUAL is to facilitate the explanation of
network performance tests.  The `picl' backend aids in the explanation
by making it easy to show graphically how tasks communicate with each
other in an arbitrary CONCEPTUAL program.


File: conceptual.info,  Node: The latex_vis backend,  Next: The dot_ast backend,  Prev: The picl backend,  Up: Supplied backends

3.3.9 The `latex_vis' backend
-----------------------------

The `latex_vis' backend visualizes a program's communication pattern as
an Encapsulated PostScript time-space diagram.  The backend thereby
provides a static counterpart to the animated visualizations made
possible by the `picl' backend and the ParaGraph tool (*note The picl
backend::).  `latex_vis' outputs LaTeX code with calls to the PSTricks
package (available from `http://www.tug.org/applications/PSTricks/' but
shipped with most LaTeX distributions) to draw the figure, then runs `latex'
(or whatever the `LATEX' environment variable is set to) to convert the
`.tex' file to DVI format and `dvips' (or whatever the `DVIPS'
environment variable is set to) to convert the `.dvi' file into EPS
format.  Finally, `latex_vis' runs the graphic through Ghostscript (`gs'
or whatever the `GS' environment variable is set to) to tighten the
graphic's bounding box.(1) `latex_vis' requires that LaTeX and PSTricks
be installed in order to produce its visualizations.  However, it can
still run with `--no-compile' (*note Compiling coNCePTuaL programs::)
to produce a `.tex' file which can later be run manually through `latex'.

   As an example, the following is the default `latex_vis' output from
the CONCEPTUAL program `TASK 0 SENDS A 1 MEGABYTE MESSAGE TO TASK 1
THEN ALL TASKS SYNCHRONIZE THEN TASK 1 SENDS A 3 KILOBYTE MESSAGE TO
TASK 0' when run with three tasks:


                        |     (0)   (1)   (2)
                        |        \
                        |         \
                                   *
                        T     (0)   (1)   (2)
                        i
                        m     ===============
                        e
                              (0)   (1)   (2)
                        |          /
                        |         /
                        |        *
                        V     (0)   (1)   (2)

Currently, the output diagram does not indicate message size but this
may change in a future release of `latex_vis'.

   The `latex_vis' backend has a number of uses.  First, it can be used
to illustrate a nontraditional communication pattern for a
presentation, research paper, or technical report.  Second, it can be
used as a teaching aid to demonstrate common communication patterns
(e.g., a butterfly pattern) to students.  Third, it can be used as a
debugging aid to ensure that a CONCEPTUAL program is, in fact,
performing the expected communication operations.

   `latex_vis' is derived from `interpret' (*note The interpret
backend::) and therefore supports the `interpret' backend's `--tasks'
and `--mcastsync' options as well as the standard options described in
*note Running coNCePTuaL programs::.  However, because `latex_vis' does
not produce log files, the `--logfile' option is absent.  `latex_vis'
additionally supports the following five command-line options:

       -A, --annotate=<string>         Annotation level (0=no annotations;
                                       1=annotate communication events;
                                       2=annotate all events;
                                       "<event>..."=annotate only the
                                       specified events) [default: "0"]
       -B, --binary-tasks=<number>     Display task numbers in binary rather
                                       than decimal (0=decimal; 1=binary)
                                       [default: 0]
       -E, --every-event=<number>      Events requiring nonzero time to complete
                                       (0=only communication events;
                                       1=every event) [default: 0]
       -G, --stagger=<number>          Number of points by which to stagger
                                       overlapping arrows [default: 2]
       -L, --source-lines=<number>     Associate source-code line numbers with
                                       each event annotation (0=no; 1=yes)
                                       [default: 0]
       -R, --arrow-width=<string>      Python expression to map m, representing
                                       a message size in bytes, to an arrow
                                       width in points [default: "1"]
       -Z, --zero-latency=<number>     Depict communication as having zero
                                       latency (0=unit latency; 1=zero
                                       latency) [default: 0]

   In the preceding list a "point" refers to a PostScript point.  TeX
calls these "big points" and defines 1bp = 1/72")

   The `--annotate' option places adjacent to appropriate nodes in the
output diagram a list of textual annotations which indicate the
communication operations that were posted or completed and the
non-communication operations that were executed by the corresponding
task at the corresponding time.  Some sample annotations are `Post
SEND', `Complete SEND', and `Execute OUTPUT'.  With `--annotate=1',
only communication events are annotated; with `--annotate=2', all
events are annotated;(2); otherwise, a list of specific events can be
annotated.  For example, `--annotate=SYNC,MCAST' causes only the `SYNC'
and `MCAST' events to be annotated.  Compiling a CONCEPTUAL program
with the `latex_vis' backend and the `--keep-ints' options (*note
Compiling coNCePTuaL programs::) produces a `.tex' file which contains
in the prologue comments a list of all events used by a program that
were and were not annotated.

   Task numbers are normally shown in base ten.  The `--binary-tasks'
option causes task numbers to be output in base two and using the same
number of bits for each task.  For example, a 3-task visualization with `--binary-tasks'
would number the tasks `00', `01', and `11'.

   Because `latex_vis' is intended primarily for visualizing
communication patterns, by default only communication operations take
nonzero time to complete.  The `--every-event' option indicates that
all events--including local events such as `SLEEP', `COMPUTE', `LOG',
and `OUTPUT'--should be deemed to complete in unit time.

   `latex_vis' output distinguishes coincident arrows (consider the
phrase `TASK 0 ASYNCHRONOUSLY SENDS 5 MESSAGES TO TASK 1') by
staggering them slightly.  The `--stagger' option specifies the number
of PostScript points by which to stagger each overlapping arrow with a
default of 2 points.  Large stagger values are more visually
distinctive while small stagger values allow arrows to drift less from
their associated nodes.

   The `--source-lines' option, when used with `--annotate', augments
each event annotation with the corresponding line(s) of source code
which produced that event.  This feature improves the utility of `latex_vis'
as a debugging aid for CONCEPTUAL programs.

   By default, all arrows which indicate message transmissions are drawn
with equally thick line widths.  The `--arrow-width' option lets you
specify a Python function to map a message size in bytes, m, to a line
width in PostScript points, with the default being 1 point.  As a
simple example, `--arrow-width=2' doubles the arrow width for all
message (i.e., it represents the constant function f: m -> 2).  A more
typical example would be `--arrow-width="log10(m)"' (representing f: m
-> log10(m)), which causes a tenfold increase in message size to yield
a unit increase in line width.  The argument to `--arrow-width' can be
any Python expression using either built-in functions or functions from
the Python `math' module.  See the Python library reference
(http://docs.python.org/lib/lib.html) for details.

   Normally, messages are considered to require one time unit to travel
from source to destination.  The `--zero-latency' option shows messages
as being received in the same timestep as they were sent.  Some
communication patterns are more aesthetically pleasing when drawn this
way.

* Menu:

* Further customizations::      Exploting ncptl's --filter option

   ---------- Footnotes ----------

   (1) If Ghostscript is not installed or fails to run, `latex_vis'
issues a warning message, not an error message.  The figure is still
usable without a tight bounding box and a loose bounding box can be
corrected manually by editing the `%%BoundingBox:' line in the
generated EPS file.

   (2) To avoid the confusion of annotating what is essentially a "do
nothing" event, `latex_vis' does not annotate the `NEWSTMT' event,
which is injected at the beginning of each top-level statement.


File: conceptual.info,  Node: Further customizations,  Prev: The latex_vis backend,  Up: The latex_vis backend

Further customizations
......................

In addition to the options described above, the front end's `--filter'
option (*note Compiling coNCePTuaL programs::) is a useful mechanism
for customizing the formatting of the communication diagram.  For
example, specifying `--filter="s/rowsep=30bp/rowsep=1.5in/g"' increases
the separation between rows from 30bp (where 1bp = 1/72") to 1.5".  See
the PSTricks documentation (http://www.tug.org/applications/PSTricks/)
for more information about the PSTricks commands used in the generated
`.tex' files.

   To facilitate the use of `--filter', the `latex_vis' backend uses a
helper macro (`\viscolor') to define colors.  `\viscolor' takes an
argument of the form `NAME=COLOR' and defines a macro `\'NAME`color'
which expands to COLOR.  To further facilitate the use of `--filter',
the LaTeX code generated by the `latex_vis' backend contains a number of
strategically placed placeholder comments of the form `% PLACEHOLDER:
TAG'.  A `--filter' command can thereby insert code into the document
by replacing an appropriate `PLACEHOLDER' line.  In alphabetical order,
the currently defined placeholder tags are

   `ANNOTATIONS', `COLORS', `COMMUNICATION', `DEADLOCK', `DOCUMENT', `END', `NODESHAPE', `PACKAGES', `PSMATRIX', `TEXTOEPS',
and `TIMELINE'.  Look through any `latex_vis'-generated `.tex' file to
see where these are situated.  As an example, the following command-line
options define a "chartreuse" color then change the color used to
indicate point-to-point messages from blue to chartreuse:

     --filter="s/% PLACEHOLDER: COLORS/\\newrgbcolor{chartreuse}{0.5 1 0}/"
     --filter="s/sendrecv=blue/sendrecv=chartreuse/"

(According to the PSTricks documentation
(http://www.tug.org/applications/PSTricks/), the predefined colors are
`red', `green', `blue', `cyan', `magenta', and `yellow', and the
predefined grayscales are `black', `darkgray', `gray', `lightgray', and
`white'.)

   Tasks which are active at a given time are drawn using the `\task'
macro, which takes the task number as an argument.  Tasks which are
idle at a given time are drawn using the `\idle' macro, which is
initially defined to be the same as `\task'.  The following "cookbook"
examples showcase some of the ways that the power of LaTeX and PSTricks
can be exploited to display idle tasks in a variety of different styles
(best used with `--annotate=2'):

omitting idle tasks
     `--filter="s/\\let\\idle=\\task/\\newcommand*{\\idle}[1]{[mnode=R]}/"'

showing each idle task as a gray dot
     `--filter="s/\\let\\idle=\\task/\\newcommand*{\\idle}[1]{[mnode=dot,
     linecolor=gray]}/"'

drawing idle tasks with a dotted circle instead of a solid circle
     `--filter="s/\\let\\idle=\\task/\\newcommand*{\\idle}[1]
     {[linestyle=dotted]\\task{#1}}/"'

As an alternative to replacing the `\let\idle=\task' binding, the
preceding substitutions can be expressed as the insertion of a LaTeX
`\renewcommand'.  That is, idle tasks can also be omitted by specifying
`--filter="s/% PLACEHOLDER:
NODESHAPE/\\renewcommand*{\\idle}[1]{[mnode=R]}/"'.

   In short, the `latex_vis' backend produces highly customizable
illustrations of communication patterns.  Because `latex_vis' produces
commented LaTeX code, any customization not provided through the use of `--filter'
or one of the backend-specific command-line options is easily performed
by compiling with `--keep-ints' (*note Compiling coNCePTuaL programs::)
and editing the generated LaTeX code.


File: conceptual.info,  Node: The dot_ast backend,  Prev: The latex_vis backend,  Up: Supplied backends

3.3.10 The `dot_ast' backend
----------------------------

dot is a format for describing graphs in terms of their edges and
vertices.  The tools in the Graphviz suite typeset dot files in a
variety of output formats and using a variety of graph-layout
algorithms.  CONCEPTUAL's `dot_ast' backend outputs in dot format the
abstract-syntax tree corresponding to a given CONCEPTUAL program.  As
an example, `dot_ast' renders the one-line CONCEPTUAL program `TASK 0
SLEEPS FOR 10 SECONDS.' as follows:


                             +---------+
                             | program |
                             +---------+     1)  TASK 0 SLEEPS FOR 10 SECONDS
                             | line 1  |
                             +---------+
                                  |
                                  V
                     +---------------------+----+
                     | top_level_stmt_list | 1L |
                     +---------------------+----+
                     |          line 1          |
                     +--------------------------+
                                  |
                                  V
                          +----------------+
                          | top_level_stmt |
                          +----------------+
                          |     line 1     |
                          +----------------+
                                  |
                                  V
                      +------------------+----+
                      | simple_stmt_list | 1L |
                      +------------------+----+
                      |        line 1         |
                      +-----------------------+
                                  |
                                  V
                           +-------------+
                           | simple_stmt |
                           +-------------+
                           |   line 1    |
                           +-------------+
                                  |
                                  V
                            +------------+
                            | sleeps_for |
                            +------------+
                            |   line 1   |
                            +------------+
                           /      |       \
                          /       V        \
          +--------------+    +--------+    +-----------+-----------+
          | source_task  |    |  expr  |    | time_unit | 'seconds' |
          +--------------+    +--------+    +-----------+-----------+
          |   line 1     |    | line 1 |    |        line 1         |
          +--------------+    +--------+    +-----------------------+
                  |               |
                  V               V
    +-----------+--------+    +-------------+
    | task_expr | 'expr' |    | ifelse_expr |
    +-----------+--------+    +-------------+
    |       line 1       |    |   line 1    |
    +--------------------+    +-------------+
              |                       |
              V                       V
          +--------+             +----------+
          |  expr  |             | add_expr |
          +--------+             +----------+
          | line 1 |             |  line 1  |
          +--------+             +----------+
              |                       |
              V                       V
       +-------------+          +-----------+
       | ifelse_expr |          | mult_expr |
       +-------------+          +-----------+
       |   line 1    |          |  line 1   |
       +-------------+          +-----------+
              |                       |
              V                       V
         +----------+           +------------+
         | add_expr |           | unary_expr |
         +----------+           +------------+
         |  line 1  |           |   line 1   |
         +----------+           +------------+
              |                       |
              V                       V
        +-----------+           +------------+
        | mult_expr |           | power_expr |
        +-----------+           +------------+
        |  line 1   |           |   line 1   |
        +-----------+           +------------+
              |                       |
              V                       V
        +------------+        +--------------+
        | unary_expr |        | primary_expr |
        +------------+        +--------------+
        |   line 1   |        |    line 1    |
        +------------+        +--------------+
              |                       |
              V                       V
        +------------+        +---------+-----+
        | power_expr |        | integer | 10L |
        +------------+        +---------+-----+
        |   line 1   |        |     line 1    |
        +------------+        +---------------+
              |
              V
       +--------------+
       | primary_expr |
       +--------------+
       |    line 1    |
       +--------------+
              |
              V
       +---------+----+
       | integer | 0L |
       +---------+----+
       |    line 1    |
       +--------------+

`dot_ast' is expected to be of particular use to backend developers,
who can use it to help prioritize the methods that need to be
implemented (i.e., implementing first the AST node types needed by in a
trivial program, then those needed by successively more complex
programs).

   The `dot_ast' backend accepts the following options from the `ncptl'
command line:

`--format=DOT_FORMAT'
     The programs in the Graphviz suite can output graphs in a variety
     of formats such as PostScript, SVG, and PNG.  By default, the `dot_ast'
     backend outputs PostScript.  The `--format' option specifies an
     alternate format to use.  At the time of this writing, the Graphviz
     programs support the following formats: `canon', `cmap', `dot',
     `fig', `gd', `gd2', `gif', `hpgl', `imap', `ismap', `jpeg', `jpg',
     `mif', `mp', `pcl', `pic', `plain', `plain-ext', `png', `ps',
     `ps2', `svg', `svgz', `vrml', `vtx', `wbmp', and `xdot'.  See the Graphviz
     documentation for more information about these formats.

`--node-code=CHARACTERS'
     To facilitate associating nodes in the AST with fragments of the
     CONCEPTUAL program being graphed, the `dot_ast' backend provides a `--node-code'
     option that labels each node with the fragment of code to which it
     corresponds.  The argument to `--node-code' is a number of
     characters at which to truncate the code fragment or `-1' to
     inhibit truncation.  (The purpose of truncation is to prevent
     excessively large nodes from disturbing the graph layout.  The
     `program' node, for example, includes the complete program source
     code if not truncated.)

`--extra-dot=DOT_CODE'
     The `dot_ast' backend's `--extra-dot' option enables the user to
     inject arbitrary dot code into the generated file.  For example,
     specifying `--extra-dot="node [shape=Mrecord]"'(1) tells dot to use
     draw nodes as rounded rectangles and specifying `--extra-dot='edge
     [color="green"]'' colors all edges green.  `--extra-dot' can be
     specified repeatedly on the command line; `dot_ast' concatenates
     all of the extra dot code with intervening semicolons.

`--no-lines'
     By default, each AST node indicates the lines in the program's
     source code to which it corresponds.  The `--no-lines' option
     suppresses the outputting of source-code line numbers.

`--no-attrs'
     Every node in the AST has a type.  Some nodes additionally have an
     attribute.  `dot_ast' normally outputs attributes but `--no-attrs'
     prevents `dot_ast' from doing so.

`--no-source'
     The complete source code corresponding to the AST is included in
     the generated dot graph unless `--no-source' is specified on the
     command line.

   The `DOT' environment variable names the Graphviz program that `dot_ast'
should run on the generated code.  If `DOT' is not set, `dot_ast' uses
whatever value was specified/discovered at configuration time (*note
configure::), with the default being `dot'.  By default, `dot_ast'
produces dot code and runs this through the designated Graphviz program
to produce a PostScript file (or whatever format is named by the `--format'
option).  If `ncptl' is run with either the `--no-link' or `--no-compile'
options, it produces a dot file that should be run manually through `dot'
or another Graphviz tool.

   ---------- Footnotes ----------

   (1) `dot_ast' automatically places a semicolon after the extra dot
code.


File: conceptual.info,  Node: Running coNCePTuaL programs,  Next: Interpreting coNCePTuaL log files,  Prev: Supplied backends,  Up: Usage

3.4 Running coNCePTuaL programs
===============================

CONCEPTUAL programs can be run like any other program built with the
same compiler and communication library.  For example if a program
`myprog' was built with CONCEPTUAL's C+MPI backend, the program might
be run with a command like `mpirun -np NODES myprog' or `prun -NNODES
myprog' or `pdsh -w NODE_LIST myprog'.  The important point is that job
launching is external to CONCEPTUAL.  A CONCEPTUAL program is oblivious
to whether it is being run with a single thread on each multiprocessor
node or with one thread on each CPU, for example.  However, CONCEPTUAL
log files do include the host name in the prologue comments (*note
Log-file format::) so job-launching parameters can potentially be
inferred from those.

   CONCEPTUAL programs automatically support a "help" option.  This is
usually specified as `--help' or `-?', depending on which
option-parsing library `configure' configured in.  (*Note configure::.)
The output of running `myprog --help' most likely looks something like
this:

     Usage: myprog [OPTION...]
       -C, --comment=<string>      Additional commentary to write to the log
                                   file, @FILE to import commentary from FILE,
                                   or !COMMAND to import commentary from COMMAND
                                   (may be specified repeatedly)
       -L, --logfile=<string>      Log-file template [default: "a.out-%p.log"]
       -N, --no-trap=<string>      List of signals that should not be trapped
                                   [default: ""]
       -S, --seed=<number>         Seed for the random-number generator
                                   [default: 0]

     Help options:
       -?, --help                  Show this help message
       --usage                     Display brief usage message

   Although a CONCEPTUAL program can specify its own command-line
options (*note Command-line arguments::), a few are provided by
default.  In addition to `--help' these include `--comment', `--logfile', `--no-trap',
and `--seed'.

`--comment'
     `--comment' makes it possible to add arbitrary commentary to a log
     file.  This is useful for incorporating information that CONCEPTUAL
     would be unable to (or simply does not currently) determine on its
     own, for example, `--comment="Last experiment before upgrading the
     network device driver"'.  Two special cases are supported:

       1. If the comment string begins with `@' then the remainder of
          the string is treated as a filename.  Each line of the
          corresponding file is treated as a separate comment string.
          Hence, if the file `sysdesc.txt' contains the lines `Using
          FooBarNet' and `Quux is enabled', then specifying
          `--comment=sysdesc.txt' is similar to specifying both
          `--comment="Using FooBarNet"' and `--comment="Quux is
          enabled"'.

       2. If the comment string begins with `!' then the remainder of
          the string is treated as a shell command.  The command is
          executed and each line of its output is treated as a separate
          comment string.  For example, `--comment='!lspci | grep -i
          net'' executes `lspci', extracts only those lines containing
          the string `net', and makes log-file comments out of the
          result.  Note that `--comment='!COMMAND'' differs from
          `--comment="`COMMAND`"' in that the former causes COMMAND to
          be executed individually by each process in the program while
          the latter executes COMMAND only once and only before
          launching the program.  Also note that `!' must be escaped in `csh'
          and derivitive shells (i.e., `--comment='\!COMMAND'').

     As an example, the command line arguments `--comment="This is a
     simple comment." --comment=!lspci --comment=@/proc/version
     --comment="This is another simple comment."' produce log-file lines
     like the following:

          # User comment 1: This is a simple comment.
          # Output of 'lspci', line 1: 00:00.0 Host bridge: Intel Corp. 82860 860 (Wombat) Chipset Host Bridge (MCH) (rev 04)
          # Output of 'lspci', line 2: 00:01.0 PCI bridge: Intel Corp. 82850 850 (Tehama) Chipset AGP Bridge (rev 04)
          # Output of 'lspci', line 3: 00:02.0 PCI bridge: Intel Corp. 82860 860 (Wombat) Chipset AGP Bridge (rev 04)
          # Output of 'lspci', line 4: 00:1e.0 PCI bridge: Intel Corp. 82801BA/CA/DB PCI Bridge (rev 04)
          # Output of 'lspci', line 5: 00:1f.0 ISA bridge: Intel Corp. 82801BA ISA Bridge (LPC) (rev 04)
          # Output of 'lspci', line 6: 00:1f.1 IDE interface: Intel Corp. 82801BA IDE U100 (rev 04)
          # Output of 'lspci', line 7: 00:1f.2 USB Controller: Intel Corp. 82801BA/BAM USB (Hub  (rev 04)
          # Output of 'lspci', line 8: 00:1f.3 SMBus: Intel Corp. 82801BA/BAM SMBus (rev 04)
          # Output of 'lspci', line 9: 00:1f.4 USB Controller: Intel Corp. 82801BA/BAM USB (Hub  (rev 04)
          # Output of 'lspci', line 10: 00:1f.5 Multimedia audio controller: Intel Corp. 82801BA/BAM AC'97 Audio (rev 04)
          # Output of 'lspci', line 11: 01:00.0 VGA compatible controller: nVidia Corporation NV11 [GeForce2 MXR] (rev b2)
          # Output of 'lspci', line 12: 02:1f.0 PCI bridge: Intel Corp. 82806AA PCI64 Hub PCI Bridge (rev 03)
          # Output of 'lspci', line 13: 03:00.0 PIC: Intel Corp. 82806AA PCI64 Hub Advanced Programmable Interrupt Controller (rev 01)
          # Output of 'lspci', line 14: 04:0b.0 Ethernet controller: 3Com Corporation 3c905C-TX/TX-M [Tornado] (rev 78)
          # Output of 'lspci', line 15: 04:0d.0 Multimedia audio controller: Creative Labs SB Live! EMU10k1 (rev 08)
          # Output of 'lspci', line 16: 04:0d.1 Input device controller: Creative Labs SB Live! MIDI/Game Port (rev 08)
          # Contents of /proc/version, line 1: Linux version 2.4.20-28.7 (bhcompile@porky.devel.redhat.com) (gcc version 2.96 20000731 (Red Hat Linux 7.3 2.96-126)) #1 Thu Dec 18 11:31:59 EST 2003
          # User comment 2: This is another simple comment.

     To facilitate log-file parsing, all colons in the name of an
     `@'-file or `!'-command are written as periods.  This affects only
     the display, not the file to read or command to execute.

     Be careful when using shell backquotes with `--comment' (e.g., as
     in `--comment="`who`"').  Different shells have different ways of
     handling newlines output by a backquoted command.  In some shells
     (e.g., `bash'), the CONCEPTUAL program sees the entire output with
     embedded newlines as a single argument; in others (e.g., `tcsh'),
     each line of output constitutes a separate command-line argument.
     Because CONCEPTUAL programs currently ignore arguments which do
     not begin with `--', the comment written to the log file in the
     latter case terminates at the first newline.  In virtually all
     shells, the double quotes around the backquoted command are needed
     to prevent the shell from splitting arguments at word boundaries.
     As a consequence, `--comment=`who`' logs only the first word
     output by the `who' command.

`--logfile'
     `--logfile' specifies a template for naming log files.  Each task
     maintains a log file based on the template name but with `%p'
     replaced with the processor number, `%r' replaced with the run
     number (the smallest nonnegative integer that produces a filename
     which does not already exist), and `%%' replaced with a literal
     "%" character.  The program outputs an error message and aborts if
     the log-file template does not contain at least one `%p'.  The
     only exception is that an empty template (i.e., `--logfile=""')
     inhibits the production of log files entirely.

     Like C's `printf()' function, a numeric field width can occur
     between the `%' and the conversion specifier (the `p' or `r' in
     the case of `--logfile').  The field is padded on the left with
     spaces to the given width.  More practically, if the field width
     begins with the number `0' the field is padded on the left with
     zeroes to the given width.  For example, specifying
     `--logfile="mydata-%03p.log"' on the command line produces log
     files named `mydata-000.log', `mydata-001.log', `mydata-002.log',
     `mydata-003.log', and so forth.

`--no-trap'
     `--no-trap' specifies a list of signals or ranges of signals that
     should not be trapped.  For example, `--no-trap=10-12,17' prevents
     signals 10, 11, 12, and 17 from being trapped.(1) Signals can also
     be referred to by name, with or without a `SIG' prefix.  Also,
     names and numbers can be freely mixed.  Hence,
     `--no-trap=10-12,INT,17,SIGSTOP,SIGCONT' is a valid argument to a
     CONCEPTUAL program.  Because signal reception can adversely affect
     performance, CONCEPTUAL's default behavior is to terminate the
     program on receipt of a signal.  However, some signals may be
     necessary for the underlying communication layer's proper
     operation.  `--no-trap' enables such signals to pass through
     CONCEPTUAL untouched.  (Some signals, however, are needed by
     CONCEPTUAL or by a particular backend and are always trapped.)

`--seed'
     `--seed' (which selects a different default value on each run) is
     used in any program that utilizes the `RANDOM TASK' construct
     (*note Binding variables::) or that sends message `WITH
     VERIFICATION' (*note Message specifications::).

   If the `LOGS' statement (*note Writing to a log file::) is used
anywhere in a CONCEPTUAL program, then _all_ processes write a log
file.  This is done because CONCEPTUAL log files--even those which
contain no measurement data--include a wealth of important information
in prologue comments, as described *note Interpreting coNCePTuaL log
files::.  As a consequence, a program run with thousands of processes
produces thousands of log files.  If process 0 is the only process
which logs actual data, the `ncptl-logmerge' script (*note
ncptl-logmerge::) can merge these log files into a single, more
manageable, file.  For situations in which it is unreasonable for every
process to write a log file (e.g., if the filesystem is unable to
handle large numbers of simultaneous file creations, the `NCPTL_LOG_ONLY'
environment variable lets the user limit the set of processes which
produce log files.  `NCPTL_LOG_ONLY' accepts a comma-separated list of
dash-separated process ranges such as `0-3,12-16,24,25,32-48'.  Only
processes included in the list produce log files.

   ---------- Footnotes ----------

   (1) On some platforms, these signals correspond to `SIGUSR1',
`SIGSEGV', `SIGUSR2', and `SIGCHLD', respectively.


File: conceptual.info,  Node: Interpreting coNCePTuaL log files,  Prev: Running coNCePTuaL programs,  Up: Usage

3.5 Interpreting coNCePTuaL log files
=====================================

Any CONCEPTUAL program that uses the `LOGS' keyword (*note Writing to a
log file::) will produce a log file as it runs.  The CONCEPTUAL
run-time library writes log files in a simple, plain-text format.  In
addition to measurement data, a wealth of information is stored within
log-file comments.  CONCEPTUAL comes with a tool, `ncptl-logextract',
which can extract data and other information from a log file and
convert it into any of a variety of other formats.

* Menu:

* Log-file format::             Syntax and semantics of program log files
* ncptl-logextract::            A tool for extracting log-file information
* ncptl-logmerge::              A tool for merging and comparing log files
* ncptl-logunmerge::            A tool for undoing the effects of ncptl-logmerge


File: conceptual.info,  Node: Log-file format,  Next: ncptl-logextract,  Prev: Interpreting coNCePTuaL log files,  Up: Interpreting coNCePTuaL log files

3.5.1 Log-file format
---------------------

The CONCEPTUAL run-time library writes log files in the following
(textual) format:

   * Lines beginning with `#' are comments.

   * Columns are separated by commas.

   * Strings are output between double quotes.  Literal double-quotes
     are output as `\"' and literal backslashes are output as `\\'.

A sample log file is listed below.  The log file is presented in its
entirety.

     ###########################################################################
     # ===================
     # coNCePTuaL log file
     # ===================
     # coNCePTuaL version: 0.6.4a
     # coNCePTuaL backend: c_mpi (C + MPI)
     # Executable name: /home/pakin/src/coNCePTuaL/example
     # Working directory: /home/pakin/src/coNCePTuaL
     # Command line: ./example
     # Number of tasks: 2
     # Processor (0<=P<tasks): 0
     # Host name: a1
     # Operating system: Linux version 2.4.21-3.5qsnet (root@a31) (gcc version 2.96 20000731 (Red Hat Linux 7.2 2.96-108.1)) #2 SMP Thu Aug 7 10:51:04 MDT 2003
     # CPU vendor: GenuineIntel
     # CPU architecture: ia64
     # CPU count: 2
     # CPU frequency: 1300000000 Hz (1.3 GHz)
     # Cycle-counter frequency: 1300000000 Hz (1.3 GHz)
     # OS page size: 16384 bytes
     # Physical memory: 2047901696 bytes (1.9 GB)
     # Elan capability: [1965771c.6213bf5d.3a26411c.5b4c5d58] Version 10002 Type a001 Context 640.640.640 Node 1.2
     # coNCePTuaL configuration: ./configure '--prefix=/tmp/ncptl' 'MPICPPFLAGS=-I/usr/local/include' 'CFLAGS=-g -O3 -ansi_alias -ansi' 'MPICC=/usr/lib/mpi/mpi_intel/bin/mpicc' '--enable-maintainer-mode' 'CC=ecc' '--disable-shared'
     # Library compiler+linker: /opt/intel-7.1.033/compiler70/ia64/bin/ecc
     # Library compiler version: Intel(R) C++ gcc 3.0 mode [7.1]
     # Library compiler options: -g -O3 -ansi_alias -ansi
     # Library linker options: -lrmscall -lelan -lpopt
     # Library compiler mode: LP64
     # Dynamic libraries used: /usr/lib/qsnet/elan3/lib/librmscall.so.1 /usr/lib/qsnet/elan3/lib/libelan.so.1 /usr/lib/libpopt.so.0.0.0 /usr/lib/mpi/mpi_intel/lib/libmpi.so.1.0 /lib/libm-2.2.4.so /opt/intel-7.1.033/compiler70/ia64/lib/libcxa.so.4 /lib/libc-2.2.4.so /usr/lib/qsnet/elan3/lib/libelan3.so.1 /usr/lib/qsnet/elan/lib/libelanctrl.so.2 /lib/ld-2.2.4.so
     # Microsecond timer type: gettimeofday()
     # Average microsecond timer overhead: <1 microsecond
     # Microsecond timer increment: 1.00466 +/- 0.123576 microseconds (ideal: 1 +/- 0)
     # Minimum sleep time: 1946.6 +/- 31.5872 microseconds (ideal: 1 +/- 0)
     # WARNING: Sleeping exhibits poor granularity (not a serious problem).
     # WARNING: Sleeping has a large error component (not a serious problem).
     # Process CPU timer: getrusage()
     # Process CPU-time increment: 976.59 +/- 0.494311 microseconds (ideal: 1 +/- 0)
     # WARNING: Process timer exhibits poor granularity (not a serious problem).
     # Log-file template: example-%p.log
     # Number of minutes after which to kill the job (-1=never): -1
     # List of signals which should not be trapped: 14
     # Log-file checkpointing interval: 60 seconds (i.e., 1 minute)
     # MPI send routine: MPI_Send()
     # MPI error checking: off
     # Front-end compilation command line: ncptl --backend=c_mpi example.ncptl
     # Back-end compilation command line: /usr/lib/mpi/mpi_intel/bin/mpicc -I/tmp/ncptl/include  -I/usr/local/include -g -O3 -ansi_alias -ansi tmppG76Fv.c  -L/tmp/ncptl/lib   -lncptl -lrmscall -lelan -lpopt -o example
     # Log creator: Scott Pakin
     # Log creation time: Mon Dec 19 12:02:18 2005
     #
     # Environment variables
     # ---------------------
     # CVS_RSH: /usr/bin/ssh
     # DISPLAY: localhost:19.0
     # DYNINSTAPI_RT_LIB: /home/pakin/dyninstAPI-3.0/lib/i386-unknown-linux2.2/libdyninstAPI_RT.so.1
     # DYNINST_ROOT: /home/pakin/dyninstAPI-3.0
     # EDITOR: /usr/bin/emacs
     # GROUP: CCS3
     # HOME: /home/pakin
     # HOST: a0
     # HOSTNAME: a0
     # HOSTTYPE: unknown
     # KDEDIR: /usr
     # LANG: en_US
     # LD_LIBRARY_PATH: /opt/intel-7.1.033/compiler70/ia64/lib:/users/pakin/lib:/usr/lib:/usr/ccs/lib:/opt/SUNWspro/lib:/usr/dt/lib:/usr/openwin/lib:/usr/X11R6/lib:/usr/local/gnu/lib:/usr/local/lib:/usr/ucblib:/users/pakin/dyninstAPI-3.0/lib/i386-unknown-linux2.2
     # LESSOPEN: |/usr/bin/lesspipe.sh %s
     # LOGNAME: pakin
     # LPDEST: lwy
     # LS_COLORS: no=00:fi=00:di=01;34:ln=01;36:pi=40;33:so=01;35:bd=40;33;01:cd=40;33;01:or=01;05;37;41:mi=01;05;37;41:ex=01;32:*.cmd=01;32:*.exe=01;32:*.com=01;32:*.btm=01;32:*.bat=01;32:*.sh=01;32:*.csh=01;32:*.tar=01;31:*.tgz=01;31:*.arj=01;31:*.taz=01;31:*.lzh=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.gz=01;31:*.bz2=01;31:*.bz=01;31:*.tz=01;31:*.rpm=01;31:*.cpio=01;31:*.jpg=01;35:*.gif=01;35:*.bmp=01;35:*.xbm=01;35:*.xpm=01;35:*.png=01;35:*.tif=01;35:
     # MACHTYPE: unknown
     # MAIL: /var/mail/pakin
     # MANPATH: /opt/intel-7.1.033/compiler70/man:/home/pakin/man:/usr/man:/opt/SUNWspro/man:/usr/dt/man:/usr/openwin/man:/usr/X11R6/man:/usr/local/gnu/man:/usr/local/man:/usr/share/man:/usr/lanl/man
     # MOZILLA_HOME: /usr/local/netscape/java
     # NAME: Scott Pakin
     # ORGANIZATION: Los Alamos National Lab
     # OSTYPE: linux
     # PATH: /opt/intel-7.1.033/compiler70/ia64/bin:.:/home/pakin/bin:/usr/local/bin:/usr/dt/bin:/usr/openwin/bin:/usr/X11R6/bin:/usr/local/gnu/bin:/usr/local/bin:/sbin:/bin:/opt/SUNWspro/bin:/usr/sbin:/usr/bin:/usr/ccs/bin:/usr/ucb:/usr/local/teTeX/bin:.:/usr/lanl/bin
     # PRINTER: lwy
     # PVM_ROOT: /usr/share/pvm3
     # PVM_RSH: /usr/bin/rsh
     # PWD: /home/pakin/src/coNCePTuaL
     # QTDIR: /usr/lib/qt-2.3.1
     # REMOTEHOST: antero.c3.lanl.gov
     # RMS_JOBID: 308
     # RMS_MACHINE: a
     # RMS_NNODES: 2
     # RMS_NODEID: 0
     # RMS_NPROCS: 2
     # RMS_PROCID: 0
     # RMS_RANK: 0
     # RMS_RESOURCEID: parallel.318
     # RMS_STOPONELANINIT: 0
     # SHELL: /bin/tcsh
     # SHLVL: 2
     # SSH_AGENT_PID: 8586
     # SSH_ASKPASS: /usr/libexec/openssh/gnome-ssh-askpass
     # SSH_AUTH_SOCK: /tmp/ssh-XXfG457q/agent.8562
     # SSH_CLIENT: 128.165.20.177 47456 22
     # SSH_TTY: /dev/pts/10
     # SUPPORTED: en_US:en
     # TERM: xterm
     # TZ: MST7MDT
     # USER: pakin
     # VENDOR: unknown
     #
     # coNCePTuaL source code
     # ----------------------
     #     FOR 10 REPETITIONS {
     #       TASK 0 RESETS ITS COUNTERS THEN
     #       TASK 0 SENDS A 0 BYTE MESSAGE TO TASK 1 THEN
     #       TASK 1 SENDS A 0 BYTE MESSAGE TO TASK 0 THEN
     #       TASK 0 LOGS EACH elapsed_usecs/2 AS "Latency (usecs)"
     #     }
     #
     ###########################################################################
     "Latency (usecs)"
     "(all data)"
     192.5
     7
     5.5
     5.5
     5
     5
     5
     5.5
     5
     5.5
     ###########################################################################
     # Program exited normally.
     # Log completion time: Mon Dec 19 12:02:18 2005
     # Elapsed time: 0 seconds
     # Process CPU usage (user+system): 0 seconds
     # Number of interrupts received (all CPUs): 22
     # Peak memory allocation: 1072987 bytes (1.0 MB)
     ###########################################################################


   As the preceding example indicates, a log file's comment block can be
divided into multiple stanzas:

   * a list of <KEY:VALUE> pairs that describe various characteristics
     of the run-time environment, including hardware and software
     identification, timer quality, values of command-line arguments,
     and a timestamp

   * a dump of the environment variables active when the program ran

   * the complete CONCEPTUAL source program

Two rows of column headers and the measurement data follow the prologue
comment block.  A set of epilogue comments completes the log file.

   The motivation for writing so much information to the log file is to
facilitate reproduction of the experiment.  The ideal situation is for
a third party to be able to look at a CONCEPTUAL log file and from that,
recreate the original experiment and get identical results.


   Some of the comments that may benefit from additional explanation
include the following:

`Library compiler mode'
     `LP64' means "*L*ong integers and *P*ointers contain exactly *64*
     bits while ordinary integers contain exactly 32 bits".  `ILP32'
     means "ordinary *I*ntegers, *L*ong integers, and *P*ointers all
     contain exactly *32* bits".  The library compiler mode will be
     `nonstandard' for any other combination of datatype sizes.

`Average timer overhead'
     During initialization, the CONCEPTUAL run-time library performs
     some calibration routines.  Among these routines is a measurement
     of the quality of whatever mechanisms the library is using to
     measure elapsed time.  In the sample log file presented above, the
     mechanism used was `inline assembly code', meaning the run-time
     library reads the hardware cycle counter without going through a
     standard library or system call.  The complete set of timer
     mechanisms and selection criteria is presented in *note
     Time-related functions::, in the documentation for the `ncptl_time()'
     function.

     The log file then reports "average timer overhead" as the mean time
     between back-to-back invocations of whichever timer routine is
     being used.  Ideally, the mean should be `<1 microsecond' but this
     is not the case on all systems.  Large values indicate that a
     performance penalty is charged every time a CONCEPTUAL program
     reads the timer.

`Timer increment'
     In addition to measuring the overhead of reading the timer, the
     CONCEPTUAL run-time library also measures timer accuracy.  The
     library expects to be able to read the timer with microsecond
     accuracy.  That is, the time reported should not increase by more
     than a microsecond between successive readings of the timer.  To
     gauge timer accuracy, the library's initialization routine
     performs a number of back-to-back invocations of the timer routine
     and reports the mean and standard deviation of the number of
     microseconds that elapsed between readings, discarding any deltas
     of zero microseconds.  Ideally, the microsecond timer, when read
     multiple times in rapid succession, should report nonzero
     increments of exactly one microsecond with no variation.  The log
     file will contain warning messages if the increment or standard
     deviation are excessively large, as this may indicate a large
     margin of error in the measurement data.

`Process CPU-time increment'
`Process CPU usage (user+system)'
     Log files end with an epilogue section which includes `Process CPU
     usage (user+system)', which indicates the subset of total
     wall-clock time for which the program was running (`user') or for
     which the operating system was running on the program's behalf
     (`system').  The log-file prologue reports as `Process CPU-time
     increment' the resolution of the timer user to report process CPU
     time.  Note that process CPU time is not exported to CONCEPTUAL
     programs; it is therefore much less critical than the wall-clock
     timer and is reported primarily for informational purposes.

`Number of interrupts received (all CPUs)'
     On certain platforms, CONCEPTUAL can tally the number of CPU
     interrupts that were processed during the run of the program.
     Because a multiprocessor may migrate tasks among CPUs during their
     execution, a per-CPU interrupt count may have little merit.
     Consequently, the number reported represents the sum across all
     CPUs in the same node (but not across nodes).  CONCEPTUAL attempts
     to read interrupt information from the `/proc/interrupts' file if
     no alternative mechanism is available.  In case having a large
     number of processes accessing `/proc/interrupts' poses a problem
     the `--disable-proc-interrupts' configuration option prevents
     programs from accessing that file.

`Peak memory allocation'
     CONCEPTUAL programs heap-allocate memory for a variety of purposes:
     message buffers, event lists (*note Generated code::), unaggregated
     performance data (*note Computing aggregates::), etc.  Allocated
     memory which is no longer needed is returned to the heap.  The
     total amount of allocated memory the program is holding therefore
     increases and decreases over time.  The `Peak memory allocation'
     comment reports the maximum amount of memory the program held at
     any given time.  If this value nears or exceeds the value reported
     for `Physical memory', it is possible that paging overheads may be
     negatively impacting some of the program's timing measurements.  On
     systems without demand paging, exceeding `Physical memory' is
     likely to crash the program; a large `Peak memory allocation' on a
     smaller run can therefore help explain why a larger run is
     crashing.  `Peak memory allocation' includes only memory allocated
     using the memory-allocation functions described in *note
     Memory-allocation functions::.


File: conceptual.info,  Node: ncptl-logextract,  Next: ncptl-logmerge,  Prev: Log-file format,  Up: Interpreting coNCePTuaL log files

3.5.2 `ncptl-logextract'
------------------------

To facilitate converting CONCEPTUAL log files into input data for other
applications, CONCEPTUAL provides a Perl script called `ncptl-logextract'.
`ncptl-logextract' can extract the data from a log file--as well as
various information that appears in the log file's comments--into a
variety of formats suitable for graphing or typesetting.

   Running `ncptl-logextract --usage' causes `ncptl-logextract' to list
a synopsis of its core command-line options to the standard output
device; running `ncptl-logextract --help' produces basic usage
information; and, running `ncptl-logextract --man' outputs a complete
manual page.  *Note ncptl-logextract manual page::, shows the `ncptl-logextract'
documentation as produced by `ncptl-logextract --man'.

* Menu:

* ncptl-logextract manual page::   The result of running ``ncptl-logextract --man''


File: conceptual.info,  Node: ncptl-logextract manual page,  Prev: ncptl-logextract,  Up: ncptl-logextract

NAME
....

ncptl-logextract - Extract various bits of information from a
CONCEPTUAL log file

SYNOPSIS
........

ncptl-logextract `--usage' | `--help' | `--man'


ncptl-logextract [`--extract'=[data|params|env|source|warnings]] [`--format'=FORMAT]
[FORMAT-SPECIFIC OPTIONS...]  [`--before'=STRING] [`--after'=STRING] [`--force-merge'[=NUMBER]]
[`--procs'=STRING] [`--quiet'] [`--verbose'] [`--output'=FILENAME]
[FILENAME...]

DESCRIPTION
...........

*Background*    CONCEPTUAL is a domain-specific programming language
designed to facilitate writing networking benchmarks and validation
suites.  CONCEPTUAL programs can log data to a file but in only a
single file format.  `ncptl-logextract' extracts this log data and
outputs it in a variety of formats for use with other applications.

   The CONCEPTUAL-generated log files that serve as input to
`ncptl-logextract' are plain ASCII files.  Syntactically, they contain
a number of newline-separated tables.  Each table contains a number of
newline-separated rows of comma-separated columns.  This is known
generically as COMMA-SEPARATED VALUE or CSV format.  Each table begins
with two rows of header text followed by one or more rows of numbers.
Text is written within double quotes.  Double-quote characters and
backslashes within text are escaped with a backslash.  No other escaped
characters are recognized.  Lines that begin with `#' are considered
comments.

   Semantically, there are four types of data present in every
CONCEPTUAL-generated log file:

  1. The complete source code of the CONCEPTUAL program that produced
     the log file

  2. Characteristics of the run-time environment and the values of all
     command-line parameters

  3. A list of warning messages that CONCEPTUAL issued while analyzing
     the run-time environment

  4. One or more tables of measurement data produced by the CONCEPTUAL
     program

        The first three items appear within comment lines.  The
measurement data is written in CSV format.

*Extracting information from coNCePTuaL log files*    It is common to
want to extract information (especially measurement data) from log
files.  For simple formatting operations, a one-line awk or Perl script
suffices.  However, as the complexity of the formatting increases, the
complexity of these scripts increases even more.  That's where
`ncptl-logextract' fits in.  `ncptl-logextract' makes it easy to
extract any of the four types of log data described above and format it
in variety of ways.  Although the number of options that
`ncptl-logextract' supports may be somewhat daunting, it is well worth
learning how to use `ncptl-logextract' to avoid reinventing the wheel
every time a CONCEPTUAL log file needs to be processed.
`ncptl-logextract' takes care of all sorts of special cases that crop
up when manipulating CONCEPTUAL log files.

OPTIONS
.......

`ncptl-logextract' accepts the following command-line options
regardless of what data is extracted from the log file and what
formatting occurs:

`-h', `--help'
     Output the Synopsis section and the Options section then exit the
     program.

`-m', `--man'
     Output a complete Unix man ("manual") page for `ncptl-logextract'
     then exit the program.

`-e' INFO, `--extract'=INFO
     Specify what sort of data should be extracted from the log file.
     Acceptable values for INFO are listed and described in the
     Additional Options section and include `data', `params', `env', and
     `source'.

`-f' FORMAT, `--format'=FORMAT
     Specify how the extracted data should be formatted.  Valid
     arguments depend upon the value passed to `--extract' and include
     such formats as `csv', `html', `latex', `text', and `bash'.  See
     the Additional Options section for details, explanations, and
     descriptions of applicability.

`-b' STRING, `--before'=STRING
     Output an arbitrary string of text before any other output.  STRING
     can contain escape characters such as `\n' for newline, `\t' for
     tab, and `\\' for backslash.

`-a' STRING, `--after'=STRING
     Output an arbitrary string of text after all other output.  STRING
     can contain escape characters such as `\n' for newline, `\t' for
     tab, and `\\' for backslash.

`-F' [NUMBER], `--force-merge'[=NUMBER]
     Try extra hard to merge multiple log files, even if they seem to
     have been produced by different programs or in different execution
     environments.  This generally implies padding empty rows and
     columns with blanks.  However, if `--force-merge' is given a
     numeric argument, the value of that argument is used instead of
     blanks to pad empty locations.  Note that `--force-merge' is
     different from `--force-merge=0' because data-merging functions
     (`mean', `max', etc.) ignore blanks but consider zeroes.

`-p' STRING, `--procs'=STRING
     When given a "merged" log file, unmerge only the data
     corresponding to the comma-separated processor ranges in STRING.
     For example, `--procs=0,16-20,25' unmerges the data for
     processors 0, 16, 17, 18, 19, 20, and 25.  By default,
     `ncptl-logextract' uses all of the data from a merged log file.

`-q', `--quiet'
     Suppress progress output.  Normally, `ncptl-logextract' outputs
     status information regarding its operation.  The `--quiet' option
     instruct `ncptl-logextract' to output only warning and error
     messages.

`-v', `--verbose'
     Increase progress output.  Normally, `ncptl-logextract' outputs
     basic status information regarding its operation.  The `--verbose'
     option instruct `ncptl-logextract' to output more detailed
     information.  Each time `--verbose' is specified, the program's
     verbosity increases (up to a maximum).

`-o' FILENAME, `--output'=FILENAME
     Redirect the output from `ncptl-logextract' to a file.  By default,
     `ncptl-logextract' writes to the standard output device.

   The above is merely a terse summary of the `ncptl-logextract'
command-line options.  The reader is directed to the Additional Options
section for descriptions of the numerous ways that `ncptl-logextract'
can format information.  Note that `--extract' and `--format' are the
two most common options as they specify what to extract and how to
format it; most of the remaining options in the Additional Options
section exist to provide precise control over formatting details.

ADDITIONAL OPTIONS
..................

The `ncptl-logextract' command-line options follow a hierarchy.  At the
top level is `--extract', which specifies which of the four types of
data `ncptl-logextract' should extract.  Next, `--format' specifies how
the extracted data should be formatted.  Valid values for `--format'
differ based on the argument to `--extract'.  Finally, there are
various format-specific options that fine-tune the formatted output.
Each output format accepts a different set of options.  Many of the
options appear at multiple places within the hierarchy, although
usually with different default values.

   The following hierarchical list describes all of the valid
combinations of `--extract', `--format', and the various
format-specific options:

`--extract=data' [default]
     Extract measurement data

    `--format=csv' [default]
          Output each table in comma-separated-value format

         `--noheaders'
               Do not output column headers

         `--colbegin='STRING
               Specify the text placed at the beginning of each data
               column [default: "`'"]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "`,'"]

         `--colend='STRING
               Specify the text placed at the end of each data column
               [default: "`'"]

         `--rowbegin='STRING
               Specify the text placed at the beginning of each data
               row [default: "`'"]

         `--rowsep='STRING
               Specify the text used to separate data rows [default:
               "`'"]

         `--rowend='STRING
               Specify the text placed at the end of each data row
               [default: "`\\n'"]

         `--hcolbegin='STRING
               Specify the text placed at the beginning of each header
               column [default: same as `colbegin']

         `--hcolsep='STRING
               Specify the text used to separate header columns
               [default: same as `colsep']

         `--hcolend='STRING
               Specify the text placed at the end of each header column
               [default: same as `colend']

         `--hrowbegin='STRING
               Specify the text placed at the beginning of each header
               row [default: same as `rowbegin']

         `--hrowsep='STRING
               Specify the text used to separate header rows [default:
               same as `rowsep']

         `--hrowend='STRING
               Specify the text placed at the end of each header row
               [default: same as `rowend']

         `--tablebegin='STRING
               Specify the text placed at the beginning of each table
               [default: "`'"]

         `--tablesep='STRING
               Specify the text used to separate tables [default:
               "`\\n'"]

         `--tableend='STRING
               Specify the text placed at the end of each table
               [default: "`'"]

         `--quote='STRING
               Specify the text used to begin quoted text [default:
               "`"'"]

         `--unquote='STRING
               Specify the text used to end quoted text [default: same
               as `quote']

         `--excel'
               Output strings in a format readable by Microsoft Excel

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


    `--format=tsv'
          Output each table in tab-separated-value format

         `--noheaders'
               Do not output column headers

         `--colbegin='STRING
               Specify the text placed at the beginning of each data
               column [default: "`'"]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "`\\t'"]

         `--colend='STRING
               Specify the text placed at the end of each data column
               [default: "`'"]

         `--rowbegin='STRING
               Specify the text placed at the beginning of each data
               row [default: "`'"]

         `--rowsep='STRING
               Specify the text used to separate data rows [default:
               "`'"]

         `--rowend='STRING
               Specify the text placed at the end of each data row
               [default: "`\\n'"]

         `--hcolbegin='STRING
               Specify the text placed at the beginning of each header
               column [default: same as `colbegin']

         `--hcolsep='STRING
               Specify the text used to separate header columns
               [default: same as `colsep']

         `--hcolend='STRING
               Specify the text placed at the end of each header column
               [default: same as `colend']

         `--hrowbegin='STRING
               Specify the text placed at the beginning of each header
               row [default: same as `rowbegin']

         `--hrowsep='STRING
               Specify the text used to separate header rows [default:
               same as `rowsep']

         `--hrowend='STRING
               Specify the text placed at the end of each header row
               [default: same as `rowend']

         `--tablebegin='STRING
               Specify the text placed at the beginning of each table
               [default: "`'"]

         `--tablesep='STRING
               Specify the text used to separate tables [default:
               "`\\n'"]

         `--tableend='STRING
               Specify the text placed at the end of each table
               [default: "`'"]

         `--quote='STRING
               Specify the text used to begin quoted text [default:
               "`"'"]

         `--unquote='STRING
               Specify the text used to end quoted text [default: same
               as `quote']

         `--excel'
               Output strings in a format readable by Microsoft Excel

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


    `--format=html'
          Output each table in HTML table format

         `--noheaders'
               Do not output column headers

         `--colbegin='STRING
               Specify the text placed at the beginning of each data
               column [default: "`<td>'"]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "` '"]

         `--colend='STRING
               Specify the text placed at the end of each data column
               [default: "`</td>'"]

         `--rowbegin='STRING
               Specify the text placed at the beginning of each data
               row [default: "`<tr>'"]

         `--rowsep='STRING
               Specify the text used to separate data rows [default:
               "`'"]

         `--rowend='STRING
               Specify the text placed at the end of each data row
               [default: "`</tr>\\n'"]

         `--hcolbegin='STRING
               Specify the text placed at the beginning of each header
               column [default: "`<th>'"]

         `--hcolsep='STRING
               Specify the text used to separate header columns
               [default: same as `colsep']

         `--hcolend='STRING
               Specify the text placed at the end of each header column
               [default: "`</th>'"]

         `--hrowbegin='STRING
               Specify the text placed at the beginning of each header
               row [default: same as `rowbegin']

         `--hrowsep='STRING
               Specify the text used to separate header rows [default:
               same as `rowsep']

         `--hrowend='STRING
               Specify the text placed at the end of each header row
               [default: same as `rowend']

         `--tablebegin='STRING
               Specify the text placed at the beginning of each table
               [default: "`<table>\\n'"]

         `--tablesep='STRING
               Specify the text used to separate tables [default: "`'"]

         `--tableend='STRING
               Specify the text placed at the end of each table
               [default: "`</table>\\n'"]

         `--quote='STRING
               Specify the text used to begin quoted text [default:
               "`'"]

         `--unquote='STRING
               Specify the text used to end quoted text [default: same
               as `quote']

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


    `--format=gnuplot'
          Output each table as a gnuplot data file

         `--noheaders'
               Do not output column headers

         `--colbegin='STRING
               Specify the text placed at the beginning of each data
               column [default: "`'"]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "` '"]

         `--colend='STRING
               Specify the text placed at the end of each data column
               [default: "`'"]

         `--rowbegin='STRING
               Specify the text placed at the beginning of each data
               row [default: "`'"]

         `--rowsep='STRING
               Specify the text used to separate data rows [default:
               "`'"]

         `--rowend='STRING
               Specify the text placed at the end of each data row
               [default: "`\\n'"]

         `--hcolbegin='STRING
               Specify the text placed at the beginning of each header
               column [default: same as `colbegin']

         `--hcolsep='STRING
               Specify the text used to separate header columns
               [default: same as `colsep']

         `--hcolend='STRING
               Specify the text placed at the end of each header column
               [default: same as `colend']

         `--hrowbegin='STRING
               Specify the text placed at the beginning of each header
               row [default: "`# '"

         `--hrowsep='STRING
               Specify the text used to separate header rows [default:
               same as `rowsep']

         `--hrowend='STRING
               Specify the text placed at the end of each header row
               [default: same as `rowend']

         `--tablebegin='STRING
               Specify the text placed at the beginning of each table
               [default: "`'"]

         `--tablesep='STRING
               Specify the text used to separate tables [default:
               "`\\n\\n'"]

         `--tableend='STRING
               Specify the text placed at the end of each table
               [default: "`'"]

         `--quote='STRING
               Specify the text used to begin quoted text [default:
               "`"'"]

         `--unquote='STRING
               Specify the text used to end quoted text [default: same
               as `quote']

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


    `--format=octave'
          Output each table as an Octave text-format data file

         `--noheaders'
               Do not output column headers

         `--colbegin='STRING
               Specify the text placed at the beginning of each data
               column [default: "`'"]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "`'"]

         `--colend='STRING
               Specify the text placed at the end of each data column
               [default: "`\\n'"]

         `--rowbegin='STRING
               Specify the text placed at the beginning of each data
               row [default: "`'"]

         `--rowend='STRING
               Specify the text placed at the end of each data row
               [default: "`'"]

         `--hcolbegin='STRING
               Specify the text placed at the beginning of each header
               column [default: "`'"]

         `--hcolsep='STRING
               Specify the text used to separate header columns
               [default: "`_'"]

         `--hcolend='STRING
               Specify the text placed at the end of each header column
               [default: "`'"]

         `--hrowbegin='STRING
               Specify the text placed at the beginning of each header
               row [default: "`# '"]

         `--hrowsep='STRING
               Specify the text used to separate header rows [default:
               "`'"]

         `--hrowend='STRING
               Specify the text placed at the end of each header row
               [default: "`\\n'"]

         `--tablebegin='STRING
               Specify the text placed at the beginning of each table
               [default: "`'"]

         `--tablesep='STRING
               Specify the text used to separate tables [default:
               "`\\n'"]

         `--tableend='STRING
               Specify the text placed at the end of each table
               [default: "`'"]

         `--quote='STRING
               Specify the text used to begin quoted text [default:
               "`'"]

         `--unquote='STRING
               Specify the text used to end quoted text [default: same
               as `quote']

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


    `--format=custom'
          Output each table in a completely user-specified format

         `--noheaders'
               Do not output column headers

         `--colbegin='STRING
               Specify the text placed at the beginning of each data
               column [default: "`'"]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "`'"]

         `--colend='STRING
               Specify the text placed at the end of each data column
               [default: "`'"]

         `--rowbegin='STRING
               Specify the text placed at the beginning of each data
               row [default: "`'"]

         `--rowsep='STRING
               Specify the text used to separate data rows [default:
               "`'"]

         `--rowend='STRING
               Specify the text placed at the end of each data row
               [default: "`'"]

         `--hcolbegin='STRING
               Specify the text placed at the beginning of each header
               column [default: same as `colbegin']

         `--hcolsep='STRING
               Specify the text used to separate header columns
               [default: same as `colsep']

         `--hcolend='STRING
               Specify the text placed at the end of each header column
               [default: same as `colend']

         `--hrowbegin='STRING
               Specify the text placed at the beginning of each header
               row [default: same as `rowbegin']

         `--hrowsep='STRING
               Specify the text used to separate header rows [default:
               same as `rowsep']

         `--hrowend='STRING
               Specify the text placed at the end of each header row
               [default: same as `rowend']

         `--tablebegin='STRING
               Specify the text placed at the beginning of each table
               [default: "`'"]

         `--tablesep='STRING
               Specify the text used to separate tables [default: "`'"]

         `--tableend='STRING
               Specify the text placed at the end of each table
               [default: "`'"]

         `--quote='STRING
               Specify the text used to begin quoted text [default:
               "`'"]

         `--unquote='STRING
               Specify the text used to end quoted text [default: same
               as `quote']

         `--excel'
               Output strings in a format readable by Microsoft Excel

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


    `--format=latex'
          Output each table as a LaTeX tabular environment

         `--dcolumn'
               Use the dcolumn package to align numbers on the decimal
               point

         `--booktabs'
               Use the booktabs package for a more professionally
               typeset look

         `--longtable'
               Use the longtable package to enable multi-page tables

         `--merge='FUNCTION
               Specify how to merge data from multiple files [default:
               "`mean'"]

         `--showfnames='OPTION
               Add an extra header row showing the filename the data
               came from [default: "`none'"]


`--extract=params'
     Extract the program's run-time parameters and environment variables

    `--format=text' [default]
          Output the parameters in plain-text format

         `--include='FILENAME
               Read from a file the list of keys to output

         `--exclude='REGEXP
               Ignore any keys whose name matches a regular expression

         `--sort'
               Sort the list of parameters alphabetically by key

         `--noenv'
               Exclude environment variables

         `--noparams'
               Exclude run-time parameters

         `--envformat='TEMPLATE
               Format environment variable names using the given
               template [default: "`%s (environment variable)'"]

         `--columns='NUMBER
               Output the parameters as a 1-, 2-, or 3-column table
               [default: 1]

         `--colsep='STRING
               Specify the text used to separate data columns [default:
               "`: '"]

         `--rowbegin='STRING
               Specify the text that's output at the start of each data
               row [default: "`'"]

         `--rowend='STRING
               Specify the text that's output at the end of each data
               row [default: "`\\n'"]


    `--format=dumpkeys'
          Output a list of the keys only (i.e., no values)

         `--include='FILENAME
               Read the list of parameters to output from a given file

         `--exclude='REGEXP
               Ignore any keys whose name matches a regular expression

         `--envformat='TEMPLATE
               Format environment variable names using the given
               template [default: "`%s (environment variable)'"]

         `--sort'
               Sort the list of parameters alphabetically by key

         `--noenv'
               Exclude environment variables

         `--noparams'
               Exclude run-time parameters


    `--format=latex'
          Output the parameters as a LaTeX tabular environment

         `--include='FILENAME
               Read from a file the list of keys to output

         `--exclude='REGEXP
               Ignore any keys whose name matches a regular expression

         `--envformat='TEMPLATE
               Format environment variable names using the given
               template [default: "`%s (environment variable)'"]

         `--sort'
               Sort the list of parameters alphabetically by key

         `--booktabs'
               Use the booktabs package for a more professionally
               typeset look

         `--tabularx'
               Use the tabularx package to enable line wraps within the
               value column

         `--longtable'
               Use the longtable package to enable multi-page tables

         `--noenv'
               Exclude environment variables

         `--noparams'
               Exclude run-time parameters


`--extract=env'
     Extract the environment in which the program was run

    `--format=sh' [default]
          Use Bourne shell syntax for setting environment variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


    `--format=bash'
          Use Bourne Again shell syntax for setting environment
          variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


    `--format=ksh'
          Use Korn shell syntax for setting environment variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


    `--format=csh'
          Use C shell syntax for setting environment variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


    `--format=zsh'
          Use Z shell syntax for setting environment variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


    `--format=tcsh'
          Use tcsh syntax for setting environment variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


    `--format=ash'
          Use ash syntax for setting environment variables

         `--newlines'
               Separate commands with newlines instead of semicolons

         `--unset'
               Unset all other environment variables

         `--chdir'
               Switch to the program's original working directory


`--extract=source'
     Extract CONCEPTUAL source code

    `--format=text' [default]
          Output the source code in plain-text format

         `--linebegin='STRING
               Specify the text placed at the beginning of each line
               [default: "`'"]

         `--lineend='STRING
               Specify the text placed at the end of each line
               [default: "`\\n'"]

         `--kwbegin='STRING
               Specify the text placed before each keyword [default:
               "`'"]

         `--kwend='STRING
               Specify the text placed after each keyword [default:
               "`'"]

         `--strbegin='STRING
               Specify the text placed before each string [default:
               "`'"]

         `--strend='STRING
               Specify the text placed after each string [default: "`'"]

         `--combegin='STRING
               Specify the text placed before each comment [default:
               "`'"]

         `--comend='STRING
               Specify the text placed after each comment [default:
               "`'"]

         `--indent='NUMBER
               Indent each line by a given number of spaces

         `--wrap='NUMBER
               Wrap the source code into a paragraph with a given
               character width


`--extract=warnings'
     Extract a list of warnings the program issued during initialization

    `--format=text' [default]
          Output warnings in plain-text format

         `--listbegin='STRING
               Specify text to appear at the beginning of the list
               [default: "`'"]

         `--listend='STRING
               Specify text to appear at the end of the list [default:
               "`'"]

         `--itembegin='STRING
               Specify text to appear before each warning [default:
               "`* '"]

         `--itemend='STRING
               Specify text to appear after each warning [default:
               "`\\n'"]


    `--format=html'
          Output warnings as an HTML list

         `--listbegin='STRING
               Specify text to appear at the beginning of the list
               [default: "`<ul>\\n'"]

         `--listend='STRING
               Specify text to appear at the end of the list [default:
               "`</ul>\\n'"]

         `--itembegin='STRING
               Specify text to appear before each warning [default:
               "`  <li>'"]

         `--itemend='STRING
               Specify text to appear after each warning [default:
               "`</li>\\n'"]


    `--format=latex'
          Output warnings as a LaTeX list

         `--listbegin='STRING
               Specify text to appear at the beginning of the list
               [default: "`\begin@{itemize@}\\n'"]

         `--listend='STRING
               Specify text to appear at the end of the list [default:
               "`\end@{itemize@}\\n'"]

         `--itembegin='STRING
               Specify text to appear before each warning [default:
               "`  \item '"]

         `--itemend='STRING
               Specify text to appear after each warning [default:
               "`\\n'"]


   The following represent additional clarification for some of the
above:

   * If `--indent' is specified without an argument, the argument
     defaults to `2'.

   * If `--wrap' is specified without an argument, the argument
     defaults to `72'.

   * The following are examples of the different arguments to the `--columns'
     option:

    `--columns=1' (default)
                 coNCePTuaL version: 1.0
                 coNCePTuaL backend: c_mpi
                 Average timer overhead [gettimeofday()]: <1 microsecond
                 Log creation time: Thu Mar 27 19:22:48 2003
                 Log completion time: Thu Mar 27 19:22:48 2003

    `--columns=2'
                 coNCePTuaL version:                      1.0
                 coNCePTuaL backend:                      c_mpi
                 Average timer overhead [gettimeofday()]: <1 microsecond
                 Log creation time:                       Thu Mar 27 19:22:48 2003
                 Log completion time:                     Thu Mar 27 19:22:48 2003

    `--columns=3'
                 coNCePTuaL version                     : 1.0
                 coNCePTuaL backend                     : c_mpi
                 Average timer overhead [gettimeofday()]: <1 microsecond
                 Log creation time                      : Thu Mar 27 19:22:48 2003
                 Log completion time                    : Thu Mar 27 19:22:48 2003


   * `--dumpkeys' produces suitable input for the `--include' option.

   * `--exclude' can be specified repeatedly on the command line.

   * `--merge' takes one of `mean' (arithmetic mean), `hmean' (harmonic
     mean), `min' (minimum), `max' (maximum), `median' (median), `sum'
     (sum), `all' (all values from each column), or `concat' (horizontal
     concatenation of all data), and applies the function to
     corresponding data values across all of the input files.  `--merge'
     can also accept a comma-separated list of the above functions, one
     per data column.  This enables a different merge operation to be
     used for each column.  For example, `--merge=min,min,mean' will
     take the minimum value across all files of each element in the
     first and second columns and the arithmetic mean across all files
     of each element in the third column.  If the number of
     comma-separated values differs from the number of columns and `--force-merge'
     is specified, `ncptl-logextract' will cycle over the given values
     until all columns are accounted for.  The `concat' merge type
     applies to all columns and therefore cannot be combined with any
     other merge type.  The difference between `--merge=all' and `--merge=concat'
     is that the former merges three files each with columns A and B as
     {A, A, A, B, B, B} while the latter merges the same files as {A,
     B, A, B, A, B}.

   * `--showfnames' prepends to each data table in the input file an
     extra header line indicating the log file the data was extracted
     from.  This option makes sense only when data is being extracted
     and primarily when `--merge=all' is specified.  `--showfnames'
     takes one of `none', `all', or `first'.  The default is `none',
     which doesn't add an extra header row.  `all' repeats the filename
     in each column of the extra header row.  `first' outputs the
     filename in only the first column, leaving the remaining columns
     with an empty string.  The following examples show how a sample
     data table is formatted with `--showfnames' set in turn to each of
     `none', `all', and `first':

        * Set to `none' (the default):

                   "Size","Value"
                   1,2
                   2,4
                   3,6

        * Set to `all' (filename repeated in each column of the first
          row):

                   "mydata.log","mydata.log"
                   "Size","Value"
                   1,2
                   2,4
                   3,6

        * Set to `first' (filename shown only in the first column of
          the first row):

                   "mydata.log",""
                   "Size","Value"
                   1,2
                   2,4
                   3,6


   * If `--format=params' is used with both `--longtable' and `--tabularx',
     the generated table will be formatted for use with the `ltxtable' LaTeX
     package.  See the `ltxtable' documentation for more information.


NOTES
.....

If no filenames are given, `ncptl-logextract' will read from the
standard input device.  If multiple log files are specified, CONCEPTUAL
will merge the data values and take all other information from the
first file specified.  Note, however, that all of the log files must
have been produced by the same CONCEPTUAL program and that that program
must have been run in the same environment.  In other words, only the
data values may change across log files; everything else must be
invariant.  See the description of `--merge' in the Additional Options
section for more information about merging data values from multiple
log files.

   `ncptl-logextract' treats certain files specially:

   * If `ncptl-logextract' is given a filename ending in `.gz', `.bz2',
     or `.Z' it automatically decompresses the file to a temporary
     location using `gunzip', `bunzip2', or `uncompress', as
     appropriate, then recursively processes the decompressed file.

   * If `ncptl-logextract' is given a filename ending in `.tar' or
     `.zip' it automatically extracts the file's contents to a temporary
     directory using `tar' or `unzip', as appropriate, then recursively
     processes the temporary directory.

   * If `ncptl-logextract' is given the name of a directory it processes
     all of the plain files found (recursively) beneath that directory.

   * If an input file is a merged CONCEPTUAL log file (i.e., produced by
     `ncptl-logmerge'), `ncptl-logextract' automatically invokes
     `ncptl-logunmerge' to split the file into its constituent, ordinary
     log files then recursively processes those.

   `ncptl-logmerge' treats filenames ending in `.tgz' as if they ended
in `.tar.gz' and filenames ending in `.taz' as if they ended in
`.tar.Z'.

   If the argument provided to any `ncptl-logextract' option begins with
an at sign ("`@'"), the value is treated as a filename and is replaced
by the file's contents.  To specify an non-filename argument that
begins with an at sign, merely prepend an additional "`@'":

`--this=that'
     The option `this' is given the value "`that'".

`--this=@that'
     The option `this' is set to the contents of the file called `that'.

`--this=@@that'
     The option `this' is given the value "`@that'".


EXAMPLES
........

For the following examples, we assume that `results.log' is the name of
a log file produced by a CONCEPTUAL program.

   Extract the data in CSV format and write it to `results.csv':

         ncptl-logextract --extract=data results.log --output=results.csv

   Note that `--extract=data' is the default and therefore optional:

         ncptl-logextract results.log --output=results.csv

   `ncptl-logextract' can combine data from multiple log files (using an
arithmetic mean by default):

         ncptl-logextract results-*.log --output=results.csv

   Put the data from all of the log files side-by-side and produce a CSV
file that Microsoft Excel can read directly:

         ncptl-logextract results-*.log --output=results.csv --merge=all \
            --showfnames=first --excel

   Output the data from `result.log' in tab-separated-value format:

         ncptl-logextract --format=tsv results.log

   Output the data in space-separated-value format:

         ncptl-logextract --colsep=" " results.log

   Use `gnuplot' to draw a PostScript graph of the data:

         ncptl-logextract results.log --format=gnuplot \
            --before=@params.gp | gnuplot > results.eps

   In the above, the `params.gp' file might contain `gnuplot' commands
such as the following:

         set terminal postscript eps enhanced color "Times-Roman" 30
         set output
         set logscale xy
         set data style linespoints
         set pointsize 3
         plot "-" title "Latency"

   (There should be an extra blank line at the end of the file because
`ncptl-logextract' strips off a trailing newline character whenever it
reads a file using "`@'".)

   Produce a complete HTML file of the data (noting that `--format=html'
produces only tables, not complete documents):

         ncptl-logextract --format=html
            --before='<html>\n<head>\n<title>Data</title>\n</head>\n<body>\n' \
            --after='</body>\n</html>\n' results.log

   Output the data as a LaTeX `tabular', relying on both the (standard)
`dcolumn' and (non-standard) `booktabs' packages for more attractive
formatting:

         ncptl-logextract --format=latex --dcolumn --booktabs \
           --output=results.tex results.log

   Output the run-time parameters in the form "KEY `--->' VALUE" with
all of the arrows aligned:

         ncptl-logextract results.log --extract=params --columns=3 --colsep=" --> "

   Output the run-time parameters as an HTML description list:

         ncptl-logextract results.log --extract=params --before='<dl>' \
           --rowbegin='<dt>' --colsep='</dt><dd>' --rowend='</dd>\n' \
           --after='</dl>\n'

   Restore the exact execution environment that was used to produce
`results.log', including the current working directory (assuming that
`bash' is the current command shell):

         eval `ncptl-logextract --extract=env --format=bash \
           --unset --chdir results.log`

   Set all of the environment variables that were used to produce
`results.log', overwriting--but not removing--whatever environment
variables are currently set (assuming that `tcsh' is the current
command shell):

         eval `ncptl-logextract --extract=env --format=tcsh results.log`

   Extract the source code that produced `results.log':

         ncptl-logextract --extract=source results.log

   Do the same, but indent the code by four spaces then re-wrap it into
a 60-column paragraph:

         ncptl-logextract --extract=source --indent=4 --wrap=60 results.log

   Here are a variety of ways to express the same thing:

         ncptl-logextract -e source --indent=4 --wrap=60 results.log

         ncptl-logextract -e source --indent=4 results.log --wrap=60

         cat results.log | ncptl-logextract --wrap=60 --indent=4 -e source

   Output the source code wrapped to 72 columns, with no indentation,
and formatted within an HTML preformatted-text block:

         ncptl-logextract --extract=source --wrap --before="<PRE>\n" \
           after="</PRE>\n" results.log

   List all of the warning messages which occur in `results.log':

         ncptl-logextract --extract=warnings results.log

SEE ALSO
........

ncptl-logmerge(1), ncptl-logunmerge(1), the CONCEPTUAL User's Guide

AUTHOR
......

Scott Pakin, <pakin@lanl.gov>


File: conceptual.info,  Node: ncptl-logmerge,  Next: ncptl-logunmerge,  Prev: ncptl-logextract,  Up: Interpreting coNCePTuaL log files

3.5.3 `ncptl-logmerge'
----------------------

CONCEPTUAL programs produce one log file per process.  An unwieldy
number of files can therefore be generated on large-scale computer
systems.  For the case in which only a single log file contains
measurement data, `ncptl-logmerge' can merge a number of log files into
a single file.  Only lines that differ across log files are repeated,
making the result fairly space-efficient.  The primary advantage of `ncptl-logmerge'
over an archiving program such as `tar' or `zip' is that the output of `ncptl-logmerge'
is designed to be human-readable--in fact, easily readable.

   `ncptl-logmerge' can also be used to highlight differences in log
files.  It is therefore an important diagnostic tool for unearthing
subtle configuration discrepancies across nodes in a large-scale
computer system.

   Running `ncptl-logmerge --usage' causes `ncptl-logmerge' to list a
synopsis of its core command-line options to the standard output
device; running `ncptl-logmerge --help' produces basic usage
information; and, running `ncptl-logmerge --man' outputs a complete
manual page.  *Note ncptl-logmerge manual page::, shows the `ncptl-logmerge'
documentation as produced by `ncptl-logmerge --man'.

* Menu:

* ncptl-logmerge manual page::     The result of running ``ncptl-logmerge --man''


File: conceptual.info,  Node: ncptl-logmerge manual page,  Prev: ncptl-logmerge,  Up: ncptl-logmerge

NAME
....

ncptl-logmerge - Merge CONCEPTUAL log files

SYNOPSIS
........

ncptl-logmerge `--usage' | `--help' | `--man'


ncptl-logmerge [`--output'=FILENAME] [`--simplify'] FILENAME...

DESCRIPTION
...........

A CONCEPTUAL program produces one log file per process.  For large
numbers of processes the result can be unwieldy.  `ncptl-logmerge'
combines a large set of log files into a single, merged file which can
later be expanded back into its constituent log files.  There are a
number of restrictions on the input to `ncptl-logmerge'; see the
Restrictions section for details.

   The merged output file does not modify lines which are identical in
all of the input files.  Lines which do differ across input files are
prefixed with the processors and processor ranges in which they
appeared.

   As an example, the following text was extracted from a set of 186
CONCEPTUAL log files (from a 186-processor run):

         # Microsecond timer type: PAPI_get_real_usec()
         # Average microsecond timer overhead: <1 microsecond
         #[0-4,6-12,14-16,18-52,54-78,80-94,96-101,103-121,123-140,142-169,
           171-185]# Microsecond timer increment: 1 +/- 0 microseconds
           (ideal: 1 +/- 0)
         #[5]# Microsecond timer increment: 1.00229 +/- 0.15854
           microseconds (ideal: 1 +/- 0)
         #[13]# Microsecond timer increment: 1.00228 +/- 0.158442
           microseconds (ideal: 1 +/- 0)
         #[17,79]# Microsecond timer increment: 1.00228 +/- 0.158392
           microseconds (ideal: 1 +/- 0)
         #[53]# Microsecond timer increment: 1.00228 +/- 0.158409
           microseconds (ideal: 1 +/- 0)
         #[102]# Microsecond timer increment: 1.00228 +/- 0.158458
           microseconds (ideal: 1 +/- 0)
         #[95,122]# Microsecond timer increment: 1.00228 +/- 0.158474
           microseconds (ideal: 1 +/- 0)
         #[141]# Microsecond timer increment: 1.00228 +/- 0.158491
           microseconds (ideal: 1 +/- 0)
         #[170]# Microsecond timer increment: 1.00228 +/- 0.158524
           microseconds (ideal: 1 +/- 0)

   All of the input files contained the same `Microsecond timer type'
and `Average microsecond timer overhead' lines.  However, the measured
`Microsecond timer increment' varied across input files.  While many of
the processors observed an increment of `1 +/- 0', processor 5 was
alone in observing `1.00229 +/- 0.15854'; processor 13 was alone in
observing `1.00228 +/- 0.158442'; and, both processor 17 and
processor 79 observed `1.00228 +/- 0.158392' as the timer increment.

   `ncptl-logmerge' can also be instructed to output only the lines
which differ across files.  Common lines are not output.  This feature
is useful for discovering misconfigured nodes in a large computer
system.  For example, on one computer system on which CONCEPTUAL was
run, five processors were running at a higher clock rate than the
remainder which naturally affected performance.  `ncptl-logmerge' can
be used to help identify such outliers.

OPTIONS
.......

`ncptl-logmerge' accepts the following command-line options:

`-u', `--usage'
     Output the Synopsis section then exit the program.

`-h', `--help'
     Output the Synopsis section and the Options section then exit the
     program.

`-m', `--man'
     Output a complete Unix man ("manual") page for `ncptl-logmerge'
     then exit the program.

`-o' FILENAME, `--output'=FILENAME
     `ncptl-logmerge' normally writes to the standard output device.
     The `--output' option redirects `ncptl-logmerge''s output to a
     file.

`-s', `--simplify'
     Simplify the output by including only lines which differ across
     input files.  No data is output, only prologue and epilogue
     comments.  `--simplify' can be specified up to four times on the
     command line:

    once
          Omit all comments and all lines which are identical across
          all input files.

    twice
          Lines which differ across ALL output files (e.g., `Processor
          (0<=P<tasks)') are also omitted.

    three times
          The amount of output is further reduced by rounding to two
          significant digits all numbers appearing in all input files.
          Doing so makes `1.10644 +/- 0.593714' match `1.12511 +/-
          0.58829', for example.  (Both are converted to `1.1 +/-
          0.59'.)

    four times
          Lists of processors are replaced by the list size.  For
          example, `#[22,67,86,430]' becomes `#[4]'.

     Note that `--simplify' is intended as a diagnostic tool; files
     output using `--simplify' cannot be un-merged to recover the
     original input files.

   In addition to the preceding options `ncptl-logmerge' requires a list
of log files to merge.  If a directory is specified, all of the files
immediately under that directory are used.  (Note that `ncptl-logmerge'
does not descend into subdirectories, however.)  Files containing lists
of filenames can be specified with a leading at sign ("`@'").  For
example, `@filelist.txt' means to read a list of filenames from
`@filelist.txt'.  Filenames beginning with an at sign can be specified
by doubling the at sign on the command line.

DIAGNOSTICS
...........

FILENAME `does not look like an unmerged coNCePTuaL log file'
     `ncptl-logmerge' accepts as input only log files produced directly
     by a CONCEPTUAL program.  It is not a general-purpose file
     combiner nor does it accept its own output as input.  Unrecognized
     input files cause `ncptl-logmerge' to abort with the preceding
     error message.

`No processor number found in' FILENAME
     `ncptl-logmerge' needs to map filenames to processor numbers to
     indicate which processors produced which lines of output.  If an
     input file does not contain a `Processor (0<=P<tasks)' comment,
     `ncptl-logmerge' aborts with the preceding error message.


EXAMPLES
........

Merge a set of CONCEPTUAL log files:

         ncptl-logmerge mybenchmark-[0-9]*.log > mybenchmark-all.log

   The following command is equivalent to the preceding one:

         ncptl-logmerge mybenchmark-[0-9]*.log --output=mybenchmark-all.log

   Show only "interesting" differences among the input files:

         ncptl-logmerge --simplify --simplify mybenchmark-[0-9]*.log

   For convenience, one can abbreviate `--simplify' `--simplify' `--simplify' `--simplify'
to `-s' `-s' `-s' `-s' or even `-ssss':

         ncptl-logmerge -ssss mybenchmark-[0-9]*.log

RESTRICTIONS
............

The log files passed to `ncptl-logmerge' are subject to the following
restrictions:

   * All files must be produced by the same run of the same CONCEPTUAL
     program.

   * None of the files can have been previously merged by
     `ncptl-logmerge' (i.e., `ncptl-logmerge' can't read its own
     output).

   * Only the first filename passed to `ncptl-logmerge' is allowed to
     contain data.  Data from all other files is discarded with a
     warning message.


BUGS
....

`ncptl-logmerge' is not a particularly robust script.  Specifically, it
is confused when input files contain different numbers of comment
lines.  For example, if one input file includes more environment
variables than another or issued a warning about a timer where another
input file didn't, `ncptl-logmerge' will erroneously report all
subsequent lines as being mismatched across input files.

SEE ALSO
........

ncptl-logunmerge(1), ncptl-logextract(1), the CONCEPTUAL User's Guide

AUTHOR
......

Scott Pakin, <pakin@lanl.gov>


File: conceptual.info,  Node: ncptl-logunmerge,  Prev: ncptl-logmerge,  Up: Interpreting coNCePTuaL log files

3.5.4 `ncptl-logunmerge'
------------------------

The primary capability of `ncptl-logmerge' (*note ncptl-logmerge::) is
to merge multiple CONCEPTUAL log files into a more maintainable single
file.  `ncptl-logunmerge' performs the complementary operation of
splitting that merged file back into the original set of CONCEPTUAL log
files.

   Running `ncptl-logunmerge --usage' causes `ncptl-logunmerge' to list
a synopsis of its core command-line options to the standard output
device; running `ncptl-logunmerge --help' produces basic usage
information; and, running `ncptl-logunmerge --man' outputs a complete
manual page.  *Note ncptl-logunmerge manual page::, shows the `ncptl-logunmerge'
documentation as produced by `ncptl-logunmerge --man'.

* Menu:

* ncptl-logunmerge manual page::   The result of running ``ncptl-logunmerge --man''


File: conceptual.info,  Node: ncptl-logunmerge manual page,  Prev: ncptl-logunmerge,  Up: ncptl-logunmerge

NAME
....

ncptl-logunmerge - Recover individual CONCEPTUAL log files from
ncptl-logmerge output

SYNOPSIS
........

ncptl-logunmerge `--usage' | `--help' | `--man'


ncptl-logunmerge [`--logfile'=TEMPLATE] [`--procs'=PROCESS_LIST] [`--quiet']
[`--memcache'=MEGABYTES] FILENAME

DESCRIPTION
...........

`ncptl-logmerge' merges a set of CONCEPTUAL log files into a more
convenient, single file.  `ncptl-logunmerge' performs the inverse
operation, splitting a merged file into separate CONCEPTUAL log files.
Specifically, unadorned comment lines such as the following are written
to all log files:

         # Executable name: /home/me/mybenchmark

   Comment lines which specify processor ranges are written to the
appropriate log files.  For example, the following line--with the
leading `#[35,43,89]' stripped--is written only to the log files
corresponding to processes 35, 43, and 89:

         #[35,43,89]# Minimum sleep time: 9.68 +/- 0.556776
           microseconds (ideal: 1 +/- 0)

   Non-comment lines (i.e., measurement data) such as the following are
written only to process 0's log file:

         "Contention factor","Msg. size (B)","1/2 RTT (us)","MB/s"
         "(all data)","(all data)","(all data)","(all data)"
         0,1048576,1368.0295,730.978389
         0,524288,691.9115,722.6357706

OPTIONS
.......

`ncptl-logunmerge' accepts the following command-line options:

`-u', `--usage'
     Output the Synopsis section then exit the program.

`-h', `--help'
     Output the Synopsis section and the Options section then exit the
     program.

`-m', `--man'
     Output a complete Unix man ("manual") page for `ncptl-logunmerge'
     then exit the program.

`-L' TEMPLATE, `--logfile'=TEMPLATE
     Specify a template for the names of the generated log files.  The
     template must contain the literal string `%p' which will be
     replaced by the appropriate processor number.  The template may
     contain the literal string `%r' (run number) which will be
     replaced by the smallest integer which produces a filename that
     does not already exist.  In addition, `printf()'-style field
     widths can be used with `%p' and `%r'.  For example, `%04p'
     outputs the processor number as a four-digit number padded on the
     left with zeroes.

     If `--logfile' is not specified, `ncptl-logunmerge' takes the
     default template from the merged log file's `Log-file template'
     line, discards the directory component of the filename, and uses
     the result as the log-file template.

`-p' PROCESS_LIST, `--procs'=PROCESS_LIST
     Identify a subset of log files to extract from the merged log file.
     By default, `ncptl-logunmerge' extracts all of the constituent log
     files.  PROCESS_LIST is a comma-separated list of process number or
     process ranges.

`-q', `--quiet'
     Suppress progress output.  Normally, `ncptl-logunmerge' outputs
     status information regarding its operation.  The `--quiet' option
     instruct `ncptl-logunmerge' to output only warning and error
     messages.

`-M' MEGABYTES, `--memcache'=MEGABYTES
     Specify the size of the in-memory file cache.  By default the
     program keeps up to 8 MB of extracted file data resident in memory
     to improve performance.  The `--memcache' option enables more or
     less data to be cached in memory.  For example, `--memcache=512'
     specifies that 512 MB of memory should be reserved for the file
     cache.

   In addition to the preceding options `ncptl-logunmerge' requires the
name of a merged CONCEPTUAL log file.  If not provided,
`ncptl-logunmerge' reads the contents of the merged CONCEPTUAL log file
from standard input.

DIAGNOSTICS
...........

`The input file does not look like a merged coNCePTuaL log file'
     Only the output from `ncptl-logmerge' is valid input to
     `ncptl-logunmerge'.

`Unable to find a unique number of tasks in the input file'
     `ncptl-logunmerge' determines the number of files to generate from
     the `Number of tasks' prologue comment.  If that comment does not
     appear or takes on different values, `ncptl-logunmerge' rejects the
     input file.


EXAMPLES
........

Extract a set of CONCEPTUAL log files from a merged log file and name
the extracted files `happy-0.log', `happy-1.log', `happy-2.log', etc.:

         ncptl-logunmerge --logfile=happy-%p.log mybenchmark-all.log

   Extract only `mybenchmark-0.log', `mybenchmark-50.log',
`mybenchmark-51.log', `mybenchmark-52.log', and `mybenchmark-100.log'
from `mybenchmark-all.log' (assuming that `mybenchmark-all.log'
contains the line `# Log-file template: mybenchmark-%p.log'):

         ncptl-logunmerge --procs=0,50-52,100 mybenchmark-all.log

SEE ALSO
........

ncptl-logmerge(1), ncptl-logextract(1), printf(3), the CONCEPTUAL
User's Guide

AUTHOR
......

Scott Pakin, <pakin@lanl.gov>


File: conceptual.info,  Node: Grammar,  Next: Examples,  Prev: Usage,  Up: Top

4 Grammar
*********

The CONCEPTUAL language was designed to produce precise specifications
of network correctness and performance tests yet read like an
English-language document that contains a hint of mathematical
notation.  Unlike more traditional programming languages, CONCEPTUAL is
more descriptive than imperative.  There are no classes, functions,
arrays, pointers, or even variable assignments (although expressions
can be let-bound to identifiers).(1) The language operates primarily on
integers, with support for string constants and floating-point numbers
in only a few constructs.  A CONCEPTUAL program merely describes a
communication pattern and the CONCEPTUAL compiler generates code to
implement that pattern.

   As a domain-specific language, CONCEPTUAL contains primitives to send
and receive messages.  It is capable of measuring time, computing
statistics, and logging results.  It knows that it will be run in a
shared-nothing SPMD(2) style with explicit message-passing.  As a
result of its special-purpose design CONCEPTUAL can express
communication patterns in a clearer and terser style than is possible
using a general-purpose programming language.

   The CONCEPTUAL language is case-insensitive.  `Hello' is the same as
`HELLO' or `hello'.  Furthermore, whitespace is insignificant; one
space has the same meaning as multiple spaces.  Comments are designated
with a `#' character and extend to the end of the line.

   We now describe the CONCEPTUAL grammar in a bottom-up manner, i.e.,
starting from primitives and working up to complete programs.  Note
that many of the sections in this chapter use the following syntax to
formally describe language elements:

<NONTERMINAL>
     a placeholder for a list of language primitives and additional
     placeholders

::=
     "is defined as"

`KEYWORD'
     a primitive with special meaning to the language

[...]
     optional items

(...)
     grouping of multiple items into one

*
     zero or more occurrences of the preceding item

+
     one or more occurrences of the preceding item

|
     either the item to the left or the item to the right but not both

* Menu:

* Primitives::                  Identifiers, strings, and integers
* Expressions::                 Ways to combine primitives
* Task descriptions::           Selecting groups of tasks at once
* Communication statements::    Sending and receiving messages
* I/O statements::              Relaying status and logging results
* Counter and timer statements::
* Complex statements::          Combining statements into larger entities
* Other statements::            Statements for neither communication nor I/O
* Header declarations::         Language versioning and command-line parsing
* Complete programs::           Arguments + complex statements = programs
* Summary of the grammar::      Repeat of the EBNF rules that appeared above

   ---------- Footnotes ----------

   (1) CONCEPTUAL is not even Turing-complete.  That is, it cannot
perform arbitrary computations.

   (2) Single Program, Multiple Data


File: conceptual.info,  Node: Primitives,  Next: Expressions,  Prev: Grammar,  Up: Grammar

4.1 Primitives
==============

At the lowest level, CONCEPTUAL programs are composed of identifiers,
strings, and integers (and a modicum of punctuation).  Identifiers
consist of a letter followed by zero or more alphanumerics or
underscores.  `potato', `x', and `This_is_program_123' are all examples
of valid identifiers.  Identifiers are used for two purposes: variables
and keywords.  Variables--referred to in the formal grammar as <IDENT>s--can
be bound but not assigned.  That is, once a variable is given a value
it retains that value for the entire scope although it may be given a
different value within a subordinate scope for the duration of that
scope.  All variables are of integer type.  There are a number of
variables that are predeclared and maintained automatically by
CONCEPTUAL.  These are listed and described in *note Predeclared
variables::.  Predeclared variables can be used by CONCEPTUAL programs
but cannot be redeclared; an attempt to do so will result in a
compile-time error message.

   Keywords introduce actions.  For example, `SEND' and `RECEIVE' are
keywords.  (A complete list of CONCEPTUAL keywords is presented in
*note Keywords::.)  Most keywords can appear in multiple forms.  For
example, `OUTPUT' and `OUTPUTS' are synonymous, as are `COMPUTE' and `COMPUTES', `A'
and `AN', and `TASK' and `TASKS'.  The intention is for programs to use
whichever sounds better in an English-language sentence.  Keywords may
not be used as variable names; an attempt to do so will cause the
compiler to output a parse error.

   As a special case to increase program readability, a single `-'
preceding a keyword is treated as a whitespace character.  Hence,
`INTEGER-SIZED PAGE-ALIGNED MESSAGE' is equivalent to `INTEGER SIZED
PAGE ALIGNED MESSAGE' and `10 64-BYTE MESSAGES' is equivalent to `10 64
BYTE MESSAGES'.  However, `x-3' and `3-x' still represent subtraction
operations.

   Although identifiers are case insensitive--`SEND' is the same as
`send' is the same as `sENd'--to increase clarity, this manual presents
keywords in uppercase and variables in lowercase.

   Strings consist of double-quoted text.  Within a string--and only
within a string--whitespace and case are significant.  In particular, a
literal newline is honored but can be suppressed by preceding it with a
backslash as in the following example:

     "This string\
     contains some
     newline characters."
=> This string contains some
newline characters.

   Use `\"' for a double-quote character, `\\' for a backslash, and
`\n' for a newline character.  All other escape sequences produce a
warning message and are discarded.  As examples of valid
escape-sequence usage, the string `"August 2009"' represents the text
"August 2009" and `"I store \"stuff\" in C:\\MyStuff."' represents the
text "I store "stuff" in C:\MyStuff."

   Integers consist of an optional `+' or `-'(1) followed by one or
more digits followed by an optional multiplier.  This multiplier is
unique to CONCEPTUAL and consists of one of the following four letters:

`K' (kilo)
     multiplies the integer by 1,024

`M' (mega)
     multiplies the integer by 1,048,576

`G' (giga)
     multiplies the integer by 1,073,741,824

`T' (tera)
     multiplies the integer by 1,099,511,627,776

In addition, a multiplier can be `E' (exponent) followed by a positive
integer.  An `E' multiplier multiplies the base integer by 10 raised to
the power of the alternate integer.

   Some examples of valid integers include `2009', `-42', `64K'
(= 65,536), and `8E3' (= 8,000).

   ---------- Footnotes ----------

   (1) From the lexer's perspective, integers are always unsigned and
`+' or `-' are merely operators (*note Arithmetic expressions::).  The
parser, however, applies unary operators to integer literals.  This
alteration is evident in the output of the `dot_ast' backend (*note The
dot_ast backend::).


File: conceptual.info,  Node: Expressions,  Next: Task descriptions,  Prev: Primitives,  Up: Grammar

4.2 Expressions
===============

Expressions, as in any language, are a combination of primitives and
other expressions in a semantically meaningful juxtaposition.
CONCEPTUAL provides arithmetic expressions, which evaluate to a number,
and relational expressions, which evaluate to either TRUE or FALSE.  In
addition, CONCEPTUAL provides the notion of an "aggregate expression",
which represents a function (e.g., statistical mean) applied to every
value taken on by an arithmetic expression during the run of a program.

* Menu:

* Arithmetic expressions::      Arithmetic expressions
* Built-in functions::          List of additional arithmetics
* Aggregate expressions::       Expressions with a function applied
                                to all instances
* Aggregate functions::         List of functions allowed in the above
* Relational expressions::      Relating one expression to another


File: conceptual.info,  Node: Arithmetic expressions,  Next: Built-in functions,  Prev: Expressions,  Up: Expressions

4.2.1 Arithmetic expressions
----------------------------

CONCEPTUAL supports a variety of arithmetic expressions.  The following
is the language's order of operations from highest to lowest precedence:

unary            `+', `-', `NOT', `<FUNCTION>(<EXPR>, ...)', `REAL(<EXPR>)'
power            `**'
multiplicative   `*', `/', `MOD', `<<', `>>', `&'
additive         `+', `-', `|', `XOR'
conditional      `<EXPR> `IF' <REL_EXPR> `OTHERWISE' <EXPR>'

In addition, as in most programming languages, parentheses can be used
to group subexpressions.

   The `&' ("and"), `|' ("or"), `XOR', and `NOT' operators perform
bitwise, not logical, operations.  That is, they accept numerical
arguments, not truth-value arguments.  Hence, for example, `3 | 5' is
equal to `7'.

   `<<' and `>>' are bit-shift operators.  That is, `a << b' is the
CONCEPTUAL equivalent of the mathematical expression a * 2^b and `a >>
b' is the CONCEPTUAL equivalent of the mathematical expression a / 2^b.
Consequently, negative values of b are valid and correspond to a shift
in the opposite direction.  (In contrast, C and Perl treat a negative
shift amount as a--usually large--unsigned number and Python raises a
`ValueError' exception on negative shifts.)

   `MOD' is a modulo (i.e., remainder) operator: `10 MOD 3' returns
`1'.  `MOD' is guaranteed to return a nonnegative remainder.  Hence,
`16 MOD 7' and `16 MOD -7' both return `2' even though `-5' is also
mathematically valid.  Similarly, `-16 MOD 7' and `-16 MOD -7' both
return `5' even though `-2' is also mathematically valid.

   The function calls allowed in `<FUNCTION>(<EXPR>, ...)' are listed
and described in *note Built-in functions::.  All functions take one or
more arithmetic expressions as an argument.  The operator `*'
represents multiplication; `/' represents division; and `**' represents
exponentiation (i.e., `x ** y' == x^y, rounded towards zero).  Note
that 0^y generates a run-time error for y <= 0.

   A conditional expression `<EXPR1> `IF' <REL_EXPR> `OTHERWISE' <EXPR2>'
evaluates to <EXPR1> if the relational expression <REL_EXPR> evaluates
to TRUE and <EXPR2> if <REL_EXPR> evaluates to FALSE.(1)  Relational
expressions are described in *note Relational expressions::.  As some
examples of conditional expressions, `666 IF 2+2=5 OTHERWISE 777'
returns `777' while `666 IF 2+2=4 OTHERWISE 777' returns `666'.

   All operations proceed left-to-right except power and conditional
expressions, which proceed right-to-left.  That is, `4-3-2' means
(4-3)-2 but `4**3**2' means 4^(3^2).  Similarly, `2 IF p=0 OTHERWISE 1
IF p=1 OTHERWISE 0' associates like `2 IF p=0 OTHERWISE (1 IF p=1
OTHERWISE 0)', not like `(2 IF p=0 OTHERWISE 1) IF p=1 OTHERWISE 0'.

* Menu:

* Evaluation contexts::         Integer context vs. floating-point context
* Formal grammar for arithmetic expressions::  EBNF version of the preceding
                                               prose

   ---------- Footnotes ----------

   (1) It is therefore analogous to `<REL_EXPR> ? <EXPR1> : <EXPR2>' in
the C programming language.


File: conceptual.info,  Node: Evaluation contexts,  Next: Formal grammar for arithmetic expressions,  Prev: Arithmetic expressions,  Up: Arithmetic expressions

Evaluation contexts
...................

CONCEPTUAL normally evaluates arithmetic expressions in "integer
context", meaning that each subexpression is truncated to the nearest
integer after being evaluated.  Hence, `24/5*5' is `20', not `24',
because `24/5*5' = `(24/5)*5' = `4*5' = `20'.  There are a few
situations, however, in which CONCEPTUAL evaluates expressions in
"floating-point context", meaning that no truncation occurs:

   * within an `OUTPUTS' statement (*note Writing to standard output::)

   * within a `LOGS' statement (*note Writing to a log file::)

   * within a `BACKEND EXECUTES' statement (*note Injecting arbitrary
     code::)

   * within a range in a `FOR EACH' statement (*note Range loops::)
     when CONCEPTUAL is unable to find an arithmetic or geometric
     progression by evaluating the component <EXPR>s in integer context


Within any of the preceding statements, the expression `24/5*5'
evaluates to 24.  Furthermore, the expression `24/5' evaluates to 4.8,
which is a number that can't be entered directly in a CONCEPTUAL
program.  (The language supports only integral constants, as mentioned
in *note Primitives::.)

   The CONCEPTUAL language provides a special form called `REAL' which
resembles a single-argument function.  When evaluated in floating-point
context, `REAL' returns its argument evaluated normally, as if `REAL'
were absent.  When evaluated in integer context, however, `REAL'
evalutes its argument in floating-point context and then rounds the
result to the nearest integer.  For example, in integer context, `9/2 +
1/2' is `4' while `REAL(9/2 + 1/2)' is `5'.


File: conceptual.info,  Node: Formal grammar for arithmetic expressions,  Prev: Evaluation contexts,  Up: Arithmetic expressions

Formal grammar for arithmetic expressions
.........................................

For completeness, the following productions formalize the process by
which CONCEPTUAL parses arithmetic expressions:

<EXPR>   ::=   <COND_EXPR>

<COND_EXPR>   ::=   <ADD_EXPR> `IF' <REL_EXPR> `OTHERWISE' <ADD_EXPR>

<ADD_EXPR>   ::=   <MULT_EXPR>
             |     <ADD_EXPR> `+' <MULT_EXPR>
             |     <ADD_EXPR> `-' <MULT_EXPR>
             |     <ADD_EXPR> `|' <MULT_EXPR>
             |     <ADD_EXPR> `XOR' <MULT_EXPR>

<MULT_EXPR>   ::=   <UNARY_EXPR>
              |     <MULT_EXPR> `*' <UNARY_EXPR>
              |     <MULT_EXPR> `/' <UNARY_EXPR>
              |     <MULT_EXPR> `MOD' <UNARY_EXPR>
              |     <MULT_EXPR> `>>' <UNARY_EXPR>
              |     <MULT_EXPR> `<<' <UNARY_EXPR>
              |     <MULT_EXPR> `&' <UNARY_EXPR>

<POWER_EXPR>   ::=   <PRIMARY_EXPR> [`**' <UNARY_EXPR>]

<UNARY_EXPR>   ::=   <POWER_EXPR>
               |     <UNARY_OPERATOR> <UNARY_EXPR>

<UNARY_OPERATOR>   ::=   `+' | `-' | `NOT'

<PRIMARY_EXPR>   ::=   `(' <EXPR> `)'
                 |     <IDENT>
                 |     <INTEGER>
                 |     <FUNC_NAME> `(' <ENUMERATED_EXPRS> `)'
                 |     `REAL' `(' <EXPR> `)'

<ENUMERATED_EXPRS>   ::=   <EXPR> [`,' <EXPR>]*


File: conceptual.info,  Node: Built-in functions,  Next: Aggregate expressions,  Prev: Arithmetic expressions,  Up: Expressions

4.2.2 Built-in functions
------------------------

In addition to the operators described in *note Arithmetic
expressions::, CONCEPTUAL contains a number of built-in functions that
perform a variety of arithmetic operations that are often found to be
useful in network correctness and performance testing codes.  These
include simple functions that map one number to another as well as a
set of topology-specific functions that help implement communication
across various topologies, specifically N-ary trees, meshes, tori, and
K-nomial trees.  CONCEPTUAL currently supports the following functions:

<FUNC_NAME>   ::=   `ABS' | `BITS' | `CBRT' | `FACTOR10' | `LOG10' |      `MAX' | `MIN' | `ROOT' |
                    `SQRT'
              |     `CEILING' | `FLOOR' | `ROUND'
              |     `TREE_PARENT' | `TREE_CHILD'
              |     `KNOMIAL_PARENT' | `KNOMIAL_CHILD' | `KNOMIAL_CHILDREN'
              |     `MESH_NEIGHBOR' | `MESH_COORDINATE'
              |     `TORUS_NEIGHBOR' | `TORUS_COORDINATE'
              |     `RANDOM_UNIFORM' | `RANDOM_GAUSSIAN' | `RANDOM_POISSON'

   All of the above take as an argument one or more integers (which may
be the result of an arithmetic expression).  The following sections
describe each function in turn.

* Menu:

* Integer functions::           Map one integer to another
* Floating-point functions::    Map one floating-point value to another
* n-ary tree functions::        Find parents and children in n-ary trees
* k-nomial tree functions::     Find parents and children in k-nomial trees
* Mesh functions::              Find neighbors in 1-D, 2-D, and 3-D meshes
* Torus functions::             Find neighbors in 1-D, 2-D, and 3-D tori
* Random-number functions::     Generate unsynchronized pseudorandom numbers


File: conceptual.info,  Node: Integer functions,  Next: Floating-point functions,  Prev: Built-in functions,  Up: Built-in functions

Integer functions
.................

`ABS' returns the absolute value of its argument.  For example,
`ABS(99)' and `ABS(-99)' are both `99'.

   `BITS' returns the minimum number of bits needed to store its
argument.  For example, `BITS(12345)' is `14' because 2^14 is 16,384,
which is larger than 12,345, while 2^13 is 8,192, which is too small.
`BITS(0)' is defined to be `0'.  Essentially, `BITS(x)' represents the
ceiling of the base-2 logarithm of x.  Negative numbers are treated as
their two's-complement equivalent.  For example, `BITS(-1)' returns
`32' on a 32-bit system and `64' on a 64-bit system.

   `CBRT' is an integer cube root function.  It is essentially just
syntactic sugar for the more general `ROOT' function: `CBRT(X)' =
`ROOT(3, X)'.

   `FACTOR10' rounds its argument down (more precisely, towards zero)
to the largest single-digit factor of an integral power of 10.
`FACTOR10(4975)' is therefore `4000'.  Similarly, `FACTOR10(-4975)'
is `-4000'.

   `LOG10(x)' is the floor of the base-10 logarithm of x.  For instance,
`LOG10(12345)' is `4' because 10^4 is the largest integral power of 10
that does not exceed 12,345.

   `MIN' and `MAX' return, respectively, the minimum and maximum value
in a list of numbers.  Unlike the other built-in functions, `MIN' and `MAX'
accept an arbitrary number of arguments (but at least one).  For
example, `MIN(8,6,7,5,3,0,9)' is `0' and `MAX(8,6,7,5,3,0,9)' is `9'.

   `ROOT(n, x)' returns the nth root of x.  More precisely, it returns
the largest integer r such that r^n <= x.  `ROOT' is not currently
defined on negative values of x but this may change in a future release
of CONCEPTUAL.  As an example of `ROOT' usage, `ROOT(5, 245)' is `3'
because 3^5 = 243 <= 245 but 4^5 = 1024 > 245.  Similarly,
`ROOT(2, 16)' = `4'; `ROOT(3, 27)' = `3'; `ROOT(0, 0)' and `ROOT(4,
-245)' each return a run-time error; and, `ROOT(-3, 8)' = `0' (because
`ROOT(-3, 8)' = `1/ROOT(3, 8)' = `1/2' = `0').

   `SQRT' is an integer square root function.  It is essentially just
syntactic sugar for the more general `ROOT' function: `SQRT(X)' =
`ROOT(2, X)'.


File: conceptual.info,  Node: Floating-point functions,  Next: n-ary tree functions,  Prev: Integer functions,  Up: Built-in functions

Floating-point functions
........................

As stated in *note Arithmetic expressions::, there are certain
constructs in which expressions are evaluated in floating-point context
instead of integer context.  In such constructs, all of CONCEPTUAL's
built-in functions return floating-point values.  Furthermore, the `CBRT', `LOG10', `ROOT',
and `SQRT' functions (*note Integer functions::) compute floating-point
results, not integer results which are coerced into floating-point
format.

   The following functions are not meaningful in integer context but are
in floating-point context:

   * `CEILING'

   * `FLOOR'

   * `ROUND'

   `CEILING' returns the smallest integer not less than its argument.
For example, `CEILING(-7777/10)' is `-777'.  (-778 is less than -777.7
while -777 is not less than -777.7.)

   `FLOOR' returns the largest integer not greater than its argument.
For example, `FLOOR(-7777/10)' is `-778'.  (-778 is not greater than
-777.7 while -777 is greater than -777.7.)

   `ROUND' rounds its argument to the nearest integer.  For example,
`ROUND(-7777/10)' is `-778'.

   It is not an error to use `CEILING', `FLOOR', and `ROUND' in an
integer context; each function merely return its argument unmodified.


File: conceptual.info,  Node: n-ary tree functions,  Next: k-nomial tree functions,  Prev: Floating-point functions,  Up: Built-in functions

n-ary tree functions
....................

N-ary trees are used quite frequently in communication patterns because
they require only logarithmic time (in the number of tasks) for a
message to propagate from the root to a leaf.  CONCEPTUAL supports
N-ary trees in the form of the `TREE_PARENT' and `TREE_CHILD' functions.

 -- Function: TREE_PARENT (TASK_ID [, FAN-OUT])
     `TREE_PARENT' takes a task number and an optional tree fan-out (n)
     and returns the task's parent in an N-ary tree.  n defaults to
     `2', i.e., a binary tree.  Taking the `TREE_PARENT' of any task
     less than 1 returns the value `-1'.

 -- Function: TREE_CHILD (TASK_ID, CHILD [, FAN-OUT])
     `TREE_CHILD' takes a task number, a child number (0 <= i < N), and
     an optional tree fan-out (n), which again defaults to `2'.  It
     returns the task number corresponding to the given task's childth
     child.

   The following illustrations show how tasks are numbered in,
respectively, a 2-ary and a 3-ary tree:


                                  0
                              +--/ \--+
                             /         \
                            1           2
                          /   \       /   \
                         3     4     5     6
                        / \   / \   / \   / \
                       7   8 9  10 11 12 13 14

                                  0
                                 /|\
                           +----/ | \----+
                          /       |       \
                         1        2        3
                       / | \    / | \    / | \
                      4  5  6  7  8  9 10 11 12

As shown by the 2-ary tree, task 1's children are task 3 and task 4.
Therefore, `TREE_PARENT(3)' and `TREE_PARENT(4)' are both `1';
`TREE_CHILD(1, 0)' is `3'; and, `TREE_CHILD(1, 1)' is `4'.  In a 3-ary
tree, each task has three children.  Hence, the following expressions
hold:

   * `TREE_PARENT(7, 3)' => `2'

   * `TREE_PARENT(8, 3)' => `2'

   * `TREE_PARENT(9, 3)' => `2'

   * `TREE_CHILD(2, 0, 3)' => `7'

   * `TREE_CHILD(2, 1, 3)' => `8'

   * `TREE_CHILD(2, 2, 3)' => `9'


File: conceptual.info,  Node: k-nomial tree functions,  Next: Mesh functions,  Prev: n-ary tree functions,  Up: Built-in functions

K-nomial tree functions
.......................

K-nomial trees are an efficient way to implement
collective-communication operations in software.  Unlike in an N-ary
tree, the number of children in a K-nomial tree decreases with
increasing task depth (i.e., no task has more children than the root).
The advantage is that the tasks that start communicating earlier
perform more work, which reduces the total latency of the collective
operation.  In contrast, in an N-ary tree, the tasks that start
communicating earlier finish earlier, at the expense of increased total
latency.  CONCEPTUAL supports K-nomial trees via the `KNOMIAL_PARENT', `KNOMIAL_CHILDREN',
and `KNOMIAL_CHILD' functions, as described below.

 -- Function: KNOMIAL_PARENT (TASK_ID [, FAN_OUT [, NUM_TASKS]])
     `KNOMIAL_PARENT' takes a task number, the tree fan-out factor (the
     "K" in "K-ary"), and the number of tasks in the tree.  It returns
     the task ID of the given task's parent.  FAN_OUT defaults to `2'
     and the number of tasks defaults to `num_tasks' (*note Predeclared
     variables::).

 -- Function: KNOMIAL_CHILDREN (TASK_ID [, FAN_OUT [, NUM_TASKS]])
     `KNOMIAL_CHILDREN' takes the same arguments as `KNOMIAL_PARENT'
     but returns the number of immediate descendents the given task has.

 -- Function: KNOMIAL_CHILD (TASK_ID, CHILD [, FAN_OUT [, NUM_TASKS]])
     `KNOMIAL_CHILD' takes a task number, a child number (0 <= i <
     `KNOMIAL_CHILDREN(...)'), the tree fan-out factor, and the number
     of tasks in the tree.  It returns the task number corresponding to
     the given task's ith child.  As in `KNOMIAL_PARENT' and `KNOMIAL_CHILDREN',
     FAN_OUT defaults to `2' and the number of tasks defaults to `num_tasks'
     (*note Predeclared variables::).

   The following figure shows how CONCEPTUAL numbers tasks in a
K-nomial tree with k=2 (a.k.a. a 2-nomial or binomial tree).


                                   0
                                  /|\
                                 1 | |
                                /| | |
                               3 | 2 |
                               | | | |
                               7 5 6 4

The figure is structured with time flowing downwards.  That is, for a
multicast operation expressed over a 2-nomial tree, task 0 sends a
message to task 1 in the first time step.  Then, task 0 sends to task 2
while task 1 sends to task 3.  In the final step, task 0 sends to
task 4, task 1 sends to task 5, task 2 sends to task 6, and task 3
sends to task 7--all concurrently.  The following expressions also
hold, assuming there are a total of eight tasks in the computation:

   * `KNOMIAL_PARENT(0)' => `-1'

   * `KNOMIAL_PARENT(1)' => `0'

   * `KNOMIAL_CHILDREN(1)' => `2'

   * `KNOMIAL_CHILD(1, 0)' => `3'

   * `KNOMIAL_CHILD(1, 1)' => `5'

   * `KNOMIAL_CHILDREN(7)' => `0'

   * `KNOMIAL_CHILD(7, 0)' => `-1'

   K-nomial trees for k>2 are much less common in practice than
2-nomial trees.  However, they may perform well when a task has
sufficient bandwidth to support multiple, simultaneous, outgoing
messages.  For example, a trinomial tree (i.e., a K-nomial tree with
k=3) should exhibit good performance if there is enough bandwidth to
send two messages simultaneously.  The following illustration shows how
CONCEPTUAL constructs a 27-task trinomial tree:


                                     0
                                     |
            +--------------------+---+-------+------+----+---+
            |                    |           |      |    |   |
            1                    2           |      |    |   |
            |                    |           |      |    |   |
   +------+-+--+---+    +------+-+--+---+    |      |    |   |
   |      |    |   |    |      |    |   |    |      |    |   |
   4      7    |   |    5      8    |   |    3      6    |   |
   |      |    |   |    |      |    |   |    |      |    |   |
 +-+-+  +-+-+  |   |  +-+-+  +-+-+  |   |  +-+-+  +-+-+  |   |
 |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
13  22 16  25 10  19 14  23 17  26 11  20 12  21 15  24  9  18

As before, time flows downward (assuming a multicast operation) and
tasks are expected to communicate with their children in order.  The
following are some CONCEPTUAL K-nomial tree expressions and their
evaluations, assuming `num_tasks' is `27':

   * `KNOMIAL_PARENT(0, 3)' => `-1'

   * `KNOMIAL_PARENT(2, 3)' => `0'

   * `KNOMIAL_CHILDREN(2, 3)' => `4'

   * `KNOMIAL_CHILD(2, 0, 3)' => `5'

   * `KNOMIAL_CHILD(2, 1, 3)' => `8'

   * `KNOMIAL_CHILD(2, 2, 3)' => `11'

   * `KNOMIAL_CHILD(2, 3, 3)' => `20'

   * `KNOMIAL_CHILD(2, 4, 3)' => `-1'

   * `KNOMIAL_CHILDREN(8, 3)' => `2'

   * `KNOMIAL_CHILDREN(8, 3, 26)' => `1'

   * `KNOMIAL_CHILDREN(8, 3, 10)' => `0'


File: conceptual.info,  Node: Mesh functions,  Next: Torus functions,  Prev: k-nomial tree functions,  Up: Built-in functions

Mesh functions
..............

CONCEPTUAL provides two functions, `MESH_NEIGHBOR' and `MESH_COORDINATE',
that help treat (linear) task IDs as positions on a multidimensional
mesh.  Each of these functions takes a variable number of arguments,
determined by the dimensionality of the mesh (1-D, 2-D, or 3-D).

 -- Function: MESH_NEIGHBOR (TASK_ID, WIDTH, X_OFFSET [, HEIGHT,
          Y_OFFSET [, DEPTH, Z_OFFSET]])
     `MESH_NEIGHBOR' returns a task's neighbor on a 1-D, 2-D, or 3-D
     mesh.  It always takes a task number, the mesh's width, and the
     desired x offset from the given task.  For a 2-D or 3-D mesh, the
     next two arguments are the mesh's height and the desired y offset
     from the given task.  For a 3-D mesh only, the next two arguments
     are the mesh's depth and the desired z offset from the given task.
     Offsets that move off the mesh cause `MESH_NEIGHBOR' to return the
     value `-1'.

 -- Function: MESH_COORDINATE (TASK_ID, COORDINATE, WIDTH [, HEIGHT
          [, DEPTH]])
     `MESH_COORDINATE' returns a task's X, Y, or Z coordinate on a 1-D,
     2-D, or 3-D mesh.  The first argument to `MESH_COORDINATE' is a
     task number.  The second argument should be `0' to calculate an X
     coordinate, `1' to calculate a Y coordinate, or `2' to calculate a
     Z coordinate.  The remaining arguments are the mesh's width,
     height, and depth, respectively.  The height can be omitted for a
     1-D mesh and the depth can be omitted for a 2-D mesh.  (Both
     default to `1'.)

   `MESH_NEIGHBOR' and `MESH_COORDINATE' number tasks following the
right-hand rule: left-to-right, then top-to-bottom, and finally
back-to-front, as shown in the following illustrations of a 4-element
(1-D) mesh, a 4x3 (2-D) mesh, and a 4x3x2 (3-D) mesh.  Examples of `MESH_NEIGHBOR'
and `MESH_COORDINATE' for 1-D, 2-D, and 3-D meshes follow the
corresponding illustration.



                           0 -- 1 -- 2 -- 3

   * `MESH_NEIGHBOR(0, 4, -1)' => `-1'

   * `MESH_NEIGHBOR(0, 4, +1)' => `1'

   * `MESH_NEIGHBOR(1, 4, -1)' => `0'

   * `MESH_NEIGHBOR(1, 4, +1)' => `2'

   * `MESH_NEIGHBOR(2, 4, -1)' => `1'

   * `MESH_NEIGHBOR(2, 4, +1)' => `3'

   * `MESH_NEIGHBOR(3, 4, -1)' => `2'

   * `MESH_NEIGHBOR(3, 4, +1)' => `-1'


   * `MESH_COORDINATE(-1, 0, 4)' => `-1'

   * `MESH_COORDINATE( 0, 0, 4)' => `0'

   * `MESH_COORDINATE( 1, 0, 4)' => `1'

   * `MESH_COORDINATE( 2, 0, 4)' => `2'

   * `MESH_COORDINATE( 3, 0, 4)' => `3'

   * `MESH_COORDINATE( 4, 0, 4)' => `-1'

   * `MESH_COORDINATE( 2, 1, 4)' => `0'

   * `MESH_COORDINATE( 2, 2, 4)' => `0'


                           0 -- 1 -- 2 -- 3
                           |    |    |    |
                           |    |    |    |
                           |    |    |    |
                           4 -- 5 -- 6 -- 7
                           |    |    |    |
                           |    |    |    |
                           |    |    |    |
                           8 -- 9 - 10 - 11

   * `MESH_NEIGHBOR(5, 4, -1, 3, -1)' => `0'

   * `MESH_NEIGHBOR(5, 4,  0, 3, -1)' => `1'

   * `MESH_NEIGHBOR(5, 4, +1, 3, -1)' => `2'

   * `MESH_NEIGHBOR(5, 4, -1, 3,  0)' => `4'

   * `MESH_NEIGHBOR(5, 4,  0, 3,  0)' => `5'

   * `MESH_NEIGHBOR(5, 4, +1, 3,  0)' => `6'

   * `MESH_NEIGHBOR(5, 4, -1, 3, +1)' => `8'

   * `MESH_NEIGHBOR(5, 4,  0, 3, +1)' => `9'

   * `MESH_NEIGHBOR(5, 4, +1, 3, +1)' => `10'


   * `MESH_COORDINATE( 1, 0, 4, 3)' => `1'

   * `MESH_COORDINATE( 6, 0, 4, 3)' => `2'

   * `MESH_COORDINATE( 6, 1, 4, 3)' => `1'

   * `MESH_COORDINATE( 6, 2, 4, 3)' => `0'

   * `MESH_COORDINATE( 8, 0, 4, 3)' => `0'

   * `MESH_COORDINATE( 8, 1, 4, 3)' => `2'

   * `MESH_COORDINATE(12, 0, 4, 3)' => `-1'


			   0 -- 1 -- 2 -- 3
			   |\   |\   |\   |\
			   | 12 + 13 + 14 + 15
			   |  | |  | |  | |  |
			   4 -+ 5 -+ 6 -+ 7  |
			   |\ | |\ | |\ | |\ |
			   | 16 + 17 + 18 + 19
			   |  | |  | |  | |  |
			   8 -+ 9 -+10 -+11  |
			    \ |  \ |  \ |  \ |
			     20 - 21 - 22 - 23

   * `MESH_NEIGHBOR(0, 4,  0, 3,  0, 2, +1)' => `12'

   * `MESH_NEIGHBOR(0, 4,  0, 3, +1, 2,  0)' => `4'

   * `MESH_NEIGHBOR(0, 4, +1, 3,  0, 2,  0)' => `1'

   * `MESH_NEIGHBOR(0, 4, +1, 3, +1, 2, +1)' => `17'

   * `MESH_NEIGHBOR(17, 4, +2, 3, -1, 2, -1)' => `3'

   * `MESH_NEIGHBOR(23, 4, +1, 3, +1, 2, +1)' => `-1'


   * `MESH_COORDINATE(-5, 0, 4, 3, 2)' => `-1'

   * `MESH_COORDINATE( 1, 0, 4, 3, 2)' => `1'

   * `MESH_COORDINATE( 6, 0, 4, 3, 2)' => `2'

   * `MESH_COORDINATE( 6, 1, 4, 3, 2)' => `1'

   * `MESH_COORDINATE( 6, 2, 4, 3, 2)' => `0'

   * `MESH_COORDINATE(18, 0, 4, 3, 2)' => `2'

   * `MESH_COORDINATE(18, 1, 4, 3, 2)' => `1'

   * `MESH_COORDINATE(18, 2, 4, 3, 2)' => `1'

   * `MESH_COORDINATE(18, 3, 4, 3, 2)' => error--> Invalid coordinate


File: conceptual.info,  Node: Torus functions,  Next: Random-number functions,  Prev: Mesh functions,  Up: Built-in functions

Torus functions
...............

A torus is a mesh with wraparound edges.  CONCEPTUAL provides analogues
to `MESH_NEIGHBOR' and `MESH_COORDINATE' called `TORUS_NEIGHBOR' and `TORUS_COORDINATE'
which enable (linear) task IDs to be treated as positions on a 1-D,
2-D, or 3-D torus.

 -- Function: TORUS_NEIGHBOR (TASK_ID, WIDTH, X_OFFSET [, HEIGHT,
          Y_OFFSET [, DEPTH, Z_OFFSET]])
     `TORUS_NEIGHBOR' takes the same arguments as `MESH_NEIGHBOR' but
     calculates neighbors on a torus instead of a mesh.  *Note Mesh
     functions::, for explanations of the arguments.  While task offsets
     that go out-of-bounds on a mesh return `-1', such offsets merely
     wrap around a torus.

 -- Function: TORUS_COORDINATE (TASK_ID, COORDINATE, WIDTH [, HEIGHT
          [, DEPTH]])
     `TORUS_COORDINATE' returns a task's X, Y, or Z coordinate on a
     1-D, 2-D, or 3-D torus.  It performs exactly the same function as `MESH_COORDINATE'
     and is aliased in the language merely for symmetry.  *Note Mesh
     functions::, for details.

   * `TORUS_NEIGHBOR(0, 4, -1)' => `3'

   * `TORUS_NEIGHBOR(0, 4, +1)' => `1'

   * `TORUS_NEIGHBOR(1, 4, -1)' => `0'

   * `TORUS_NEIGHBOR(1, 4, +1)' => `2'

   * `TORUS_NEIGHBOR(2, 4, -1)' => `1'

   * `TORUS_NEIGHBOR(2, 4, +1)' => `3'

   * `TORUS_NEIGHBOR(3, 4, -1)' => `2'

   * `TORUS_NEIGHBOR(3, 4, +1)' => `0'


   * `TORUS_COORDINATE(-1, 0, 4)' => `-1'

   * `TORUS_COORDINATE( 0, 0, 4)' => `0'

   * `TORUS_COORDINATE( 1, 0, 4)' => `1'

   * `TORUS_COORDINATE( 2, 0, 4)' => `2'

   * `TORUS_COORDINATE( 3, 0, 4)' => `3'

   * `TORUS_COORDINATE( 4, 0, 4)' => `-1'

   * `TORUS_COORDINATE( 2, 1, 4)' => `0'

   * `TORUS_COORDINATE( 2, 2, 4)' => `0'


   * `TORUS_NEIGHBOR(0, 4, -1, 3, -1)' => `11'

   * `TORUS_NEIGHBOR(0, 4,  0, 3, -1)' => `8'

   * `TORUS_NEIGHBOR(0, 4, +1, 3, -1)' => `9'

   * `TORUS_NEIGHBOR(0, 4, -1, 3,  0)' => `3'

   * `TORUS_NEIGHBOR(0, 4,  0, 3,  0)' => `0'

   * `TORUS_NEIGHBOR(0, 4, +1, 3,  0)' => `1'

   * `TORUS_NEIGHBOR(0, 4, -1, 3, +1)' => `7'

   * `TORUS_NEIGHBOR(0, 4,  0, 3, +1)' => `4'

   * `TORUS_NEIGHBOR(0, 4, +1, 3, +1)' => `5'


   * `TORUS_NEIGHBOR(23, 4, +1, 3, +1, 2, +1)' => `0'

   * `TORUS_NEIGHBOR(23, 4, +2, 3, +2, 2, +2)' => `17'

   * `TORUS_NEIGHBOR(23, 4, +3, 3, +3, 2, +3)' => `10'


   * `TORUS_COORDINATE( 1, 0, 4, 3)' => `1'

   * `TORUS_COORDINATE( 6, 0, 4, 3)' => `2'

   * `TORUS_COORDINATE( 6, 1, 4, 3)' => `1'

   * `TORUS_COORDINATE( 6, 2, 4, 3)' => `0'

   * `TORUS_COORDINATE( 8, 0, 4, 3)' => `0'

   * `TORUS_COORDINATE( 8, 1, 4, 3)' => `2'

   * `TORUS_COORDINATE(12, 0, 4, 3)' => `-1'


   * `TORUS_COORDINATE(-5, 0, 4, 3, 2)' => `-1'

   * `TORUS_COORDINATE( 1, 0, 4, 3, 2)' => `1'

   * `TORUS_COORDINATE( 6, 0, 4, 3, 2)' => `2'

   * `TORUS_COORDINATE( 6, 1, 4, 3, 2)' => `1'

   * `TORUS_COORDINATE( 6, 2, 4, 3, 2)' => `0'

   * `TORUS_COORDINATE(18, 0, 4, 3, 2)' => `2'

   * `TORUS_COORDINATE(18, 1, 4, 3, 2)' => `1'

   * `TORUS_COORDINATE(18, 2, 4, 3, 2)' => `1'

   * `TORUS_COORDINATE(18, 3, 4, 3, 2)' => error--> Invalid coordinate


File: conceptual.info,  Node: Random-number functions,  Prev: Torus functions,  Up: Built-in functions

Random-number functions
.......................

CONCEPTUAL programs can utilize randomness in one of two ways.  The
functions described below are _unsynchronized_ across tasks.  That is,
they can--and usually do--return a different value to each task on each
invocation.  One consequence is that these functions are not permitted
within a task expression (*note Task descriptions::) because randomness
would cause the tasks to disagree about who the sources and targets of
an operation are.  In contrast, the `A RANDOM TASK' construct described
in *note Binding variables:: returns a value guaranteed to be
synchronized across tasks and thereby enables random-task selection.

 -- Function: RANDOM_UNIFORM (LOWER_BOUND, UPPER_BOUND)
     Return a number selected at random from a uniform distribution over
     the range [LOWER_BOUND, UPPER_BOUND).

 -- Function: RANDOM_GAUSSIAN (MEAN, STDDEV)
     Return a number selected at random from a Gaussian distribution
     with mean MEAN and standard deviation STDDEV.

 -- Function: RANDOM_POISSON (MEAN)
     Return an integer selected at random from a Poisson distribution
     with mean MEAN and standard deviation sqrt(MEAN).


File: conceptual.info,  Node: Aggregate expressions,  Next: Aggregate functions,  Prev: Built-in functions,  Up: Expressions

4.2.3 Aggregate expressions
---------------------------

Aggregate expressions (<AGGR_EXPR>s) are currently used exclusively by
the `LOGS' statement.  They represent an expression with a given
function applied to the aggregate of all (dynamic) instances of that
expression.  <AGGR_EXPR>s take one of four forms:

<AGGR_EXPR>   ::=   [`EACH'] <EXPR>
              |     `THE' <EXPR>
              |     `THE' <AGGR_FUNC> [`OF' [`THE']] <EXPR>
              |     `A HISTOGRAM OF' [`THE'] <EXPR>

(In the above, <EXPR> refers to an arithmetic expression defined in
*note Arithmetic expressions:: and <AGGR_FUNC> refers to one of the
functions defined in *note Aggregate functions::.)

   The first form does not summarize <EXPR>; every individual instance
of <EXPR> is utilized.  The second form asserts that <EXPR> is a
constant (i.e., all values are identical) and utilizes that
constant.(1)  The third form applies <AGGR_FUNC> to the set of all
values of <EXPR> and utilizes the result of that function.  The fourth
form produces a histogram of all values of <EXPR>, i.e., a list of
{UNIQUE VALUE, TALLY} pairs, sorted by UNIQUE VALUE.

   ---------- Footnotes ----------

   (1) The program aborts with a run-time error if <EXPR> is not a
constant.


File: conceptual.info,  Node: Aggregate functions,  Next: Relational expressions,  Prev: Aggregate expressions,  Up: Expressions

4.2.4 Aggregate functions
-------------------------

The following functions, referred to collectively as <AGGR_FUNC>s, may
be used in an aggregate expression (*note Aggregate expressions::):

<AGGR_FUNC>   ::=   [`ARITHMETIC'] `MEAN' | `HARMONIC MEAN' | `GEOMETRIC MEAN' |
                    `MEDIAN' | `STANDARD DEVIATION' | `VARIANCE' | `SUM' |      `MINIMUM' |
                    `MAXIMUM' | `FINAL'

   `MEAN' and `ARITHMETIC MEAN' are equivalent.  `MEDIAN' is the value
such that there are as many larger as smaller values.  If there are an
even number of values, `MEDIAN' is the arithmetic mean of the two
medians.  `FINAL' returns only the final value measured.  The
interpretation of the remaining functions should be unambiguous.


File: conceptual.info,  Node: Relational expressions,  Prev: Aggregate functions,  Up: Expressions

4.2.5 Relational expressions
----------------------------

Relational expressions (<REL_EXPR>s) compare two arithmetic expressions
(*note Arithmetic expressions::) or test an arithmetic expression for a
property.  A relational expression can be either TRUE or FALSE.

   CONCEPTUAL supports a variety of relational expressions.  The
following is the language's order of operations from highest to lowest
precedence:

unary/        `IS EVEN', `IS ODD'
binary/       `=', `<', `>', `<=', `>=', `<>', `DIVIDES', `IS IN', `IS NOT IN'
conjunctive   `/\'
disjunctive   `\/'

In addition, as in most programming languages, parentheses can be used
to group subexpressions.

   The unary relation `IS EVEN' is TRUE if a given arithmetic
expression represents an even number and the unary relation `IS ODD' is
TRUE if a given arithmetic expression represents an odd number.  For
example, `456 IS EVEN' is TRUE and `64 MOD 6 IS ODD' is FALSE.

   The CONCEPTUAL operators `=', `<', `>', `<=', `>=', and `<>'
represent, respectively, the mathematical relations =, <, >, <=, >=,
and <> (i.e., not equal).  These are all binary relations that operate
on arithmetic expressions (*note Arithmetic expressions::).  For
example, `2+2 = 4' is TRUE and `2**3 > 2**4' is FALSE. The `DIVIDES'
relation is TRUE if the first expression evenly divides the second,
i.e., that e2 = 0 (mod e1).  Hence, `2 DIVIDES 1234' (equivalent to
`1234 MOD 2 = 0') is TRUE while `2 DIVIDES 4321' (equivalent to
`4321 MOD 2 = 0') is FALSE.

   The binary relation `IS IN' has the form

        <EXPR> `IS IN' <RANGE> [`,' <RANGE>]*

in which a <RANGE> is a comma-separated list of <EXPR>s within curly
braces.  An ellipsis can be used to indicate that CONCEPTUAL should
fill in the missing numbers.  More formally,

<RANGE>   ::=   `{'
                <EXPR> [`,' <EXPR>]*
                [`, ... ,' <EXPR>]
                `}'

   Ranges are described in detail in *note Range loops::, in the
context of `FOR EACH' loops, but the same principles apply to `IS IN'
tests as well.  In short, a <RANGE> represents a finite set of <EXPR>s,
either listed explicitly or described in terms of an arithmetic or
geometric progression.  As an example, the relational expression `x `IS
IN' {1, ..., 5}' is TRUE if and only if x is one of 1, 2, 3, 4, or 5.
As a more complex example, `p*2 `IS IN'
{0}, {1, 2, 4, ..., num_tasks*2}' is TRUE if and only if twice p is
either zero or a power of two less than or equal to twice the number of
tasks being used.

   The complementary operation to `IS IN' is the binary relation `IS
NOT IN'.  Hence, `4 IS NOT IN {3, ..., 5}' is FALSE while  `6 IS NOT IN
{3, ..., 5}' is TRUE.

   Conjunction ("and") and disjunction ("or") combine multiple
relational expressions.  <REL_EXPR> `/\' <REL_EXPR> is TRUE if and only
if both <REL_EXPR>s are TRUE, and <REL_EXPR> `\/' <REL_EXPR> is TRUE if
and only if either <REL_EXPR> is TRUE.  For example, `456 IS EVEN \/
2**3 > 2**4' is TRUE and `456 IS EVEN /\ 2**3 > 2**4' is FALSE.
Conjunction and disjunction are both short-circuiting operations.
Evaluation proceeds left-to-right.  Expressions such as `x<>0 /\ 1/x=1'
will therefore not result in a divide-by-zero error.

   CONCEPTUAL does not currently have a logical negation operator.

* Menu:

* Formal grammar for relational expressions::  EBNF version of the preceding
                                               prose


File: conceptual.info,  Node: Formal grammar for relational expressions,  Prev: Relational expressions,  Up: Relational expressions

Formal grammar for relational expressions
.........................................

For completeness, the following productions formalize the process by
which CONCEPTUAL parses relational expressions:

<REL_EXPR>   ::=   <REL_DISJ_EXPR>

<REL_DISJ_EXPR>   ::=   [<REL_DISJ_EXPR> `\/'] <REL_CONJ_EXPR>

<REL_CONJ_EXPR>   ::=   [<REL_CONJ_EXPR> `/\'] <REL_PRIMARY_EXPR>

<REL_PRIMARY_EXPR>   ::=   <EQ_EXPR>
                     |     `(' <REL_EXPR> `)'

<EQ_EXPR>   ::=   <EXPR> `=' <EXPR>
            |     <EXPR> `<' <EXPR>
            |     <EXPR> `>' <EXPR>
            |     <EXPR> `<=' <EXPR>
            |     <EXPR> `>=' <EXPR>
            |     <EXPR> `<>' <EXPR>
            |     <EXPR> `DIVIDES' <EXPR>
            |     <EXPR> `IS EVEN'
            |     <EXPR> `IS ODD'
            |     <EXPR> `IS IN' <RANGE> [`,' <RANGE>]*
            |     <EXPR> `IS NOT IN' <RANGE> [`,' <RANGE>]*

<RANGE>   ::=   `{'
                <EXPR> [`,' <EXPR>]*
                [`, ... ,' <EXPR>]
                `}'


File: conceptual.info,  Node: Task descriptions,  Next: Communication statements,  Prev: Expressions,  Up: Grammar

4.3 Task descriptions
=====================

"Task descriptions" are a powerful way of tersely describing the
sources and targets of CONCEPTUAL operations.  Task IDs range from 0 to
`num_tasks-1' (*note Predeclared variables::).  Operations involving
out-of-bound task IDs are silently ignored.

   As a side effect, a task description can declare a variable that can
be used in subsequent expressions.  (*Note Expressions::.)  There are
two types of task descriptions: one for "source" tasks and one for
"target" tasks.  The two are syntactically similar but semantically
different.  Specifically, the scope of a variable declared in a <TARGET_TASKS>
specification is more limited than one declared in a <SOURCE_TASK>
specification.

   Before introducing <SOURCE_TASK> and <TARGET_TASKS> specifications
we first introduce the notion of a <RESTRICTED_IDENT>, which is a
variable declaration that can be used to define a set of tasks.  We
then present CONCEPTUAL's complete set of mechanisms for describing
sets of source and target tasks.

* Menu:

* Restricted identifiers::      Task specifications with constraints
* Source tasks::                Who performs an operation
* Target tasks::                Whom an operation is performed upon


File: conceptual.info,  Node: Restricted identifiers,  Next: Source tasks,  Prev: Task descriptions,  Up: Task descriptions

4.3.1 Restricted identifiers
----------------------------

A "restricted identifier" declares a variable, restricting it to the
set of tasks that satisfy a given relational expression (*note
Relational expressions::).  The syntax, shown below, represents the
mathematical notion of "for all <IDENT> such that <REL_EXPR> is TRUE
and <IDENT> is between zero and the number of tasks...".

<RESTRICTED_IDENT>   ::=   <IDENT> `SUCH THAT' <REL_EXPR>

   As an example, `evno SUCH THAT evno IS EVEN' describes all
even-numbered tasks.  On each such task, the variable `evno' takes on
that task's ID.  Similarly, `thr SUCH THAT 3 DIVIDES thr-1' describes
tasks 1, 4, 7, 10, 13....  On each of those tasks, `thr' will be bound
to the task ID.  On all other tasks, `thr' will be undefined.  When
order matters (as in the cases described in *note Sending:: and *note
Reordering task IDs::), <IDENT> takes on task IDs in increasing order.


File: conceptual.info,  Node: Source tasks,  Next: Target tasks,  Prev: Restricted identifiers,  Up: Task descriptions

4.3.2 Source tasks
------------------

A <SOURCE_TASK> specification takes one of four forms:

<SOURCE_TASK>   ::=   `ALL TASKS'
                |     `ALL TASKS' <IDENT>
                |     `TASK' <EXPR>
                |     `TASKS' <RESTRICTED_IDENT>

   `ALL TASKS' specifies that each task will perform a given operation.
If followed by a variable name (<IDENT>), each task will individually
bind <IDENT> to its task ID--a number from zero to one less than the
total number of tasks.  That is, `ALL TASKS me' will bind `me' to `0'
on task 0, `1' on task 1, and so forth.

   `TASK <EXPR>' specifies that only the task described by arithmetic
expression <EXPR> will perform the given operation.  For example, `TASK
2*3+1' says that only task 7 will act; the other tasks will do nothing.

   `TASKS <RESTRICTED_IDENT>' describes a set of tasks that will
perform a given operation.  For instance, `TASKS x SUCH THAT x>0 /\
x<num_tasks-1'--read as "tasks x such that x is greater than zero and x
is less than num_tasks minus one"--expresses that a given operation
should be performed on all tasks except the first and last in the
computation.  On each task that satisfies the relational expression,
`x' will be bound to the task ID as in `ALL TASKS' above.  Hence, `x'
will be undefined on task 0, `1' on task 1, `2' on task 2, and so forth
up to task `num_tasks-1', on which `x' will again be undefined.

   As per the definitions in *note Primitives:: and *note Restricted
identifiers::, respectively, <IDENT>s and <RESTRICTED_IDENT>s do not
accept parentheses.  Hence, `TASKS (bad SUCH THAT bad IS EVEN)' and
`ALL TASKS (no_good)' result in parse errors while `TASKS fine SUCH THAT
fine IS EVEN' and `ALL TASKS dandy' are acceptable constructs.  As an
analogy, `x = 3' is valid in many general-purpose programming languages
while `(x) = 3' is not.

   Variables declared in a `source_task' specification are limited in
scope to the surrounding statement.


File: conceptual.info,  Node: Target tasks,  Prev: Source tasks,  Up: Task descriptions

4.3.3 Target tasks
------------------

A <TARGET_TASKS> specification takes one of three forms:

<TARGET_TASKS>   ::=   `ALL OTHER TASKS'
                 |     `TASK' <EXPR>
                 |     `TASKS' <RESTRICTED_IDENT>

   `ALL OTHER TASKS' is just like `ALL TASKS' in a <SOURCE_TASK>
specification (*note Source tasks::) but applies to all tasks _except_
the source task.  Also, unlike `ALL TASKS', `ALL OTHER TASKS' does not
accept an <IDENT> term.

   `TASK <EXPR>' specifies that only the task described by arithmetic
expression <EXPR> is the target of the given operation.  <EXPR> can use
a variable declared as a <SOURCE_TASK>.  For example, if <SOURCE_TASK>
is `ALL TASKS x', then a <TARGET_TASKS> of `TASK (x+1) MOD num_tasks'
refers to each task's right neighbor (with wraparound from `num_tasks'
to `0').

   `TASKS <RESTRICTED_IDENT>' describes a set of tasks that will
perform a given operation.  As with its `source_task' counterpart, a <RESTRICTED_IDENT>
declares a variable.  However, in a <TARGET_TASKS> specification the
variable's scope is limited to the relational expression within the <RESTRICTED_IDENT>.
As an example, `TASKS dst SUCH THAT dst>src' refers to all tasks `dst'
with a greater ID than a (previously declared) task `src'.


File: conceptual.info,  Node: Communication statements,  Next: I/O statements,  Prev: Task descriptions,  Up: Grammar

4.4 Communication statements
============================

Communication statements are the core of any CONCEPTUAL program.  The
CONCEPTUAL language makes it easy to express a variety of communication
features:

   * synchronous or asynchronous communication

   * unaligned, aligned (to arbitrary byte boundaries), or misaligned
     (from a page boundary) message buffers

   * ignored, touched, or verified message contents

   * unique or recycled message buffers

   * point-to-point or collective operations

   Communication statements are performed by an arbitrary <SOURCE_TASK>
(*note Source tasks::) and may involve arbitrary <TARGET_TASKS> (*note
Target tasks::).  After explaining how to describe a message to
CONCEPTUAL (*note Message specifications::) this section presents each
communication statement in turn and explains its purpose, syntax, and
semantics.

* Menu:

* Message specifications::      Describing message parameters
* Sending::                     Sending and implicitly receiving messages
* Receiving::                   Explicitly receiving messages
* Awaiting completion::         Completing asynchronous sends/receives
* Multicasting::                One-to-many communication
* Reducing::                    Many-to-many communication
* Synchronizing::               Barrier synchronization


File: conceptual.info,  Node: Message specifications,  Next: Sending,  Prev: Communication statements,  Up: Communication statements

4.4.1 Message specifications
----------------------------

A "message specification" describes a set of messages.  The following
is a formal definition:

<MESSAGE_SPEC>        ::=   <ITEM_COUNT>
                            [`NONUNIQUE' | `UNIQUE']
                            <ITEM_SIZE>
                            [`UNALIGNED' |
                            <MESSAGE_ALIGNMENT> `ALIGNED' |
                            <MESSAGE_ALIGNMENT> `MISALIGNED']
                            `MESSAGES'
                            [`WITH VERIFICATION' | `WITH DATA TOUCHING' |
                            `WITHOUT VERIFICATION' | `WITHOUT DATA TOUCHING']
                            [`FROM' `BUFFER' <EXPR> |
                            `FROM' `THE DEFAULT BUFFER']

   Within a `RECEIVE' statement (*note Receiving::), a <MESSAGE_SPEC>'s `FROM'
keyword must be replaced with `INTO'.

   A `SEND' statement's `WHO RECEIVES IT' clause (*note Sending::)
utilizes a slightly different message specification, which is referred
to here as a <RECV_MESSAGE_SPEC>:

<RECV_MESSAGE_SPEC>   ::=   [`SYNCHRONOUSLY' | `ASYNCHRONOUSLY']
                            [`AS' [`A'|`AN']
                            [`NONUNIQUE' | `UNIQUE']
                            [`UNALIGNED' |
                            <MESSAGE_ALIGNMENT> `ALIGNED' |
                            <MESSAGE_ALIGNMENT> `MISALIGNED']
                            `MESSAGES']
                            [`WITH VERIFICATION' | `WITH DATA TOUCHING' |
                            `WITHOUT VERIFICATION' | `WITHOUT DATA TOUCHING']
                            [`FROM' `BUFFER' <EXPR> |
                            `FROM' `THE DEFAULT BUFFER']

   Although not indicated by the preceding grammatical rule, a <RECV_MESSAGE_SPEC>
is not allowed to be empty.  That is, at least one of the optional
clauses must be specified.

   A `REDUCE' statement (*note Reducing::) utilizes the following
variations of <MESSAGE_SPEC> and <RECV_MESSAGE_SPEC>, respectively:

<REDUCE_MESSAGE_SPEC>   ::=   <ITEM_COUNT>
                              [`NONUNIQUE' | `UNIQUE']
                              [`UNALIGNED' |
                              <MESSAGE_ALIGNMENT> `ALIGNED' |
                              <MESSAGE_ALIGNMENT> `MISALIGNED']
                              `INTEGERS' | `DOUBLEWORDS'
                              [`WITH DATA TOUCHING' | `WITHOUT DATA TOUCHING']
                              [`FROM' `BUFFER' <EXPR> |
                              `FROM' `THE DEFAULT BUFFER']

<REDUCE_TARGET_MESSAGE_SPEC>   ::=   [`AS' <ITEM_COUNT>
                                     [`NONUNIQUE' | `UNIQUE']
                                     [<MESSAGE_ALIGNMENT> `ALIGNED' |
                                     <MESSAGE_ALIGNMENT> `MISALIGNED']
                                     `INTEGERS' | `DOUBLEWORDS']
                                     [`WITH DATA TOUCHING' | `WITHOUT DATA TOUCHING']
                                     [`INTO' `BUFFER' <EXPR> |
                                     `INTO' `THE DEFAULT BUFFER']


   We now describe in turn each component of a <MESSAGE_SPEC>, <RECV_MESSAGE_SPEC>, <REDUCE_MESSAGE_SPEC>,
and <REDUCE_TARGET_MESSAGE_SPEC>.

* Menu:

* Item count::                  How many messages should be sent?
* Unique messages::             Should messages recycle memory or not?
* Item size::                   How big is each message?
* Message alignment::           How should messages be aligned in memory?
* Data touching::               Should message contents be accessed explcitly?
* Buffer control::              What buffer should be used for each message?
* Blocking semantics::          Should the sender/receiver wait before proceeding?


File: conceptual.info,  Node: Item count,  Next: Unique messages,  Prev: Message specifications,  Up: Message specifications

Item count
..........

The <ITEM_COUNT> says how many messages the <MESSAGE_SPEC> represents:

<ITEM_COUNT>   ::=   `A' | `AN' | <EXPR>

`A' and `AN' are synonyms for the value `1'.


File: conceptual.info,  Node: Unique messages,  Next: Item size,  Prev: Item count,  Up: Message specifications

Unique messages
...............

Normally, the CONCEPTUAL backends recycle message memory to reduce the
program's memory requirements and improve performance.  By adding the
keyword `UNIQUE', every message buffer will reside in a unique memory
region.  `NONUNIQUE' explicitly specifies the default, buffer-recycling
behavior.


File: conceptual.info,  Node: Item size,  Next: Message alignment,  Prev: Unique messages,  Up: Message specifications

Item size
.........

The message size is represented by the <ITEM_SIZE> nonterminal.  It can
be empty or expressed in one of two other ways:

<ITEM_SIZE>   ::=   <empty>
              |     <EXPR> <DATA_MULTIPLIER>
              |     <DATA_TYPE> `SIZED'

   A <DATA_MULTIPLIER> is a scaling factor that converts a unitless
number into a number of bytes.  The following are the valid
possibilities for <DATA_MULTIPLIER> and the number of bytes which which
they multiply <EXPR>:

<DATA_MULTIPLIER>   ::=   `BIT' | `BYTE' | `HALFWORD' | `WORD' |      `INTEGER' |
                          `DOUBLEWORD' | `QUADWORD' | `PAGE' |      `KILOBYTE' | `MEGABYTE'
                          | `GIGABYTE'

`BIT'
     1/8 bytes, rounded up to the nearest integral number of bytes

`BYTE'
     1 byte

`HALFWORD'
     2 bytes

`WORD'
     4 bytes

`INTEGER'
     the number of bytes in the backend's fundamental integer type

`DOUBLEWORD'
     8 bytes

`QUADWORD'
     16 bytes

`PAGE'
     the number of bytes in an operating-system page

`KILOBYTE'
     1,024 bytes

`MEGABYTE'
     1,048,576 bytes

`GIGABYTE'
     1,073,741,824 bytes


   A <DATA_TYPE> is an "atomic" unit of data.  It can be any of the
following:

<DATA_TYPE>   ::=   `BYTE' | `HALFWORD' | `WORD' |      `INTEGER' |
                    `DOUBLEWORD' | `QUADWORD' | `PAGE'

`BYTE'
     1 byte

`HALFWORD'
     2 bytes

`WORD'
     4 bytes

`INTEGER'
     the number of bytes in the backend's fundamental integer type

`DOUBLEWORD'
     8 bytes

`QUADWORD'
     16 bytes

`PAGE'
     the number of bytes in an operating-system page

   Hence, valid <ITEM_SIZE>s include, for example, `16 MEGABYTE' or
`PAGE SIZED'.   Empty  <ITEM_SIZE>s are equivalent to `0 BYTE'.  Note
that `INTEGER' varies in size based on the backend, backend compiler,
and CPU architecture but is commonly either 4 or 8 bytes; `PAGE' varies
in size from operating system to operating system; each of the other <DATA_TYPE>s
has a fixed size, as indicated above.


File: conceptual.info,  Node: Message alignment,  Next: Data touching,  Prev: Item size,  Up: Message specifications

Message alignment
.................

Messages are normally allocated with arbitrary alignment in memory.
However, CONCEPTUAL can force a specific alignment relative to the
operating-system page size (commonly 4KB or 8KB, but significantly
larger sizes are gaining popularity).  A <MESSAGE_ALIGNMENT> is
represented as follows:

<MESSAGE_ALIGNMENT>   ::=   <DATA_TYPE>
                      |     <EXPR> <DATA_MULTIPLIER>

   `64 BYTE', `3 MEGABYTE', and `QUADWORD' are therefore all valid
examples of <MESSAGE_ALIGNMENT>s.  Bit counts are rounded up to the
nearest byte count, so `27 BITS' is in fact equivalent to `4 BYTES'.

   The `ALIGNED' keyword forces CONCEPTUAL to align messages on
_exactly_ the specified alignment.  Hence, a `HALFWORD ALIGNED' message
can begin at memory locations 0, 2, 4, 6, 8, ..., 2k (where k is a
positive integer).  In contrast, the `MISALIGNED' keyword forces
CONCEPTUAL to align messages the given number of bytes (positive or
negative) past a page boundary.  For example, if pages are 8192 bytes
in size then a message described as `HALFWORD MISALIGNED' can begin at
memory locations 2, 8194, 16386, 24578, ..., 8192k+2 (where k is a
positive integer).  Unlike `ALIGNED', `MISALIGNED' supports negative
alignments.  If the page size is 4096 bytes, then `-10 BYTE MISALIGNED'
enables a message to begin at memory locations 4086, 8182, 12278, etc.
The `MISALIGNED' alignment is taken modulo the page size.  Therefore,
with a 4096-byte page size, `10000 BYTE MISALIGNED' is the same as
`1808 BYTE MISALIGNED'.

   The `UNALIGNED' keyword explicitly specifies the default behavior,
with messages aligned on arbitrary boundaries.


File: conceptual.info,  Node: Data touching,  Next: Buffer control,  Prev: Message alignment,  Up: Message specifications

Data touching
.............

A <MESSAGE_SPEC> described as being `WITH DATA TOUCHING' will force
every word in a message to be both read and written ("touched").  When <MESSAGE_SPEC>
describes an outgoing message, the data will be touched before
transmission.  When <MESSAGE_SPEC> describes an incoming message, the
data will be touched after reception.  In a sense, `WITH DATA TOUCHING'
presents a more realistic assessment of network performance, as real
applications almost always access the data they send or receive.  It
also distinguishes between messaging layers that implicitly touch data
and those that can transmit data without having to touch it.  One would
expect the latter to perform better when the data is not touched, as
the former may be paying a penalty for touching the data.  However,
either could perform better when messages are sent `WITH DATA
TOUCHING', because the latter now has to pay the penalty that the
former has already paid.

   Another form of data-touching supported by CONCEPTUAL is `WITH
VERIFICATION'.  This causes the source task to write known, but
randomly generated, data into the message before transmission and the
target task to verify that every bit was correctly received.  When a
message is received `WITH VERIFICATION', the `bit_errors' variable
(*note Predeclared variables::) is updated appropriately.

   `WITHOUT DATA TOUCHING' and `WITHOUT VERIFICATION' are synonymous.
Both explicitly specify the default behavior of neither touching nor
verifying message contents.


File: conceptual.info,  Node: Buffer control,  Next: Blocking semantics,  Prev: Data touching,  Up: Message specifications

Buffer control
..............

The CONCEPTUAL run-time library allocates a unique message buffer for
each message sent/received with the `UNIQUE' keyword (*note Unique
messages::).  The message buffers for `NONUNIQUE' messages are recycled
subject to the constraint that no two concurrent transmissions will
reference the same buffer.  For example, if a task performs a
synchronous send followed by a synchronous receive, those operations
must be executed serially and will therefore share a message buffer.
If, instead, a task performs an asynchronous send followed by an
asynchronous receive, those operations may overlap, so CONCEPTUAL will
use different message buffers for the two operations.

   Message specifications enable the programmer to override the default
buffer-allocation behavior.  If a message is sent `FROM BUFFER' <EXPR>
or received `INTO BUFFER' <EXPR>, the message is guaranteed to be
sent/received using the specified buffer number.  For example, `FROM
BUFFER' and `INTO BUFFER' can be used to force a synchronous send and
synchronous receive to use different buffers or an asynchronous send
and asynchronous receive to use the same buffer.  If <EXPR> is
negative, the behavior is the same as if `FROM BUFFER'/`INTO BUFFER'
was not specified.  `FROM THE DEFAULT BUFFER' and `INTO THE DEFAULT
BUFFER' also explicitly specify the default buffer-allocation behavior.


File: conceptual.info,  Node: Blocking semantics,  Prev: Buffer control,  Up: Message specifications

Blocking semantics
..................

By default--or if `SYNCHRONOUSLY' is specified--messages are sent
synchronously.  That is, a sender blocks (i.e., waits) until the
message buffer is safe to reuse before it continues and a receiver
blocks until it actually receives the message.  The `ASYNCHRONOUSLY'
keyword specifies that messages should be sent and received
asynchronously.  That is, the program merely posts the message (i.e.,
declares that it should eventually be sent and/or received) and
immediately continues executing.  Asynchronous messages must be
"completed" as described in *note Awaiting completion::.


File: conceptual.info,  Node: Sending,  Next: Receiving,  Prev: Message specifications,  Up: Communication statements

4.4.2 Sending
-------------

The `SEND' statement is fundamental to CONCEPTUAL.  It is used to send
a multiple messages from multiple source tasks to multiple target
tasks.  The syntax is formally specified as follows:

<SEND_STMT>   ::=   <SOURCE_TASK>
                    [`ASYNCHRONOUSLY'] `SENDS'
                    <MESSAGE_SPEC>
                    `TO' [`UNSUSPECTING'] <TARGET_TASKS>
              |     <SOURCE_TASK>
                    [`ASYNCHRONOUSLY'] `SENDS'
                    <MESSAGE_SPEC>
                    `TO' <TARGET_TASKS>
                    `WHO RECEIVE IT'
                    <RECV_MESSAGE_SPEC>

<SOURCE_TASK> is described in *note Source tasks::; <MESSAGE_SPEC> and <RECV_MESSAGE_SPEC>
are described in *note Message specifications::; and, <TARGET_TASKS> is
described in *note Target tasks::.

   The `SEND' statement's simplest form, "<SOURCE_TASK> `SENDS' <MESSAGE_SPEC> `TO' <TARGET_TASKS>",
is fairly straightforward.  The following is a example:

     TASK 0 SENDS A 0 BYTE MESSAGE TO TASK 1

The only subtlety in the preceding statement is that it implicitly
causes task 1 to perform a corresponding receive.  This receive can be
suppressed by adding the keyword `UNSUSPECTING' before the <TARGET_TASKS>
description:

     TASK 0 SENDS A 0 BYTE MESSAGE TO UNSUSPECTING TASK 1

   Here are some further examples of valid <SEND_STMT>s:

   * `ALL TASKS SEND A 64 KILOBYTE MESSAGE TO TASK 0'

   * `TASK num_tasks-1 SENDS 5 53 BYTE PAGE ALIGNED MESSAGES TO ALL
     OTHER TASKS'

   * `TASKS upper SUCH THAT upper>=num_tasks/2 ASYNCHRONOUSLY SEND A 0
     BYTE MESSAGE TO TASK upper/2'

   * `TASKS nonzero SUCH THAT nonzero>0 SEND nonzero 1E3 BYTE MESSAGES
     TO UNSUSPECTING TASK 0'

   One subtlety of the `SEND' statement when used without `UNSUSPECTING'
involves the orderings of the sends and receives.  The rule is that
receives are posted before sends.  Furthermore, <RESTRICTED_IDENT>s
(*note Restricted identifiers::) are evaluated in order from 0
to NUM_TASKS-1.  The implication is that a statement such as `TASKS ev
SUCH THAT ev IS EVEN /\ ev<6 SEND A 4 WORD MESSAGE TO TASK ev+2' is
exactly equivalent to the following ordered sequence of statements
(assuming NUM_TASKS >= 5):

  1. `TASK 2 RECEIVES A 4 WORD MESSAGE FROM TASK 0'

  2. `TASK 4 RECEIVES A 4 WORD MESSAGE FROM TASK 2'

  3. `TASK 6 RECEIVES A 4 WORD MESSAGE FROM TASK 4'

  4. `TASK 0 SENDS A 4 WORD MESSAGE TO UNSUSPECTING TASK 2'

  5. `TASK 2 SENDS A 4 WORD MESSAGE TO UNSUSPECTING TASK 4'

  6. `TASK 4 SENDS A 4 WORD MESSAGE TO UNSUSPECTING TASK 6'

(The `RECEIVE' statement is described in *note Receiving::.)

   If the above sequence were executed, tasks 2, 4, and 6 would
immediately block on their receives (steps 1-3).  Task 0 would awaken
task 2 by sending it a message (step 4).  Then, task 2 would be able to
continue to step 5 at which point it would send a message to task 4.
Task 4 would then finally be able to send a message to task 6 (step 6).
Hence, even though the original CONCEPTUAL statement encapsulates
multiple communication operations, the component communications proceed
sequentially because of data dependences and because the operations are
blocking.

   As should now be apparent, there are a number of attributes
associated with every message transmission:

   * synchronous vs. asynchronous operation

   * unique vs. recycled message buffers

   * unaligned vs. aligned vs. misaligned message buffers

   * no data touching vs. data touching vs. data verification

   * implicit vs. explicit message-buffer selection

   When `UNSUSPECTING' is omitted, the implicit `RECEIVE' statement
normally inherits all of the attributes of the corresponding `SEND'.
However, the second form of a <SEND_STMT>, which contains a `WHO
RECEIVES IT' (or `WHO RECEIVES THEM') clause, enables the receiver's
attributes to be overridden on a per-attribute basis.  For instance,
consider the following `SEND' statement:

     TASK 0 SENDS A 1 MEGABYTE MESSAGE TO TASK 1 WHO RECEIVES IT
     ASYNCHRONOUSLY

The alternative sequence of statements that does not use `WHO RECEIVES
IT' is less straightforward:

  1. `TASK 1 ASYNCHRONOUSLY RECEIVES A 1 MEGABYTE MESSAGE FROM TASK 0'

  2. `TASK 0 SENDS A 1 MEGABYTE MESSAGE TO UNSUSPECTING TASK 1'

   Some further examples of `WHO RECEIVES IT' follow:

     TASKS left SUCH THAT left IS EVEN SEND 5 2 KILOBYTE 64 BYTE ALIGNED
     MESSAGES TO TASKS left+1 WHO RECEIVE THEM AS UNALIGNED MESSAGES WITH
     DATA TOUCHING

     TASK num_tasks-1 ASYNCHRONOUSLY SENDS A 1E5 BYTE MESSAGE WITH
     VERIFICATION TO TASK 0 WHO RECEIVES IT SYNCHRONOUSLY

     TASK leaf SUCH THAT KNOMIAL_CHILDREN(leaf,2)=0 SENDS A UNIQUE 1536
     BYTE MESSAGE WITH DATA TOUCHING TO TASK KNOMIAL_PARENT(leaf,2) WHO
     RECEIVES IT ASYNCHRONOUSLY AS A NONUNIQUE QUADWORD ALIGNED MESSAGE
     WITHOUT DATA TOUCHING INTO BUFFER KNOMIAL_PARENT(leaf,2)


File: conceptual.info,  Node: Receiving,  Next: Awaiting completion,  Prev: Sending,  Up: Communication statements

4.4.3 Receiving
---------------

*note Sending::, mentioned the <SEND_STMT>'s `UNSUSPECTING' keyword,
which specifies that the targets should not implicitly perform a
receive operation.  Because every send must have a matching receive,
CONCEPTUAL offers a `RECEIVE' statement which explicitly receives a set
of messages.  A <RECEIVE_STMT> is much like a <SEND_STMT> (*note
Sending::) with the <SOURCE_TASK> and <TARGET_TASKS> in the reverse
order:

<RECEIVE_STMT>   ::=   <TARGET_TASKS>
                       [`ASYNCHRONOUSLY'] `RECEIVE'
                       <MESSAGE_SPEC>
                       `FROM' <SOURCE_TASK>

<TARGET_TASKS> is described in *note Target tasks::; <MESSAGE_SPEC> is
described in *note Message specifications::; and, <SOURCE_TASK> is
described in *note Source tasks::.

   For each message sent via a `SEND'...`TO UNSUSPECTING' statement
there must be a `RECEIVE' statement that receives a message of the same
size.  The <TARGET_TASKS>'s <MESSAGE_SPEC> can, however, specify
different values for message uniqueness, message alignment, and data
touching.  In addition, the source and target do not need to agree on
the use of the `ASYNCHRONOUSLY' keyword.  The only restriction is that `WITH
VERIFICATION' will return spurious results if used by the target but
not by the source.  Hence, the following <SEND_STMT> and <RECEIVE_STMT>
correctly match each other:

     TASK 0 SENDS 3 4 KILOBYTE MESSAGES TO UNSUSPECTING TASK 1

     TASK 1 ASYNCHRONOUSLY RECEIVES 3 UNIQUE 4 KILOBYTE 48 BYTE ALIGNED
     MESSAGES WITH DATA TOUCHING FROM TASK 0.

   In general, it is better to use a single `SEND' statement with a `WHO
RECEIVES IT' clause (*note Sending::) than a `RECEIVE' plus a matching `SEND'...`TO
UNSUSPECTING'; the former is less error-prone than the latter.
However, the latter is useful for programs in which a set of receives
is posted, then the tasks perform various communication, computation,
and synchronization operations, and--towards the end of the
program--the matching sends are posted.  That sort of split-phase
structure requires separate `SEND' and `RECEIVE' statements.


File: conceptual.info,  Node: Awaiting completion,  Next: Multicasting,  Prev: Receiving,  Up: Communication statements

4.4.4 Awaiting completion
-------------------------

When a message is sent or received asynchronously it must eventually be
"completed".  In some messaging layers, asynchronous messages are not
even sent or received until completion time.  CONCEPTUAL provides the
following statement for completing messages that were send/received
asynchronously:

<WAIT_STMT>   ::=   <SOURCE_TASK>
                    `AWAITS COMPLETION'

That is, a <WAIT_STMT> simply specifies the set of tasks that should
block until all of their pending communications complete.  <SOURCE_TASK>
is as defined in *note Source tasks::.  Note that a <WAIT_STMT> blocks
until _all_ pending communications complete.  CONCEPTUAL does not
provide finer-grained control over completions.  It is safe, however,
for a task to `AWAIT COMPLETION' even if it has no asynchronous messages
pending.


File: conceptual.info,  Node: Multicasting,  Next: Reducing,  Prev: Awaiting completion,  Up: Communication statements

4.4.5 Multicasting
------------------

Although a single <SEND_STMT> (*note Sending::) can specify multiple
messages at once, these messages are sent one at a time.
"Multicasting" is a form of collective communication in which a set of
tasks collaborates to deliver a message from a source to multiple
targets.  With many messaging layers, multicasting a message to N tasks
is more efficient than sending a sequence of N individual messages.
CONCEPTUAL supports multicasting as follows:

<MCAST_STMT>    ::=   <SOURCE_TASK>
                      [`ASYNCHRONOUSLY'] `MULTICASTS'
                      <MESSAGE_SPEC>
                      `TO' <TARGET_TASKS>

   Unlike <SEND_STMT>s, <MCAST_STMT>s do not support the `UNSUSPECTING'
keyword.  This is because `MULTICASTS' is a collective operation: all
parties are active participants in delivering messages to the <TARGET_TASKS>.

   <SOURCE_TASK> (*note Source tasks::) and <TARGET_TASKS> (*note
Target tasks::) can be either disjoint or overlapping sets.  That is,
either of the following is legal:

     TASK 0 MULTICASTS A 16 BYTE MESSAGE TO TASKS recip SUCH THAT recip<4
     TASK 0 MULTICASTS A 16 BYTE MESSAGE TO TASKS recip SUCH THAT recip>=4

Note that in the first <MCAST_STMT>, task 0 both sends and receives a
message, while in the second <MCAST_STMT>, task 0 sends but does not
receive.


File: conceptual.info,  Node: Reducing,  Next: Synchronizing,  Prev: Multicasting,  Up: Communication statements

4.4.6 Reducing
--------------

A reduction operation is, in a sense, a complementary operation to a
multicast (*note Multicasting::).  While a multicast delivers a message
from one source to multiple targets, a reduction combines messages from
multiple sources (by applying a commutative/associative operator to
corresponding elements) to a single target.  Reduction is a collective
operation: all parties collaborate to calculate the reduced value(s).
As an example, if tasks 0, 1, and 2 collectively reduce the messages
{5, 1}, {2, 7}, and {3, 4} to task 2 using the "+" operator, then
task 2 will receive the message {10, 12}.  In fact, CONCEPTUAL's
implementation of reductions also supports reductions to multiple
targets with each target receiving a copy of the reduced value.

   The following grammatical rules define CONCEPTUAL's many-to-many and
many-to-one reduction facilities:

<REDUCE_STMT>   ::=   <SOURCE_TASK>
                      `REDUCES'
                      <REDUCE_MESSAGE_SPEC>
                      `TO' <SOURCE_TASK>
                      [`WHO RECEIVES THE RESULT' <REDUCE_TARGET_MESSAGE_SPEC>]
                |     <SOURCE_TASK>
                      `REDUCES'
                      <REDUCE_MESSAGE_SPEC>
                      [`TO' <REDUCE_MESSAGE_SPEC>]

   <REDUCE_MESSAGE_SPEC> is defined in *note Message specifications::.
Both the data providers and data receivers are specified as <SOURCE_TASK>
nonterminals (*note Source tasks::).  This design enables any set of
tasks to provide the data to reduce and any disjoint or overlapping set
of tasks to receive the reduced data.  As with all communication in
CONCEPTUAL, message contents are opaque.  Furthermore, the grammar does
not currently enable the programmer to specify the
commutative/associative operator to use.

   A simple many-to-one reduction can be expressed in CONCEPTUAL with
`ALL TASKS REDUCE 5 DOUBLEWORDS TO TASK 0'.  Note that the definition
of <REDUCE_MESSAGE_SPEC> and <REDUCE_TARGET_MESSAGE_SPEC> (*note
Message specifications::) supports reductions only of `INTEGER's and `DOUBLEWORD's,
not arbitrary <DATA_TYPE> values.  Omitting the optional ``TO' <REDUCE_MESSAGE_SPEC>',
as in `TASKS rt SUCH THAT 3 DIVIDES rt REDUCE AN INTEGER', specifies
that all tasks performing the reduction will receive a copy of the
reduced value.  The sources and targets can also be designated
explicitly as in `TASKS xyz SUCH THAT xyz<num_tasks/2 REDUCE 100
DOUBLEWORDS TO TASKS xyz+num_tasks/4'.  When that code is run with
8 tasks, tasks 0-3 reduce 100 doublewords (800 bytes) apiece and
tasks 2-5 each receive identical copies of the 100 doublewords of
reduced data.

   Message data used with `REDUCES' can be transfered DATA TOUCHING
`WITH DATA TOUCHING' (but not `WITH VERIFICATION'); data alignment can
be specified; and, messages buffers can be named explicitly.  The
following example represents fairly complex many-to-many usage of `REDUCES':

     TASKS rsrc SUCH THAT rsrc IS EVEN REDUCE 32 64-BYTE-ALIGNED INTEGERS
     WITH DATA TOUCHING FROM BUFFER 2 TO TASKS rtarg SUCH THAT
     rtarg<num_tasks/4 \/ rtarg>(3*num_tasks)/4 WHO RECEIVE THE RESULT AS
     32 UNIQUE PAGE-ALIGNED INTEGERS WITHOUT DATA TOUCHING.


File: conceptual.info,  Node: Synchronizing,  Prev: Reducing,  Up: Communication statements

4.4.7 Synchronizing
-------------------

CONCEPTUAL enables sets of tasks to perform "barrier synchronization".
The semantics are that no task can finish synchronizing until all tasks
have started synchronizing.  The syntax is as follows:

<SYNC_STMT>   ::=   <SOURCE_TASK>
                    `SYNCHRONIZES'

   A <SYNC_STMT> can be used to ensure that one set of statements has
completed before beginning another set.  For example, a CONCEPTUAL
program might have a set of tasks post a series of asynchronous
receives (*note Receiving::), then make `ALL TASKS SYNCHRONIZE' before
having another set of tasks perform the corresponding `UNSUSPECTING'
sends (*note Sending::).  This procedure ensures that all of the target
tasks are ready to receive before the source tasks start sending to
them.


File: conceptual.info,  Node: I/O statements,  Next: Counter and timer statements,  Prev: Communication statements,  Up: Grammar

4.5 I/O statements
==================

CONCEPTUAL provides two statements for presenting information.  One
statement writes simple messages to the standard output device and is
intended to be used for providing status information during the run of
a program.  The other statement provides a powerful mechanism for
storing performance and correctness data to a log file.

* Menu:

* Utilizing log-file comments::  Treating log-file comments as a database
* Writing to standard output::  Displaying status messages
* Writing to a log file::       Storing test results


File: conceptual.info,  Node: Utilizing log-file comments,  Next: Writing to standard output,  Prev: I/O statements,  Up: I/O statements

4.5.1 Utilizing log-file comments
---------------------------------

<OUTPUT_STMT>s and <LOG_STMT>s have limited access to the <KEY:VALUE>
pairs that are written as comments at the top of every log file as
shown in *note Log-file format::.  Given a key, KEY, the string
expression `THE VALUE OF KEY' represents the value associated with that
key or the empty string if KEY does not appear in the log-file comments:

<STRING_OR_LOG_COMMENT>   ::=   <STRING>
                          |     `THE VALUE OF' <STRING>

That is, `"CPU frequency"' means the literal string "CPU frequency"
while `THE VALUE OF "CPU frequency"' translates to a string like
"1300000000 Hz (1.3 GHz)".  Environment variables are also considered
keys and are therefore acceptable input to a VALUE OF `THE VALUE OF'
construct.


File: conceptual.info,  Node: Writing to standard output,  Next: Writing to a log file,  Prev: Utilizing log-file comments,  Up: I/O statements

4.5.2 Writing to standard output
--------------------------------

CONCEPTUAL's `OUTPUT' keyword is used to write a message from one or
more source tasks (*note Source tasks::) to the standard output device.
This is useful for providing progress reports during the execution of
long-running CONCEPTUAL programs.  An <OUTPUT_STMT> looks like this:

<OUTPUT_STMT>   ::=   <SOURCE_TASK>
                      `OUTPUTS'
                      <EXPR> | <STRING_OR_LOG_COMMENT>
                      [`AND'   <EXPR> | <STRING_OR_LOG_COMMENT>]*

   The following are some sample <OUTPUT_STMT>s:

     TASK 0 OUTPUTS "Hello, world!"

     TASKS nr SUCH THAT nr>0 OUTPUT nr AND "'s parent is " AND nr>>1 AND
     " and its children are " AND nr<<1 AND " and " AND nr<<1+1

     ALL TASKS me OUTPUT "Task " AND me AND " is running on host " AND THE
     VALUE OF "Host name" AND " and plans to send to task " AND (me+1) MOD
     num_tasks

   `OUTPUT' does not implicitly output spaces between terms.  Hence,
`OUTPUT "Yes" AND "No"' will output "YesNo", not "Yes No".  Although it
is unlikely that a program would ever need to output two arithmetic
expressions with no intervening text, an empty string can be used for
this purpose: `OUTPUT 6 AND "" AND 3'.

   An <OUTPUT_STMT> implicitly outputs a newline character at the end.
Additional newline characters can be output by embedding `\n' in a
string.  (*note Primitives::.)  CONCEPTUAL does not provide a means for
suppressing the newline, however.


File: conceptual.info,  Node: Writing to a log file,  Prev: Writing to standard output,  Up: I/O statements

4.5.3 Writing to a log file
---------------------------

After performing a network correctness or performance test it is almost
always desirable to store the results in a file.  CONCEPTUAL has
language support for writing tabular data to a log file.  The <LOG_STMT>
command does the bulk of the work:

<LOG_STMT>   ::=   <SOURCE_TASK>
                   `LOGS'
                   <AGGR_EXPR> `AS' <STRING_OR_LOG_COMMENT>
                   [`AND' <AGGR_EXPR> `AS' <STRING_OR_LOG_COMMENT>]*

   The idea behind a <LOG_STMT> is that a set of source tasks (*note
Source tasks::) log an aggregate expression (*note Aggregate
expressions::) to a log file under the column heading <STRING_OR_LOG_COMMENT>.
Each task individually maintains a separately named log file so there
is no ambiguity over which task wrote which entries.

   Each (static) `LOGS' statement in a CONCEPTUAL program specifies one
or more columns of the log file.  Every dynamic execution of a `LOGS'
statement writes a single row to the log file.  A single `LOGS'
statement should suffice for most CONCEPTUAL programs.

   The following are some examples of <LOG_STMT>s:

     ALL TASKS LOG bit_errors AS "Bit errors"

     TASK 0 LOGS THE msgsize AS "Bytes" AND
                 THE MEDIAN OF (1E6*bytes_sent)/(1M*elapsed_usecs) AS "MB/s"

The first example produces a log file like the following:

     "Bit errors"
     "(all data)"
     3

The second example produces a log file like this:

     "Bytes","MB/s"
     "(only value)","(median)"
     65536,179.9416266

Note that in each log file, the CONCEPTUAL run-time system writes two
rows of column headers for each column.  The first row contains <STRING_OR_LOG_COMMENT>
as is.  The second row describes the <AGGR_FUNC> (*note Aggregate
functions::) used to aggregate the data.  One or more rows of data
follow.

   Assume that the second <LOG_STMT> presented above appears within a
loop (*note Iterating::).  It is therefore important to include the `THE'
keyword before `msgsize' to assert that the expression `msgsize' is
constant across invocations of the <LOG_STMT> and that, consequently,
only a single row of data should be written to the log file.  Using
`msgsize' without the `THE' would produce a column of data with one row
per <LOG_STMT> invocation:

     "Bytes","MB/s"
     "(all data)","(median)"
     65536,179.9416266
     65536,
     65536,
     65536,
     65536,
          .
          .
          .

   The rules that determine how `LOGS' statements produce rows and
columns of a log file are presented below:

  1. Each _static_ `LOGS' statement (and `AND' clause within a `LOGS'
     statement) in a program produces a unique column.

  2. Each _dynamic_ execution of a `LOGS' statement appends a row to
     the column(s) it describes.

  3. Each top-level complex statement (*note Complete programs::)
     produces a new table in the log file.

   Note that the choice of column name is inconsequential for
determining what columns are written to the log file:

     TASK 0 LOGS 314/100 AS "Pi" AND 22/7 AS "Pi"

     "Pi","Pi"
     "(all data)","(all data)"
     3.14,3.142857143

* Menu:

* Computing aggregates::        Forcing aggregates to compute a result


File: conceptual.info,  Node: Computing aggregates,  Prev: Writing to a log file,  Up: Writing to a log file

Computing aggregates
....................

What if `msgsize' takes on a number of values throughout the execution
of the program and for each value a number of runs is performed?  How
would one log the median of each set of data?  Using `THE msgsize'
won't work because the message size is not constant.  Using `msgsize'
alone won't work either because CONCEPTUAL would then take the median
of the times gathered across _all_ message sizes, which is undesirable.
The solution is for the program to specify explicitly when aggregate
functions (`MEDIAN' and all of the other functions listed in *note
Aggregate functions::) compute a value:

<FLUSH_STMT>   ::=   <SOURCE_TASK>
                     `COMPUTES AGGREGATES'

   The intention is that an inner loop might `LOG' data after every
iteration and an outer loop would `COMPUTE AGGREGATES' after each
iteration.


File: conceptual.info,  Node: Counter and timer statements,  Next: Complex statements,  Prev: I/O statements,  Up: Grammar

4.6 Counter and timer statements
================================

Critical to any performance or correctness test is the ability to
specify which operations represent the test itself and should be
measured and which are setup or other uninteresting operations and
should not.  CONCEPTUAL automatically maintains a number of
"counters"--variables that represent message counts, byte counts,
bit-error counts, and elapsed time.  The complete list is presented in
*note Predeclared variables::.

   Normally, a CONCEPTUAL program performs some setup operations, `RESETS
ITS COUNTERS' to zero, executes a communication pattern, and logs some
function of the resulting changes in counter values (*note Writing to a
log file::).  If additional setup work needs to be performed during an
experiment, a program can COUNTERS `STORE ITS COUNTERS', perform any
arbitrarily costly operations, ITS COUNTERS `RESTORE ITS COUNTERS', and
continue the experiment as if those operations never happened.

   Some CONCEPTUAL statements implicitly store and restore counters.
For example, the `LOGS' statement *note Writing to a log file::) takes
up no time from the program's perspective, and counted loops (*note
Counted loops::) bracket any warmup repetitions and post-warmup
synchronizations between a counter store and restore so no delays, bit
errors, or messaging operations contribute to the totals measured by
the experiment.

* Menu:

* Resetting counters::          Clearing message, byte, and time tallies
* Storing counter values::      Storing message, byte, and time tallies
* Restoring counter values::    Restoring stored message, byte, and time tallies


File: conceptual.info,  Node: Resetting counters,  Next: Storing counter values,  Prev: Counter and timer statements,  Up: Counter and timer statements

4.6.1 Resetting counters
------------------------

At the start of an experiment, after all setup processing has
completed, all tasks that will eventually log measurement results
should zero out their counters:

<RESET_STMT>   ::=   <SOURCE_TASK>
                     `RESETS ITS COUNTERS'

   Hence, writing `ALL TASKS RESET THEIR COUNTERS' causes each task to
reset all of the variables listed in *note Predeclared
variables::--with the exception of `num_tasks'--to zero.  Note that
`ITS' and `THEIR', like `RESET' and `RESETS', are considered synonyms
(*note Primitives::).


File: conceptual.info,  Node: Storing counter values,  Next: Restoring counter values,  Prev: Resetting counters,  Up: Counter and timer statements

4.6.2 Storing counter values
----------------------------

A program can store the current values of all of the variables listed
in *note Predeclared variables:: as follows:

<STORE_STMT>   ::=   <SOURCE_TASK>
                     `STORES ITS COUNTERS'

   For example, writing `TASK 0 STORES ITS COUNTERS' causes task 0 to
store the current values of `elapsed_usecs', `total_msgs', `bit_errors',
etc.  The values are not modified.  Note that `ITS' and `THEIR', like
`STORE' and `STORES', are considered synonyms (*note Primitives::).


File: conceptual.info,  Node: Restoring counter values,  Prev: Storing counter values,  Up: Counter and timer statements

4.6.3 Restoring counter values
------------------------------

Counters can be restored to their most recently saved values with a <RESTORE_STMT>:

<RESTORE_STMT>   ::=   <SOURCE_TASK>
                       `RESTORES ITS COUNTERS'

   For example, writing `TASKS t SUCH THAT 3 DIVIDES t RESTORE THEIR
COUNTERS' causes every third task to replace `elapsed_usecs', `total_msgs', `bit_errors',
etc. with the values stored by a corresponding <STORE_STMT> (*note
Storing counter values::).  Note that `ITS' and `THEIR', like `RESTORE'
and `RESTORES', are considered synonyms (*note Primitives::).

   An important feature of `STORES' and `RESTORES' is that they can be
nested.  That is, each `STORE' pushes a set of counter values on a
stack, and each `RESTORE' pops a set of counter values from the stack.
Consequently, one can write code like the following:

     ALL TASKS RESET THEIR COUNTERS THEN                                 #  1
     ALL TASKS COMPUTE FOR 2 SECONDS THEN                                #  2
     ALL TASKS STORE THEIR COUNTERS THEN                                 #  3
       ALL TASKS COMPUTE FOR 5 SECONDS THEN                              #  4
       ALL TASKS STORE THEIR COUNTERS THEN                               #  5
         ALL TASKS COMPUTE FOR 9 SECONDS THEN                            #  6
         ALL TASKS LOG ROUND(elapsed_usecs/1E6) AS "Should be 16" THEN   #  7
       ALL TASKS RESTORE THEIR COUNTERS THEN                             #  8
       ALL TASKS LOG ROUND(elapsed_usecs/1E6) AS "Should be 7" THEN      #  9
       ALL TASKS STORE THEIR COUNTERS THEN                               # 10
         ALL TASKS COMPUTE FOR 1 SECOND THEN                             # 11
         ALL TASKS LOG ROUND(elapsed_usecs/1E6) AS "Should be 8" THEN    # 12
       ALL TASKS RESTORE THEIR COUNTERS THEN                             # 13
       ALL TASKS LOG ROUND(elapsed_usecs/1E6) AS "Should be 7" THEN      # 14
       ALL TASKS RESTORE THEIR COUNTERS THEN                             # 15
     ALL TASKS LOG ROUND(elapsed_usecs/1E6) AS "Should be 2".            # 16

   Indentation is used in the above to clarify which `RESTORE'
operations match which `STORE' operations.  The first `STORE' statement
(line 3) occurs after 2 seconds have elapsed.  The second `STORE'
statement (line 5) occurs after 2+5=7 seconds have elapsed.  When the `LOG'
statement in line 7 is executed, it reports that 2+5+9=16 seconds have
elapsed.  The `RESTORE' statement in line 8 then "winds back the clock"
to the previous `STORE' statement, the one in line 5.  The next `LOG'
statement (line 9) executes as if lines 5-8 never ran and therefore
reports that only 2+5=7 seconds have elapsed.  The `LOG' statement in
line 12 sees an additional second of elapsed time due to line 11's `COMPUTE'
statement, for a total of 2+5+1=8 seconds.  However, the `RESTORE' in
line 13 makes it as if that `COMPUTE' never happened.  Consequently,
the `LOG' statement in line 14 reports that only 2+5=7 seconds have
elapsed.  Finally, the `RESTORE' in line 15 sets the timer to the value
it had all the way back at line 3.  The final `LOG' statement (line 16)
therefore reports that only 2 seconds have elapsed because line 2
contains the only `COMPUTE' statement whose execution time has not been
discarded.

   Because the `interpret' backend and those derived from it use
logical time instead of physical time, the code listed above will
report all zeroes.  Replacing `ROUND(elapsed_usecs/1E6)' with just
`elapsed_usecs' will log the logical times {6,5,8,7,3}.  That is, the `LOG'
statement in line 7 sees six events after the initial `RESET';(1) the `LOG'
statement in line 9 sees only five events after the `RESET'
(corresponding to lines 2, 3, 4, 5, and 9); and so forth up to the final `LOG'
statement, which sees only three events: those produced by lines 2, 3,
and 16.

   A program that calls `RESTORE' more times than it calls `STORE' will
abort with a fatal run-time error.

   ---------- Footnotes ----------

   (1) Each statement in the example corresponds to a single event and
therefore counts as one unit of time.


File: conceptual.info,  Node: Complex statements,  Next: Other statements,  Prev: Counter and timer statements,  Up: Grammar

4.7 Complex statements
======================

The CONCEPTUAL statements presented in *note Communication statements::,
*note I/O statements::, and *note Other statements:: are all known as
"simple statements".  This section expands upon the statements already
introduced by presenting "complex statements".  In its most basic form,
a <COMPLEX_STMT> is just a <SIMPLE_STMT>.  However, the primary purpose
of a <COMPLEX_STMT> is to juxtapose simple statements and other complex
statements into more expressive forms.

   Complex statements take the following form:

<COMPLEX_STMT>   ::=   <SIMPLE_STMT> [`THEN' <COMPLEX_STMT>]

   The constituent simple statements include `FOR' loops, `LET'
bindings, `IF' conditionals, grouping constructs, and all of the
statements introduced in *note Communication statements::, *note I/O
statements::, and *note Other statements:::

<SIMPLE_STMT>   ::=   `FOR' <EXPR> `REPETITIONS'   [`PLUS' <EXPR> `WARMUP' `REPETITIONS'
                       [`AND A SYNCHRONIZATION']]   <SIMPLE_STMT>
                |      `FOR EACH' <IDENT> `IN' <RANGE>   [`,' <RANGE>]* <SIMPLE_STMT>
                |      `FOR' <EXPR> <TIME_UNIT> <SIMPLE_STMT>
                |      `LET' <LET_BINDING> [`AND' <LET_BINDING>]*   `WHILE'
                      <SIMPLE_STMT>
                |      `IF' <REL_EXPR> `THEN' <SIMPLE_STMT>   [`OTHERWISE'
                      <SIMPLE_STMT>]
                |     `{' [<COMPLEX_STMT>] `}'
                |     <SEND_STMT>
                |     <RECEIVE_STMT>
                |     <WAIT_STMT>
                |     <MCAST_STMT>
                |     <REDUCE_STMT>
                |     <SYNC_STMT>
                |     <OUTPUT_STMT>
                |     <LOG_STMT>
                |     <FLUSH_STMT>
                |     <RESET_STMT>
                |     <STORE_STMT>
                |     <RESTORE_STMT>
                |     <ASSERT_STMT>
                |     <DELAY_STMT>
                |     <TOUCH_STMT>
                |     <TOUCH_BUFFER_STMT>
                |     <PROCESSOR_STMT>
                |     <BACKEND_STMT>

   The remainder of this section describes in turn the `THEN' construct
and each of the just-introduced <SIMPLE_STMT> types.

* Menu:

* Combining statements::        Performing multiple statements in sequence
* Iterating::                   Performing a statement multiple times
* Binding variables::           Lending values to variables
* Conditional execution::       Executing statements only if a condition is met
* Grouping::                    Treating multiple statements as one


File: conceptual.info,  Node: Combining statements,  Next: Iterating,  Prev: Complex statements,  Up: Complex statements

4.7.1 Combining statements
--------------------------

The `THEN' keyword separates statements that are to be performed
sequentially.  For example, a simple ping-pong communication can be
expressed as follows:

     ALL TASKS RESET ALL COUNTERS THEN
     TASK 0 SENDS A 0 BYTE MESSAGE TO TASK 1 THEN
     TASK 1 SENDS A 0 BYTE MESSAGE TO TASK 0 THEN
     TASK 0 LOGS elapsed_usecs/2 AS "One-way latency"

   There is no implicit intertask synchronization across `THEN'
statements.  Consequently, the two communications specified in the
following statement will be performed concurrently:

     TASK 0 ASYNCHRONOUSLY SENDS AN 8 KILOBYTE MESSAGE TO TASK 1 THEN
     TASK 1 ASYNCHRONOUSLY SENDS AN 8 KILOBYTE MESSAGE TO TASK 0 THEN
     ALL TASKS AWAIT COMPLETION


File: conceptual.info,  Node: Iterating,  Next: Binding variables,  Prev: Combining statements,  Up: Complex statements

4.7.2 Iterating
---------------

CONCEPTUAL provides a variety of looping constructs designed to
repeatedly execute a <SIMPLE_STMT>.

* Menu:

* Counted loops::               Iterating for a given number of iterations
* Range loops::                 Iterating over lists of numbers
* Timed loops::                 Iterating for a given length of time

